[
{
	"uri": "/6250/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction: Welcome to Computer Networking We\u0026rsquo;ll be covering advanced concepts in networking such as software defined networking (SDN), data center networking (DCN) and content distribution. You\u0026rsquo;ll complete projects using a state of the art network emulator called mini-net to understand and explore these advanced concepts leading up to a final project replicating actual networking research.\nComputer Networking Welcome to the graduate course on computer networking. The primary goal of this course is to provide a survey of the necessary tools, techniques, and concepts to perform research in computer communications. This is a project based course, and there will be significant emphasis on hands-on experience. In networking, perhaps more than many other subjects, realization is key. You can read about concepts or techniques in a textbook, but really the most effective way to learn networking is by doing. So, you\u0026rsquo;ll gain a lot of hands on experience in this course through the assignments. In comparison to an introductory networking course which you may have taken, this course will provide more in depth coverage of networking topics, and it will also offer a crash course in some of the available tools that are now available for performing research in computer networking. You will gain experience with many of these tools through the project based assignments in the course.\nTwo Components The course has essentially two components. In the lectures you will learn about cutting edge research problems in computer networking and you\u0026rsquo;ll also gain the ability to come up with your own problems. We\u0026rsquo;ll pick up the basics along the way as necessary. In addition to the lectures there are also a number of problem sets or assignments that you will work through as you work your way through the course. The problem sets and assignments in the course will give you proficiency with the tools and technologies that are state of the art in the research community. That will allow you to follow through on the research ideas that you may come up with as we work through various topics in the course. There are tons of exciting tools to use, and the problem sets and assignments will help you gain proficiency with them.\nWhat the Course is NOT About It\u0026rsquo;s also worth bearing in mind what this course is not about. The course is not an introduction to networking, so there are a number of basic topics that won\u0026rsquo;t be covered in this course. In particular, we\u0026rsquo;ll assume that you\u0026rsquo;re already familiar with the basics of things like TCP, Socket programming, and so forth. Anything that you might have picked up in an introductory networking course, we are just going to assume as a prerequisite for this course. So before you proceed, it may be worth revisiting some of your old undergraduate networking course material. The course is also not providing any introduction to programming. However, many assignments in the course will make use of some amount of programming. So some knowledge of scripting languages like Ruby, Python or Perl will certainly be helpful in some assignments. We\u0026rsquo;ll be making a lot of use of a network emulation toolkit called Mininet, and to use that tool most effectively, you will certainly want to learn some Python if you don\u0026rsquo;t already know it. Don\u0026rsquo;t worry if you don\u0026rsquo;t know these languages already, though. There\u0026rsquo;s plenty of time to learn in the course since the deadlines are fairly spread out. And the assignments aren\u0026rsquo;t focused on knowledge of programming per say, but rather, the concepts that you are going to realize in the programming languages.\nCourse Structure The course is broken into three smaller sub-courses. The first course will cover topics including architectural principles, switching, routing, naming, addressing, and forwarding. The second part of the course will cover congestion control, streaming, rate limiting, and content distribution. And the third part of the course will have modules on software defined networking, traffic engineering, and network security. There will be about three assignments per sub course, plus a final project.\n"
},
{
	"uri": "/7646/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": " Python: Numpy \u0026amp; Pandas Dataframes A dataframe is a data structure in pandas that allows multiple datasets to be mapped to the same indices. For example, a data frame that maps dates to closing prices could be:\n    SPY AAPL GOOG GLD     2000-01-09 101.01 50.89 NaN NaN   2000-01-10 100.05 50.91 NaN NaN   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   2015-12-31 200.89 600.25 559.50 112.37    The indices in this dataframe are the dates on the left, and the closing prices for that date are stored in each column. The ”NaN”s appear because GOOG and GLD were not publicly traded during those periods.\nReading CSVs into Dataframes To begin using the dataframes, you need data first. Historical stock data from Yahoo is provided in the form of a CSV file, which can be easily read into a dataframe using pandas’s function read csv().\nimport pandas as pd def test_run(): df = pd.read_csv(\u0026quot;data/AAPL.csv\u0026quot;) print df if __name__ == \u0026quot;__main__\u0026quot;: test_run()  This example reads in a CSV corresponding to the historic data for AAPL (Apple, Inc) into the variable df. df is a DataFrame object, which means any DataFrame methods may be used on it.\nAn example of a method that can be used is max(), which returns the maximum value in the range.\nimport pandas as pd def get_max_close(symbol): df = pd.read_csv(\u0026quot;data/{}.csv\u0026quot;.format(symbol)) return df ['Close'].max() def test_run(): for symbol in ['AAPL','IBM'] print \u0026quot;Max close \u0026quot; print symbol, get_max_close(symbol) if __name__ == \u0026quot;__main__\u0026quot;: test_run()  Plotting Matplotlib can be used to plot the data in the dataframes, as pandas can conveniently tap into the matplotlib API. Plotting data in a dataframe is as simple as calling plot() on one of the series in the frame.\nExample: Plotting the Adjusted Closea price of AAPL\nimport pandas as pd import matplotlib.pyplot as plt def test_run(): df = pd.read_csv('data/AAPL.csv') print df['Adj Close'] df[['Adj Close', 'Close']].plot() plt.title('Comparison') plt.show() if __name__ == '__main__': test_run()     index AdjClose     0 669.79   1 660.59   2 662.74   3 680.44   4 676.27   \u0026hellip; \u0026hellip;   3173 24.60   3174 24.96    Issues There are some issues with the data that need to be solved to effectively use it in the way we want.\n Trading days: The NYSE only trades for a certain number of days per year, which means that indexing by dates will return some results when the exchanges were not open. This poses problems for trying to pull out certain date ranges from the dataframe. Multiple stocks: One of the dataframe’s powers is to be able to contain multiple ranges, which means that we need to be able to retrieve multiple datasets and store them into the dataframe. Date order: The data in the Yahoo CSV are in reverse chronological order (most recent at the top), so any analysis on the dataframe will be going backwards in time, which is not ideal.  Solution to the issues To solve the trading days problem, we’ll use an Exchange-Traded Fund (ETF) called SPY (S\u0026amp;P 500) to serve as a basis for what days the stock market is open. The only days that exist in the dataset for this ETF are the days the stock market traded, so if we use this as a reference and use joining on the dataframes, we can recover data on only the days which had trading.\nExample: Using joins to get only traded days\nstart_date = '2010-01-22' end_date = '2010-01-26' dates = pd.date_range(start_date, end_date) df1 = pd.DataFrame(index=dates) # build empty dataframe  If we were to print out df1, the output would be:\nEmpty DataFrame Columns : [] Index : [2010-01-22 00:00:00, 2010-01-23 00:00:00, 2010-01-25 00:00:00, 2010-01-26 00:00:00]  This empty dataframe will be the basis for the data we want to retrieve. The next step is to join this dataframe with a dataframe with the data for SPY. This will keep only indices of the SPY dataframe that also exist in the empty one.\nExample: Reading in the new dataframe and joining them\ndfSPY = pd.read_csv(\u0026quot;data/SPY.csv\u0026quot;, index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=[ 'Date','Adj Close'], na_values =['nan']) df1 = df1.join(dfSPY)  The output would now be:\n Adj Close 2010-01-22 104.34 2010-01-23 NaN 2010-01-24 NaN 2010-01-25 104.87 2010-01-26 104.43  To get rid of the ”NaN”s, you can call dropna() on the newly joined dataframe, but there is a better way of joining them such that the ”NaN”s don’t appear in the first place. The join type is called an inner join, which joins at the intersection of the two dataframes. This way, only the dates which are in both will be kept as indices. Everything else will be thrown away.\nExample: The inner join\ndf1 = df1.join(dfSPY, how='inner')  Multiple stocks Reading in multiple stocks is as easy as just adding a for loop:\nExample: Reading in multiple stocks into a single dataframe\ndfSPY = dfSPY.rename(columns={'Adj Close': 'SPY'}) df1 = df1.join(dfSPY, how='inner') symbols = ['GOOG', 'IBM', 'GLD'] for symbol in symbols: df_tmp = pd.read_csv(\u0026quot;data/{}.csv\u0026quot;.format(symbol), index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=['Date', 'Adj Close'], na_values=['nan']) # rename to prevent name clash (multiple columns with same name) df_tmp = df_tmp . rename(columns={'Adj Close': symbol}) df1 = df1.join(df_tmp, how='inner')  Here’s an example of reading and plotting multiple stocks’ closing price on one plot\nExample: Reading and plotting multiple stocks\nimport os import pandas as pd import matplotlib.pyplot as plt def plot_selected(df, columns, start_ind, end_ind): print df.ix[start_ind: end_ind, columns] plot_data(df.ix[start_ind:end_ind, columns]) def symbol_to_path(symbol, base_dir=\u0026quot;data\u0026quot;): return os.path.join(base_dir, \u0026quot;{}. csv\u0026quot;. format(str(symbol))) def get_data(symbols, dates): df = pd.DataFrame(index=dates) if 'SPY' not in symbols: symbols.insert(0, 'SPY') for symbol in symbols: tmp = pd.read_csv(symbol_to_path(symbol), index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=['Date', 'Close'], na_values=['nan']) tmp = tmp.rename(columns={'Close': symbol}) df = df.join(tmp) if symbol == 'SPY': df = df.dropna(subset=['SPY']) return df def plot_data(df, title=\u0026quot; Stock prices\u0026quot;): ax = df.plot(title=title, fontsize=12) ax.set_xlabel(\u0026quot;Date\u0026quot;) ax.set_ylabel(\u0026quot;Closing price\u0026quot;) plt.show() def test_run(): dates = pd.date_range('2010-01-01', '2010-12-31') symbols = ['IBM ', 'GLD '] df = get_data(symbols, dates) plot_selected(df, ['SPY', 'IBM '], '2010-03-01', '2010-04-01') if __name__ == '__main__': test_run()  Normalizing Sometimes when plotting, the values of a stock will be significantly different from the other stocks such that it becomes difficult to tell some of them apart. Normalizing the data allows all of them to start at the same point and then show divergences from the initial point, making it easier to compare them at the same time.\nNormalizing the dataframe is as simple as dividing the entire dataframe by its first row\nExample: Normalizing a dataframe\ndef normalize_data(df): df = df / df.ix[0 ,:] return df  NumPy The actual data in the dataframe is actually an ndarray in NumPy (A multidimensional homogeneous array). That means we can do operations on the data using NumPy. For example, if you have a dataframe df1, the ndarray would be extracted by doing\nnd1 = df1.values  Accessing a cell in the array is as simple as:\nval = nd1[row, col]  You can also access subarrays by indexing with the colon:\nsub = nd1[0:3, 1:3]  would capture the rectangular subarray from the first to the third rows and the second to third columns.\nIndexing Note that the second part of the index is 1 past the actual index that will be the last, so 0:3 only pulls out 0, 1, and 2. Like in MATLAB, you can pull out everything using just the colon. For example:\nsub = nd1[3 ,:]  would retrieve all columns of row 3.\nNegative indexing To get the last index, you can use negative numbers (the last index would be -1, second to last -2, etc.)\nsub = nd1[-1, 1:3]  would get columns 1,2 of the last row\nBoolean indexing/masking Suppose we want to get the values in an array, a, which are all less than the mean. NumPy’s masking feature makes it really intuitive, as all you need to do is:\nlessThanMean = a[a \u0026lt; a.mean()]  The array a \u0026lt; a.mean() would be a boolean array, which might look like\n[[ True, True, False, False]]  Assignment Assigning values in an array is easy using the NumPy notation. For example, say we wanted to replace the values in the first 2x2 square of nd1 with the 2x2 square in nd2 with columns 2 and 3, and rows 3, and 4. The operation would be:\nnd1[0:2, 0:2] = nd2[-2:, 2:4]  Creating an array Creating a numpy array is as easy as passing in a normal python list into the array method:\nimport numpy as np print np.array([1, 2, 3])  Creating a 2D m x n array is as simple as passing in a m-long list of n-tuples.\nprint np.array([(1, 2, 3), (4, 5, 6)])  would output\n[[1 ,2 ,3] [4 ,5 ,6]]  More initializers You can also create arrays with certain initial values.\nnp.empty((5, 3, 2))  initializes an ”empty” 5x3x2 dimensional array. The values in the array are actually whatever was in the memory locations of the array pointers, so the output could look like garbage.\nnp.ones((5, 4), dtype=np.int)  creates a 5x4 array, where the value in each cell is the integer 1.\nnp.random.random((5, 4))  creates a 5x4 array with random numbers from a uniform distribution in [0.0,1.0). An example result could be:\n[[ 0.82897637 0.36449978 0.91209931 0.96307279] [ 0.63777312 0.24482194 0.5817991 0.18043012] [ 0.85871221 0.98874123 0.68491831 0.53831711] [ 0.52908238 0.81083147 0.97440602 0.81032768] [ 0.98566222 0.38902445 0.16922005 0.0873198 ]]  Other methods or fields, such as sum() or size() can be looked up in online documentation.\n"
},
{
	"uri": "/42/",
	"title": "42",
	"tags": [],
	"description": "",
	"content": " About Georgia Tech\u0026rsquo;s online Master of Science in Computer Science (OMS CS) comprises a curriculum of courses taught by the world-class faculty in the Georgia Tech College of Computing.\nGetting Started TODO: Resources/guidance for new students\u0026hellip;\nCommunities  Slack Reddit Facebook  Resources:  OMSCS FAQ: here and here Course Reviews Awesome-OMSCS Repo  "
},
{
	"uri": "/6250/architecture-and-principles/",
	"title": "Architecture and Principles",
	"tags": [],
	"description": "",
	"content": " Architecture \u0026amp; Principles We\u0026rsquo;ll begin our foray into networking by reviewing the history of the internet and its design principles. Networking today is an eclectic mix of theory and practice in large part because the early internet architects set out with clear goals and allowed flexibility in achieving them.\nWith all that flexibility, does that mean we\u0026rsquo;ll see the rollout of IPv6 soon? Only in your dreams.\nA Brief History of the Internet In this lesson we will cover a brief history of the internet. The internet has its roots in the ARPA Net which was conceived in 1966 to connect big academic computers together. The first operational ARPA Net nodes came online in 1969 at UCLA, SRI, UCSB, and Utah. Around the same time, the National Physical Laboratory in the UK also came online. By 1971 there were about 20 ARPANet Nodes and the first host-to-host protocol. There were two cross country links, and all of the links were at 50 KBPS.\nHere is a rough sketch of the ARPANet as drawn by Larry Roberts in the late 1960s. You can see the four original Nodes here, as well as some other well known players such as Berkeley, the MAC project at MIT, BBN, Harvard, Carnegie-Mellon, Michigan, Illinois, Dartmouth, Stanford, and so forth. This is what the ARPANET looked like in the late 1960s.\nHere\u0026rsquo;s a picture of the ARPANET in June 1974. And you can see not only some additional networks that have come online, but also a diagram of the machines that are connected at each of the universities. You can also see a connection here between the ArpaNet and MPLnet. Of course, the ArpaNet wasn\u0026rsquo;t the only network. There were other networks at the time. Sat Net operated over satellite. There were packet radio networks, and there were also Ethernet local area networks. Work started in 1973 on replacing the original network control protocol with TCP/IP where IP was the Internetwork Protocol and TCP was the Transmission Control Protocol.\nTCP/IP was ultimately standardized from 1978 to 1981 and included in Berkley UNIX in 1981. And on January 1st, 1983 the internet had one of its flag days, where the ArpaNet transitioned to TCP/IP. Now the internet continued to grow, but the number of computers on the internet really didn\u0026rsquo;t start to take off until the mid 90s. You can see here that around August 1995 there were about 10 million hosts on the internet, and five years later there was an order of magnitude more hosts on the internet—more than 100 million. During this period the Internet experienced a number of technical milestones. In 1982 the internet saw the rollout of the domain name system which replaced the host.txt file containing all the world\u0026rsquo;s machine names with a distributed name lookup system. 1988 saw the rollout of TCP Congestion Control after the net suffered a series of congestion collapses. 1989 saw the NSF net and BGP inter-domain routing including support for routing policy. The 90s, on the other hand, saw a lot of new applications. In approximately 1992 we started to see a lot of streaming media including audio and video. Web was not soon after, in 1993, which allowed users to browse a mesh of hyperlinks. The first major search engine was Altavista, which came online in December of 1995, and peer to peer protocols and applications including file sharing, began to emerge around 2000.\nProblems and Growing Pains Now, today\u0026rsquo;s internet is experiencing considerable problems and growing pains, and it\u0026rsquo;s worth bearing some of these in mind and thinking about them, as many of them give rise to interesting research problems to think about as we work through the material in the course. One of the major problems is that we\u0026rsquo;re running out of addresses. The current version of the internet protocol, IPV4, uses 32-bit addresses, meaning that the IPV4 internet only has 2 to the 32 IP addresses, or about 4 billion IP addresses. Furthermore, these IP addresses need to be allocated hierarchically and many portions of the IP address space are not allocated very efficiently. For example, the Massachusetts Institute of Technology has one two fifty sixth of all the Internet address space. Another problem is congestion control. Now congestion control\u0026rsquo;s goal is to match offered load to available capacity. But one of the problems with today\u0026rsquo;s congestion control algorithms is that they have insufficient dynamic range. They don\u0026rsquo;t work very well over slow and flaky wireless links and they don\u0026rsquo;t work very well over very high speed intercontinental paths. Now, some solutions exist but change is hard and all solutions that are deployed must interact well with one another. And deployment in some sense requires some amount of consensus. A third major problem is routing. Routing is the process by which those on the internet discover paths to take to reach another destination. Today\u0026rsquo;s interdomain routing protocol, BGP, suffers a number of ills, including a lack of security, ease of misconfiguration, poor convergence, and non-determinism. But it sort of works and it\u0026rsquo;s the most critical piece of the internet infrastructure in some sense because it\u0026rsquo;s the glue that holds all of the internet service providers together. Another major problem in today\u0026rsquo;s internet is security. Now while we\u0026rsquo;re reasonably good at encryption and authentication, we are not actually so good at turning these mechanisms on. And we\u0026rsquo;re pretty bad at key management, as well as deploying secure software and secure configurations. The fifth major problem is denial of service. And the internet does a very good job of transmitting packets to a destination even if the destination doesn\u0026rsquo;t want those packets. This makes it easy for an attacker to overload servers or network links to prevent the victim from doing useful work. Distributed denial of service attacks are particularly commonplace on today\u0026rsquo;s Internet. Now, the thing that all of those problems have in common is that they all require changes to the basic\ninfrastructure, and changing basic infrastructure is really difficult. It\u0026rsquo;s not even clear what the process is to achieve consensus on changes. So as we work our way through the course, it will be interesting to see the problems that we encounter in each of these areas, various solutions that have been proposed, and also to think about ways in which new protocols and technologies can be deployed. In later parts of the course we\u0026rsquo;ll learn about a new technology called software defined networking, or SDN. That makes it easier to solve some of these problems by rolling out new software technologies, protocols, and other systems to help manage some of these issues.\nArchitectural Design Principles In this lecture we will talk about the Internet\u0026rsquo;s original design principles. These design principles were discussed in the paper reading for today, the Design Philosophy of the DARPA Internet Protocols, by Dave Clark, dated 1988. The paper has many important lessons, and we will go through many of them as we revisit many of the design decisions. Before we jump into any details let\u0026rsquo;s talk about some of the high level lessons. One of the most important conceptual lessons is that the design principles and priorities were designed for a certain type of network. And as the internet evolves, we are feeling some of the growing pains of some of those choices. In the last lesson we talked about a number of the problems and growing pains of the internet. And it\u0026rsquo;s worth bearing in mind that many of the problems that we are seeing now, are a result of some of the original design choices. Now that\u0026rsquo;s not to say that some of these design choices are right or wrong, but rather that they simply reflect the nature of our understanding at the time, as well as the environment and constraints that the designers faced for the particular network that existed at that time. Now needless to say, some of the technical lessons from the original design have turned out to be fairly timeless. One concept is packet switching, which we will discuss in this lesson. And another is the notion of fate sharing, or soft state, which we will discuss in a subsequent lesson in the course.\nGoal The fundamental design goal of the internet was multiplexed utilization of existing interconnected networks. There are two important aspects to this goal. One is multiplexing or sharing. So one of the fundamental challenges that the internet technologies needed to solve was the shared use of a single communications channel. The second major part of this fundamental goal is the interconnection of existing networks. These two sub problems had two very important solutions. Statistical multiplexing, or packet switching, was invented to solve the sharing problem, and the narrow waist was designed to solve the problem of interconnecting networks. Let\u0026rsquo;s talk about each of these now in turn. We\u0026rsquo;ll first talk about packet switching\nPacket Switching In packet switching, the information for forwarding traffic is contained in the destination address of every datagram or packet. Similar to how you would write a letter and specify the destination to where you want the letter sent, and that letter might wend its way through multiple intermediate post offices en-route to the recipient, packet switching works much the same way. There is no state established ahead of time, and there are very few assumptions made about the level of service that the network provides. This assumption about the level of service that the network provides, is sometimes called best effort. So how does packet switching enable sharing? Just as if you were sending a letter, many senders can send over the same network at the same time, effectively sharing the resources in the network. A similar phenomenon occurs in packet switching when multiple senders send network traffic or packets over the same set of shared network links. Now this is in contrast to the phone network, where if you were to make a phone call, the resources for the path between you and the recipient are dedicated and are allocated until the phone call ends. The mode of switching that the conventional phone network uses is called circuit switching, where a signaling protocol sets up the entire path, out-of-band. So this notion of packet switching and statistical multiplexing, allowing multiple users to share a resource at the same time, was really revolutionary. And it is one of the underlying design principles of the internet that has persisted. Now, an advantage of statistical multiplexing of the links and the network means that the sender never gets a busy signal. The drawbacks include things like variable delay and the potential for lost or dropped packets. In contrast, circuit switching provides resource control, better accounting and reservation of resources, and the ability to pin paths between a sender and receiver. Packet switching provides the ability to share resources and potentially better resilience properties.\nPacket Switching vs Circuit Switching Quiz Let\u0026rsquo;s take a quick quiz on packet switching versus circuit switching. Which of the following are characteristics of packet switching and circuit switching: variable delay, busy signals, sharing of network resources like an end-to-end path among multiple recipients, and dedicated resources between the sender and receiver? Each of these options only has one correct answer.\nPacket Switching vs Circuit Switching Solution Variable delay is a property of statistical multiplexing, or packet switching. Circuit switch networks can have busy signals. Packet switch networks share network resources. And circuit switch networks typically have dedicated resources along a path between the sender and receiver\nNarrow Waist Let\u0026rsquo;s now take a look at the second important fundamental design goal on the internet, interconnection, and how interconnection is achieved with the design principle called the Narrow Waist. Let\u0026rsquo;s keep in mind that one of the main goals was to interconnect many existing networks, and to hide the underlying technology of interconnection from applications. This design goal was achieved using a principle called the narrow waist. The internet architecture has many protocols that are layered on top of one another. At the center is an interconnection protocol called IP, or the internet protocol. Now every internet device must speak IP or have an IP stack. Given that a device implements the IP stack, it can connect to the internet. This layer of the network is sometimes called the network layer. Now this layer provides guarantees to the layers above. On top of the network layer sits the transport layer. The transport layer includes protocols like TCP and UDP. The network layer provides certain guarantees to the transport layer. One of those guarantees is end to end connectivity. For example, if a host has an IP address, then the network layer, or IP, provides the guarantee that a packet with that host destination IP address should reach the destination with the corresponding address with best effort. On top of the transport layer sits the application layer. The application layer includes many protocols that various internet applications use. For example, the web uses a protocol called the hypertext transfer protocol or HTTP. And mail uses a protocol called SMTP or simple mail transfer protocol. Transport layer protocols provide various guarantees to the application layer including reliable transport or congestion control. Now below the network layer we have other protocols. The link layer provides point-to-point connectivity, or connectivity on a local area network. A common link layer protocol is Ethernet. Below that, we have the physical layer, which includes protocols such as sonnet or optical networks and so forth. The physical layer is sometimes called layer 1. The link layer is sometimes called layer 2 and the network layer is sometimes called layer 3. We tend to not refer to layers above the network layer by number. The most critical aspect of this design is that the network layer essentially only has one real protocol in use, and that\u0026rsquo;s IP. That means that every device on the network must speak IP, but as long as the device speaks IP it can get on the internet. This is sometimes called IP over anything, or anything over IP, now the advantage to the narrow waist, as I mentioned, is that it is fairly easy to get a device on the network if it runs IP, but the drawback is that because every device is running IP, it\u0026rsquo;s very difficult to make any changes at this layer. However, people are trying to do so, and later in the course, when we discuss software defined networking, we will explore how various changes are being made to both the IP layer, and other layers that surround it.\nGoals: Survivability So we talked about how the internet satisfies the goals of sharing and interconnection and now let\u0026rsquo;s talk about some of the other goals that are discussed in the DARPA Design Philosophy Paper. As we discuss some of these other goals it\u0026rsquo;s worth considering and thinking about how well the current internet satisfies these other design goals in the face of evolving applications, threats, and other challenges. One of the goals discussed is survivability, which states that the network should continue to work if even some devices fail, are comprised, and so forth. There are two ways to achieve survivability. One is to replicate. So one could keep state at multiple places in the network, such that when any node crashes there\u0026rsquo;s always a replica or hot standby waiting to take over for the failure. Another way to design the network for survivability is to incorporate a concept called fate sharing. Fate sharing says that it\u0026rsquo;s acceptable to lose state information for some entity, if that entity itself is lost. For example, if a router crashes all of the state on the router, such as the routing tables, are lost. If we can design the network to sustain these types of failures, where the state of a particular device shares the fate of the device itself, then we can withstand failures better. So fate sharing makes it easier to withstand complex failure scenarios and engineering is also easier. Now it\u0026rsquo;s worth asking whether the current internet still satisfies the principle of fate sharing. In a subsequent lesson, we\u0026rsquo;ll talk about network address translation and how it violates the notion of fate sharing. There are other examples where the current internet\u0026rsquo;s design violates fate sharing and it\u0026rsquo;s worth thinking about those.\nHeterogeneity The internet supports heterogeneity through the TCP/IP protocol stack. TCP/IP was designed as a monolithic transport, where TCP provided flow control and reliable delivery, and IP provided universal forwarding. Now it became clear that not every application needed reliable, in-order delivery. For example, streaming voice and video often perform well, even if not every packet is delivered. And the domain name system, which converts domain names to IP addresses, often also doesn\u0026rsquo;t need completely reliable, in-order delivery. Fortunately, the narrow waste of IP allowed the proliferation of many different transport protocols, not just TCP. The second way that the internet\u0026rsquo;s design accommodates Heterogeneity is through a best-effort service model, whereby the network can lose packets, deliver them out of order, and doesn\u0026rsquo;t really provide any quality guarantees. It also doesn\u0026rsquo;t provide information about failures, performance, et cetera. On the plus side, this makes for a simple design, but it also makes certain kinds of debugging and network management more difficult.\nDistributed Management Another goal of the internet was distributed management. And there are many examples where distributed management has played out. In addressing, we have routing registries. For example, in North America we have ARIN, or the American Registry for Internet Numbers. And in Europe that same organization is called RIPE. DNS allows each independent organization to manage its own names and BGP allows each independently operated network to configure its own routing policy. This means that no single entity needs to be in charge and thus allows for organic growth and stable management. On the downside, the internet has no single owner or responsible party. And as Clark said, some of the most significant problems with the internet relate to the lack of sufficient tools for distributed management, especially in the area of routing. In such a network where management is distributed it can often be very difficult to figure out who or what is causing a problem, and worse, local action such as misconfiguration in a single local network can have global effects. The other three design goals that Clark discusses are cost effectiveness, ease of attachment, and accountability. It\u0026rsquo;s reasonable to argue that the network design is fairly cost effective as is and current trends are aiming to exploit redundancy even more. For example, we will learn about content distributions and distributed web caches that aim to achieve better cost effectiveness for distributing content to users. Ease of attachment was arguably a huge success. IP is essentially plug and play. Anything with a working IP stack can connect to the internet. There\u0026rsquo;s a really important lesson here, which is that if one lowers the barrier to innovation, people will get creative about the types of devices and applications that can run on top of the internet. Additionally, the narrow waist of IP allows the network to run on a wide variety of physical layers ranging from fiber, to cable, to wireless and so forth. Accountability, or the ability to essentially, bill, was mentioned in some of the early papers on TCP/IP but it really wasn\u0026rsquo;t prioritized. Datagram networks can make accounting really tricky. Phone networks had a much easier time figuring out how to bill users. Payments and billing on the internet are much less precise, and we\u0026rsquo;ll talk about these more in later lectures.\nWhat\u0026rsquo;s Missing It\u0026rsquo;s also worth noting what\u0026rsquo;s missing from Clark\u0026rsquo;s paper. There\u0026rsquo;s no discussion of security. There\u0026rsquo;s no discussion of availability. There\u0026rsquo;s no discussion of mobility or support for mobility. And there\u0026rsquo;s also no mention of scaling. There are probably a lot of other things that are missing and it\u0026rsquo;s worth thinking about on your own, some of the other things that current internet applications demand, that are not mentioned in Clark\u0026rsquo;s original design paper.\nDARPA Paper Quiz So as a quick quiz, can you quickly check all of the design goals in the list that were mentioned in Clark\u0026rsquo;s original design goals paper? Security, support for heterogeneity, support for interconnection, support for sharing and support for mobility.\nDARPA Paper Solution Clark\u0026rsquo;s original design goals, paper, mentions the need to support heterogeneity, interconnection and sharing.\nEnd-to-End Argument In this lesson, we\u0026rsquo;ll cover the End-to-End Argument as discussed in the paper, End-to-End Arguments in System Design by Saltzer, Reed, and Clark in 1981. In a nutshell, the End-to-End Argument reads as follows, \u0026ldquo;The function in question can completely and correctly be implemented only with the knowledge and application standing at the end points of the communication system. Therefore, providing that questioned function as a feature of the communication system itself is not possible.\u0026rdquo; Essentially, what the argument says is that the intelligence required to implement a particular application on the communication system should be placed at the endpoints, rather than in the middle of the network. Commonly used examples of the end-to-end argument include error handling and file transfer, encrypting end-to-end versus hop-by-hop in the network, and the partition of TCP and IP of error handling, flow control, and congestion control. Sometimes the end-to-end argument is summarized as, \u0026ldquo;the network should be dumb and minimal and the end points should be intelligent.\u0026rdquo; Many people argue that the end- to-end argument allowed the internet to grow rapidly, because innovation took place at the edge in applications and services, rather than in the middle of the network, which can be hard to change sometimes. Let\u0026rsquo;s look at one example of the end-to-end argument, error handling in file transfer.\nFile Transfer Let\u0026rsquo;s suppose that computer A wants to send a file to computer B. The file transfer program on A asks the file system to read the file from the disk. The communication system then sends the file, and finally the communication system sends the packets. On the receiving side, the communication system gives the file to the file transfer program on B, and that file transfer program asks to have the file written to disk. So what can go wrong in this simple file transfer setup? Well, first, reading and writing from the file system can result in errors. There may be errors in breaking up and reassembling the file. And, finally, there may be errors in the communication system itself. Now, one possible solution is to ensure that each step has some form of error checking, such as duplicate copies, redundancy, time out and retry, so forth. One might even do packet error checking at each hop of the network. One could send every packet three times. One might acknowledge packet reception at each hop along the network. But the problem is that none of these solutions are complete. They still require application level checking. Therefore it may not be economical to perform redundant checks at different layers and at different places of this particular operation. Another possible solution is an end-to-end check and retry where the application commits or retries based on the check sum of the file. If errors along the way are rare, this will most likely finish on the first try. Now, this is not to say that we shouldn\u0026rsquo;t take steps to correct errors at any one of these stages. Error correction at lower levels can sometimes be an effective performance booster. And the trade off here is based on performance, not correctness. So whether or not one should implement additional correctness checks at these layers depends on whether or not the amount of effort put into the reliability gains are worth the extra trouble. Another example where the intend argument applies is with encryption, where keys are maintained by the end applications, and cipher text is generated before the application sends the message across the network. Now one of the key questions in the end-to-end argument is identifying the ends. The end-to-end argument says that the complexity should be implemented at the ends but not in the middle, but the ends may vary depending on what the application is. So for example, if the application or protocol involves Internet routing, the ends may be routers, or they might be ISPs. If the application or protocol is a transport protocol, the ends might be end hosts. So, identifying the ends in the end-to-end argument is always a thorny question that you have to answer first.\nEnd-to-End Argument Violations Now, when talking about the end-to-end argument, it is worth remembering that the end-to-end argument is just that. It\u0026rsquo;s an argument. Not a theorem, or a principle, or a law. And there are many things that have come to violate the end-to-end principle. Network address translators, which we\u0026rsquo;ll talk about in the next lesson, violate the end-to-end argument. VPN tunnels, which tunnel traffic between intermediate points on a network, violate the end-to-end argument. Sometimes TCP connections are split at an intermediate node along an end-to-end path, particularly when the last hop of the end-to-end path is wireless. This is sometimes done to improve the performance of the connection because loss on the last hop lossy wireless hop may not necessarily reflect congestion, and we don\u0026rsquo;t necessarily want TCP to react to losses that are not congestion related. Even spam, in some sense, is a violation of the end-to-end argument. For e-mail the end user is generally considered to be a human, and by the end-to-end argument, the network should deliver all mail to the user. Does this mean that spam control mechanisms are in violation of end-to-end, and if so are these violations appropriate? What about peer to peer systems where files are exchanged between two nodes on the Internet but are assembled in chunks that are often traded among peers? What about caches, and in-network aggregation? So, when considering the end-to-end argument, it\u0026rsquo;s worth asking whether or not the argument is still valid today and in what cases. There are questions about what\u0026rsquo;s in versus out, certainly, and what functions belong in the dumb minimal network. For example, routing is currently in the dumb minimal network. Do we really believe that it belongs? What about multicast? Mobility quality of service? What about NAT\u0026rsquo;s? And it\u0026rsquo;s worth considering whether the end-to-end argument is constraining innovation of the infrastructure by preventing us from putting some of the more interesting or helpful functions inside the network. In the third course, we will talk about software defined networking, which in some sense reverses many aspects of this end-to-end argument.\nViolation NAT Part 1 A fairly pervasive violation of the end-to-end argument are home gateways, which often perform something called network address translation. Now on a home network we have many devices that connect to the network, but when we buy service from our internet service provider we\u0026rsquo;re typically only given one public IP address. And yet we have a whole variety of devices that we may want to connect. Now the idea behind network address translation is that we can give each of these devices a private IP address and there are designated regions of the IP address space that are for private IP addresses. One of those is 192.168.0.0/16 and there are others, which you can go read about in RFC 3130. Each one of these devices in the home gets its own private IP address. The public internet, on the other hand, sees a public IP address which typically is the IP address provided by the internet service provider. When packets traverse the home router, which is often running a network address translation process, the source address of every packet is rewritten to the public IP address. Now when traffic comes back to that public IP address, the network address translator needs to know which device behind the NAT the traffic should be sent to. So it uses a mapping of port numbers to identify which device the return traffic should be sent to in the home network. So the NAT or the network address translator maintains a table that says packets with the source IP address of 192.168.1.51 and source port 1000 should be rewritten to a source address of the public IP address and a source port of 50878. Similarly, packets with a source IP address of 192.168.1.52 and source port of 1000 should be rewritten to the public IP address and a source port of 50879. Then when traffic returns to the NAT to one of these addresses the NAT knows that it needs to rewrite the destination address on the return traffic to the appropriate destination IP address and port that\u0026rsquo;s in the private network. So for outbound traffic, the NAT device creates a table entry mapping the computer\u0026rsquo;s local IP address and port number to the public IP address at a different port number and replaces the sending computer\u0026rsquo;s non-routable IP address with the gateway or the NAT public IP address. It also replaces the sender\u0026rsquo;s source port with a different source port that allows it to de-mutiplex the packets sent to this return address and port. For inbound traffic to the home network, the NAT checks the destination port on the packet, and based on the port, it rewrites the destination IP address and port to the private IP address in the table before forwarding the traffic to a local device in the home network.\nViolation NAT Part 2 Now the NAT clearly violates the end-to-end principle, because machines behind the NAT are not globally addressable, or routable, and other hosts on the public Internet cannot initiate inbound connections to these devices behind the NAT. Now there are ways to get around this, there\u0026rsquo;re various protocols. One is called STUN, or signaling and tunneling through UDP-enabled NAT devices. And in these types of protocols, the device sends an initial outbound packet somewhere, simply to create an entry in the NAT table and once that entry is created we now have a globally routable address and port to which devices on a public Internet can send traffic. Now these devices somehow have to learn that public IP address and port that corresponds to that service and this might be done using DNS for example. It\u0026rsquo;s also possible to statically configure these tunnels or mappings on your NAT device at home. Needless to say, even with these types of hacks and workarounds for NAT, it\u0026rsquo;s clear that network address translation is a violation of the end-to-end principle because by default two hosts on the Internet, one on the home network and one on the public Internet, cannot communicate directly by default.\n"
},
{
	"uri": "/7646/statistical-analysis-of-time-series-data/",
	"title": "Statistical Analysis of Time Series Data",
	"tags": [],
	"description": "",
	"content": " Statistical Analysis of Time Series Data Pandas makes it simple to perform statistical analysis on dataframes, which is extremely important in determining different indicators and acting as inputs to the learning algorithms.\nGlobal statistics For example, if you had a dataframe df1 which had the closing prices for various stocks over a given time period, you can retrieve an ndarray with the mean of the columns by just calling df1.mean().\nFigure 2.1: Example output array for mean() (called on closing prices for January 2010 through December 2012)\nIn addition to mean, there around 32 other global statistics that are available in pandas.\nRolling statistics Instead of doing analysis on the entire dataset, you might want to do a rolling analysis, which only looks at certain snapshots of the data to sample. For example, you could have a 20-day moving mean, which you would calculate day-by-day by averaging the last 20 days’ data. In later sections, this moving average will be explained in more detail, but some critical points of interest are when the moving average crosses the data.\nBollinger bands Some analysts believe that significant deviations from the moving mean will result in movement back towards the mean. If the price dips far below the mean, then it might be a buy signal, whereas if it goes too high, it could indicate a time to sell. Bollinger bands are a way of measuring this deviation.\nBollinger observed that if you look at the volatility of the stock, and if it’s too large, then you discard the movements above and below the mean, but if it’s not, then it might be worth paying attention to.\nWhat he did was place two new moving means, one $2\\sigma$ above, and another $2\\sigma$ below the moving average. If you look at deviations near to $2\\sigma$, then they’re worth paying attention to. If the price drops below $2\\sigma$, and then rises back up through it, then it could be a buy signal. (the price is moving back towards the average).\nConversely, if the price rises above $2\\sigma$, then falls back down, it could be a sell signal.\nComputing rolling statistics in pandas Pandas provides some methods to easily calculate rolling mean (rolling mean()) and rolling standard deviation (rolling_std()).\nExample: Calculating a 20-day rolling mean\nrm_SPY = pd.rolling_mean(df['SPY'], window=20)  The Bollinger bands are calculated as follows:\ndef get_bollinger_bands(rm, rstd): return rm + 2*rstd, rm - 2*rstd  Daily returns Daily returns are how much a stock’s price went up or down on a given day. They are an extremely important statistic as they can be a good comparison between different stocks.\n$dailyReturn[t] = \\dfrac{price[t] - price[t - 1]}{price[t - 1]} = \\dfrac{price[t]}{price[t - 1]} - 1$  daily_ret = (df / df.shift(1).values) - 1 daily_ret.ix[0,:] = 0  Cumulative Returns Cumulative return is calculated by finding the gain from the beginning of the range to the current time, i.e.\n$CumlativeReturn[t] = \\dfrac{price[t]}{price[0]} - 1$  For example, if the price at the beginning was \\$125, and the current price is \\$142, then the gain/cumulative return is $\\dfrac{142}{125} - 1 = .136 = 13.6\\%$\nCumulative returns are essentially the original dataset normalized.\nIncomplete data People assume that financial data is extremely well-documented and that perfect data is recorded minute by minute. They also believe that there are no gaps or missing data points. However, for any particular stock, it might have different prices on different stock exchanges! It’s difficult to know who’s right all the time. Also, not all stocks trade every day (they might suddenly start trading or stop trading).\nYou might think you can just interpolate the data between breaks, but that’d cause statistical errors and a side-effect of ”looking into the future” when doing analysis on that subset of data. The better way of doing it to minimize error is to fill forward and backwards.\nFilling To fix the ”NaN”/empty data, you can use filling to maintain the last known value until known data is reached. For example, if you had a stock that didn’t have data until 2001 and then stopped having data in 2006 but then started having data again in 2012, you could fill forward from 2006-2012 and then fill backwards from 2001 back to whenever you want your data to start.\nExample: Filling in missing data using fillna()\ndf = get_data(symbols, dates) df.fillna(method=\u0026quot;ffill\u0026quot;, inplace=True) df.fillna(method=\u0026quot;bfill\u0026quot;, inplace=True)  Histograms and scatter plots It’s difficult to draw conclusions directly from daily returns plots, so histograms make it easier to see what’s going on. A histogram allows you to see how many occurrences of each return happens relative to other returns. This histogram typically follows a Gaussian over large periods of time.\nHistograms From the histogram we can determine a few key statistics: mean, standard deviation, and kurtosis. Kurtosis is a measure of how close the curve is to a Gaussian. In stock data, there are usually more occurrences at high deviations (causing sort of ”fat tails”), which would be reflected as a positive kurtosis. Skinny tails would mean a negative kurtosis.\nExample: Getting a histogram The histogram above was generated by just calling hist() on the daily returns dataframe as such:\ndaily_returns.hist(bins=20)  The bins parameter is essentially the resolution of the histogram. The domain is divided into 20 bins and anything within those bins counts for that bin’s count in the histogram. Other statistics like mean and standard deviation are easily calculated:\nmean = daily_returns['SPY'].mean() print \u0026quot; mean =\u0026quot;, mean std = daily_returns['SPY'].std() print \u0026quot;std deviation =\u0026quot;, std  Which outputs:\nmean = 0.000509326569142 std deviation = 0.0130565407719  We can now plot the mean and standard deviations on the plot to make analysis easier:\nplt.axvline(mean, color='w', linestyle='dashed', linewidth=2) plt.axvline(mean+std, color='r', linestyle='dashed', linewidth=2) plt.axvline(mean-std, color='r', linestyle='dashed', linewidth=2)  Showing this:\nprint daily_returns.kurtosis() \u0026gt;\u0026gt;\u0026gt; SPY 3.376644  which means that the data has fat tails since it’s positive.\nThe utility of these histograms comes when plotting them together. It’s easy to compare multiple stocks in terms of their returns and volatility. If stock A’s curve is skewed more positive and is thinner than stock B, then it has a low volatility with higher returns vs stock B.\nBy looking at this chart, you can see that SPY and XOM are about the same in volatility\n$\\sigma_{SPY} = 0.013057$, $\\sigma_{XOM} = 0.013647$. However, SPY would have higher returns since $R_{SPY} = 0.000509$ whereas $R_{XOM} = 0.000151$\nScatter plots Scatter plots are another way of visualizing the correlation between two stocks. Say you had a dataframe with the daily returns for SPY and XYZ. If you took just the ndarray containing the y-axis values, and then plotted SPY on the x-axis and XYZ on the y-axis, you would see a bunch of points that might have a certain trend.\nIf you take a linear regression of this data, the slope would be called the beta ($\\beta$) value. If the $\\beta$ value for SPY and XYZ is 1, it means that, on average, if SPY (the market) moves up by 1%, then XYZ also moves up by 1%.\nThe y-intercept of the line is called $\\alpha$. It describes how the stock on the y-axis performs with respect the stock on the x-axis. If the $\\alpha$ value of XYZ with respect to SPY is positive, then, on average, XYZ is returning more than the market overall.\nCorrelation: If there isn’t any correlation in the dataset, then the linear regression doesn’t tell you anything about the relationship. A common method for calculating the correlation is by finding the sample Pearson correlation coefficient, $r_{xy}$. It’s calculated by the following:\n$r_{xy} = \\dfrac{cov(X, Y)}{\\sigma_X\\sigma_Y} = \\dfrac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 }\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$  where $cov$ is the covariance. In this case, $X$ would be the daily return for SPY and $Y$ would be the daily return for XYZ. If $|r X,Y| = 1$, then the two are perfectly correlated (either positively or negatively, depending on the sign of $\\rho$). If $|r| \u0026lt; 1$, then there is possible correlation, but a value closer to 1 means better correlation. If $r = 0$, there is no correlation.\nExample: Plotting scatter plots, getting $\\alpha$ and $\\beta$ values, and determining correlation\n# scatter plot for XOM vs SPY daily_ret.plot(kind='scatter', x='SPY', y='XOM', title=\u0026quot;Scatterplot\u0026quot;) beta_XOM,alpha_XOM = np.polyfit(daily_ret['SPY'],daily_ret['XOM'],1) plt.plot(daily_ret['SPY'], beta_XOM*daily_ret['SPY'] + alpha_XOM,'-',color='r') plt.show() # scatterplotforGLDvsSPY daily_ret.plot(kind='scatter', x='SPY', y='GLD', title=\u0026quot;Scatterplot\u0026quot;) beta_GLD, alpha_GLD = np.polyfit(daily_ret['SPY'], daily_ret['GLD'], 1) plt.plot(daily_ret['SPY'], beta_GLD*daily_ret['SPY'] + alpha_GLD, '-', color='r') plt.show() print \u0026quot;BetaXOM: \u0026quot;, beta_XOM print \u0026quot;AlphaXOM: \u0026quot;, alpha_XOM print \u0026quot;BetaGLD: \u0026quot;, beta_GLD print \u0026quot;AlphaGLD: \u0026quot;, alpha_GLD # calculate correlation using pearson method print \u0026quot;Correlationmatrix:\\n\u0026quot;, daily_ret.corr(method='pearson')  Which results in:\nBeta XOM: 0.85753872112 Alpha XOM: -0.000285580653638 Beta GLD: 0.0663816850634 Alpha GLD: 0.000660583984316 Correlation matrix: SPY XOM GLD SPY 1.000000 0.820423 0.074771 XOM 0.820423 1.000000 0.079401 GLD 0.074771 0.079401 1.000000  Looking at the $\\beta$ values, you can see that XOM is more responsive to market changes, while GLD is relatively unresponsive. However, GLD tends to perform better than the market on average, since its $\\alpha$ is positive.\nBut these values are meaningless without seeing what their correlations are. Looking at the correlation matrix, XOM is pretty well correlated with SPY, whereas GLD has a very low correlation, so changes GLD aren’t really correlated with changes in the market.\nLooking at the plots, it’s easy to see that the points for XOM are more correlated and match the line better than do those of GLD.\nSharpe ratio and other portfolio statistics The portfolio is the collection of all stocks currently owned by a person. It’s important to know various statistics associated with the portfolio to make informed decisions on what to sell/buy.\nSuppose you begin with a portfolio p consisting of the following parameters:\nstart_val = 1000000 start_date = '2009-01-01' end_date = '2011-12-31' symbols = ['SPY','XOM','GOOG','GLD'] allocs = [0.4,0.4,0.1,0.1] # @ beginning , 40% to SPY , 40% to XOM, etc  Now suppose we want to find the value of this portfolio day-by-day. If we normalize the portfolio dataframe, we essentially have a dataframe containing cumulative returns for each index. If we multiply this by allocs, we get returns scaled by each percentage of the total portfolio. Then, multiply by start val to get each stock’s total value. Finally, take the sum of this penultimate dataframe to get a single-column dataframe with the total portfolio value at each point in time. In Python,\n# get cumulative returns df = get_data(symbols, pd.date_range(start_date,end_date)) df = normalize(df) # get changes for each stock by their percentages of the starting value alloced = df * allocs # get dollar value of changes vals = alloced * start_val # sum to get total value portfolio_value = vals.sum(axis=1)  We may now compute various statistics on the portfolio’s value.\n Daily returns: Obviously, daily returns of the entire portfolio would be an important statistic, as they indicate how the portfolio changes over time. For some statistics, we need to get rid of the 0 at the beginning of the daily return or else it’ll throw off the values.  daily_rets = daily_rets[1:]   Cumulative returns: The total cumulative return of the portfolio is another interesting statistic, as you can see if the overall gain was positive or negative.  cum_ret = (port_val[-1]/port_val.ix[0,:]) - 1   Avg. and Std. Deviation: These two are the main statistics that get thrown off by the 0 at the beginning. If it were there, the mean would be closer to 0, even though technically 0 isn’t actually one of the returns.  avg_daily_ret = daily_rets.mean() std_daily_ret = daily_rets.std()  Sharpe Ratio The Sharpe Ratio is a metric that adjusts return for risk. It enables a quantitative way to compare two stocks in terms of their returns and volatility. The Sharpe Ratio is calculated based on the assumption that, Ceteris paribus,\n Lower risk is better Higher return is better  Being an economic indicator, it also takes into account the opportunity cost/return of putting the money in a risk-free asset such as a bank account with interest. A sort of risk-adjusted return may be calculated as follows:\n$R_{adj} = \\dfrac{R_p-R_f}{\\sigma_p}$  where $R_p$ is the portfolio return, $R_f$ is the risk-free rate of return, and $\\sigma_p$ is the volatility of the portfolio return.\nThis ratio is a sort of basis for how the Sharpe Ratio is calculated. The Sharpe Ratio is as follows:\n$S = \\dfrac{E[R_p-R_f]}{std(R_p - R_f)}$  Since we’re looking at past data, the expected value is actually the mean of the dataset, so this becomes:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{std(R_p - R_f)}$  One question is where $R_f$ comes from. There are three main ways of getting the data for the risk-free rate:\n The London Inter-Bank Offer Rate (LIBOR) The interest rate on the 3-month Treasury bill 0% (what people have been using recently\u0026hellip;)  LIBOR changes each day, and the Treasury bill changes slightly each day, but interest in bank accounts are typically paid in 6-month or yearly intervals. Using this simple trick, you can convert the annual/biannual amount to a daily amount:\nSuppose the yearly interest rate is $I$. If we start at the beginning of the year with a value $P$, the new value after interest is paid will be $P\u0026rsquo;$. To find the equivalent daily interest value, $I_{eq}$,\n$P\u0026#39; = P(1 \u0026#43; I_{eq})^{252}$  $P(1 \u0026#43; I) = P(1 \u0026#43; I_{eq})^{252}$  $1 \u0026#43; I = (1 \u0026#43; I_{eq})^{252}$  $(1 \u0026#43; I_{eq}) = \\sqrt[252]{1\u0026#43;I}$  $I_{eq} = \\sqrt[252]{1\u0026#43;I} - 1$ \nTherefore, $R_f$, the daily risk-free rate, is just $\\sqrt[252]{1 + I} - 1$. The reason it’s 252 instead of 365 is because there are only 252 trading days in a year.\nSince we’re treating $R_f$ as constant, the standard deviation in the denominator just becomes $std(R_p)$, so the final equation for the Sharpe Ratio becomes:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{\\sigma_{R_p}}$  Sampling rate The Sharpe Ratio can vary widely depending on the sampling frequency. Since $SR$ is an annual measure, any calculations that are done with samples more frequent than yearly need to be scaled to get the annual ratio. To adjust the calculated Sharpe Ratio to be ”annualized”, you just multiply by a factor of $\\sqrt{\\textrm{#samples per year}}$. So if you sample daily, the Sharpe Ratio would become:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{\\sigma_{R_p}} \\sqrt{252}$  Example: Given 60 days of data with the following statistics:\n $R_p = 10bps$ $R_f = 2bps$ $\\sigma R_p = 10bps$,  what is the Sharpe Ratio? One bps is one hundredth of a percent.\nOptimizers An optimizer can: - Find minimum/maximum values of functions - Build parameterized models based on data - Refine allocations to stocks in portfolios For example, say you have the function $f(x) = (x - 1.5)^2 + .5$, and you want to find the minimum. It’s trivial to use calculus and find the minimum analytically, but you can’t always do so if you don’t have an analytical model of the data. Let’s put this in Python:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import scipy . optimize as spo def f(x): y = (x - 1.5)**2 + .5 print \u0026quot;x = {}, y = {}\u0026quot;.format(x, y) return y def test_run(): guess = 2.0 min_result = spo.minimize(f, guess, method='SLSQP', options={'disp': True}) print \u0026quot; minima found at:\u0026quot; print \u0026quot;x = {}, y = {}\u0026quot;.format(min_result.x, min_result.fun) if __name__ == \u0026quot; __main__ \u0026quot;: test_run()  outputs:\nx = [2.], y = [0.75] x = [2.], y = [0.75] x = [2.00000001], y = [0.75000001] x = [0.99999999], y = [0.75000001] x = [1.5], y = [0.5] x = [1.5], y = [0.5] x = [1.50000001], y = [0.5] Optimization terminated successfully. (Exit mode 0) Current function value: [0.5] Iterations: 2 Function evaluations: 7 Gradient evaluations: 2 minima found at: x = [1.5], y = [0.5]  Pitfalls Optimizers aren’t perfect, and since the method used above uses the gradient of the current point to move to the next point, it can be tripped up by various abnormalities in the function it’s trying to minimize, such as:\n Flat ranges: If a portion of the graph is flat (the slope is close to or is 0), then the solver will either take a lot of iterations to solve for the minimum or it might not ever be able to move to a new point, unless it can find a way out. Discontinuities: If there are discontinuities, the gradient might not be defined well enough for the solver to continue. Multiple minima: Say you have a function $f(x) = x^4 − 2x^2 + x^2$. This function has 2 minima at $(0, 0)$ and $(1, 0)$. If the solver starts at $x = 1.5$, it’ll find the minimum at $(1, 0)$, but it won’t ever reach the other minimum. Conversely, if the solver starts at $x = −1.5$, it’ll find the minimum at $(0, 0)$. Therefore, it’s easy to get trapped in a local minimum that may not be the actual global minimum.  Convex problems A real-valued function $f(x)$ defined on an interval is called convex if the line segment between any two points on the graph of $f(x)$ on that interval lies above the graph. Otherwise, it’s called non-convex.\nLeft: a convex function. Right: a non-convex function. It is much easier to find the bottom of the surface in the convex function than the non-convex surface. (Source: Reza Zadeh)\nBuilding a parameterized model If you have a set of data points representing rainfall and humidity that were gathered, you might want to find a function that best fits those points. Say you wanted to fit a line $f(x) = mx + b$ to the points. In this case, you can use linear algebra and find the leastsquares solution, but you can also use an optimizer to find the best parameters $m$ and $b$. What does ”best” mean? Well, we can devise a measure for the error for each point:\n$e_i = (y_i - f(x_i))^2 = (y_i - (mx_i \u0026#43; b))^2$  which is just the difference between the actual value and our model’s predicted value. The reason it’s squared is to ensure that negative errors don’t reduce the total error when we sum up every $e_i$.\n$E = \\sum_{i=1}^{n} (y_i - (mx_i \u0026#43; b))^2$  Now that we have what we want to minimize, $E$, we can use a minimizer to find the best $m$ and $b$. To make the parameters nicer to work with in Python (and allow generalization to higher degrees of polynomials), we’ll rename $m$ and $b$ to $C_0$ and $C_1$. Now, $f(x) = C_0x+C_1$.\nFor example in python:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import scipy . optimize as spo # line is a tuple (C0 , C1) def error(line, data): return np.sum((data[:, 1]-(line[0]*data[:, 0]+line[1]))**2) def fit_line(data, error_func): # initial guess for parameters l = np.float32([0, np.mean(data[:, 1])]) return spo.minimize(error_func, l, args=(data,), method='SLSQP', options={'disp': True}).x def test_run(): original = np . float32([4, 2]) print \u0026quot; original line : C0 = {} , C1 = {}\u0026quot;. format(original[0], original[1]) Xoriginal = np . linspace(0, 10, 40) Yoriginal = original[0] * Xoriginal + original[1] plt.plot(Xoriginal, Yoriginal, 'b--', linewidth=2.0, label=\u0026quot;Originalline\u0026quot;) # add some random noise to the data noise_sigma = 4.0 noise = np.random.normal(0, noise_sigma, Yoriginal.shape) data = np.asarray([Xoriginal, Yoriginal+noise]).T plt.plot(data[:, 0], data[:, 1], 'go', label=\u0026quot;Data points\u0026quot;) l_fit = fit_line(data, error) print \u0026quot; Fitted line : C0 = {}, C1 = {}\u0026quot;.format(l_fit[0], l_fit[1]) plt.plot(data[:, 0], l_fit[0]*data[:, 0] + l_fit[1], 'r- -', linewidth=2.0, label=\u0026quot; Fitted line \u0026quot;) plt.legend(loc='upperright') plt.show() if __name__ == '__main__': test_run()  Portfolio Optimization Now that wee have the tools to optimize a function, we can use it to optimize our portfolio! We can choose to optimize/minimize/maximize various measures, such as daily returns, cumulative returns, or Sharpe Ratio based on the percent allocation of all of the stocks in the portfolio.\nFraming the problem First we need three things:\n a function, $f(x)$, to minimize an initial guess for $x$ the optimizer  In our case, $x$ is actually the set of allocations for the stocks. Also, since we want to maximize Sharpe Ratio, we need to multiply $f(x)$ by $-1$ to call the minimizer.\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/7646/",
	"title": "7646",
	"tags": [],
	"description": "",
	"content": " About: This course is part of the OMSCS ML specialization and is taught by the Quantitative Software Research Group at Georgia Tech. It covers pythons and introductory numerical computing, computational investing, and applied machine learning.\nResources:  Course website Calendar Software Setup Q\u0026amp;A Study Guide    Credit: much of this section comes from Ryan Babaie \u0026amp; Neil Hardy\u0026#39;s guide. Download it here:   ml4t_guide.pdf  (1401 ko)    "
},
{
	"uri": "/6250/switching/",
	"title": "Switching",
	"tags": [],
	"description": "",
	"content": " Switching At its core, a network serves to route packets between machines on the network. Let\u0026rsquo;s take a look at how packets are moved across networks. It\u0026rsquo;s more complicated than it sounds at first, but quite fascinating.\nThat\u0026rsquo;s right. To even reach your screen, the packets that make up this video likely traveled across at least four or five networks, if not more.\nYou\u0026rsquo;ll learn how that works in the routing videos on BGP, a routing protocol.\nSwitching and Bridging In this lesson, we will learn about switching and bridging. In particular, we will learn about how hosts find each other on a subnet and how subnets are interconnected. We will also learn about the difference between switches and hubs, and the difference between switches and routers. And we\u0026rsquo;ll talk about the scaling problems with Ethernet and mechanisms that can be used to allow it to scale better.\nBootstrapping Networking Two Hosts To start let\u0026rsquo;s talk about how you would network two machines, each with a single interface, to each other. So Host 1 and Host 2 would be connected by two Ethernet adapters or network interfaces. And each of these would have a LAN, or physical, or MAC address. Now a host that wants to sent a datagram to another host can simply send that datagram via its Ethernet adapter with a destination MAC address of the other host that it wants to receive the frame. Frames can also be sent to a broadcast destination MAC address which would mean that the datagram would be sent to every host that it was connected to on the local area network. Now, of course, typically what happens is a host knows a DNS name or an IP address of another host, but it may not know the hardware or MAC address of the adapter on the host that it wants to send it\u0026rsquo;s datagram to. So we need to provide a way for a host to learn the MAC address of another host. The solution to this is a protocol called ARP or the address resolution protocol.\nARP: Address Resolution Protocol In ARP, a host queries with an IP address, broadcasting that query to every other node on the network. That query will be of a form, \u0026ldquo;who has a particular IP address,\u0026rdquo; such as 130.207.160.47, and that particular host who has that IP address on the LAN will respond with the appropriate MAC address. So the ARP query is a broadcast that goes to every host on the LAN from the host that wants the answer to the query and the response is a unicast response with the MAC address as the answer. That\u0026rsquo;s returned to the host that issued the query. When the host that issues the query receives a reply, it starts to build what\u0026rsquo;s called an ARP table. It\u0026rsquo;s ARP table then maps each IP address on the local area network to the corresponding MAC address. Now, instead of broadcasting a ARP query to discover the MAC address corresponding with this IP address, the host can simply consult its local ARP table.\nLet\u0026rsquo;s now take a look at what the host does with this information. When the host wants to send a packet to the destination with a particular IP address. It takes that IP packet and encapsulates it in an Ethernet frame with the corresponding destination MAC address. Essentially, it puts that IP packet inside of an Ethernet frame. So before it sends the IP packet with that destination IP address, it first puts the packet inside a larger Ethernet frame with its own source MAC address and the destination MAC address from its local ARP table.\nARP Quiz So let\u0026rsquo;s consider what we learned about ARP. So what are the formats of the queries and responses in ARP? Is the query a broadcast where a host is asking about an IP address, and the response is a unicast with a MAC address? Is the query a unicast message asking about an IP address and the response is broadcast with a MAC address? Or is the query a broadcast asking about a particular MAC address, where the response is a unicast with the response of a particular IP address?\nARP Solution The purpose of ARP is to allow a host to discover the MAC address corresponding to a particular IP address. And the host doesn\u0026rsquo;t know which host on the LAN owns that particular MAC address. So, ARP allows the host to send a broadcast query asking about who owns a particular IP address. And the response comes from the owner of that particular IP address and the response is the MAC address.\nInterconnecting LANs with Hubs The simplest way that a LAN can be connected is with something called a hub. Hubs are the simplest form of interconnection and in some sense they don\u0026rsquo;t even exist in networks anymore today, because you can build a switch for essentially the same price. But for the sake of example, let\u0026rsquo;s just take a look at how a LAN would be connected with a Hub. Now, a hub essentially creates a broadcast medium among all of the connected hosts where all packets on the network are seen everywhere. So if a particular host sends a frame that\u0026rsquo;s destined for some other host on the LAN, then a hub will simply broadcast that frame that it receives on an incoming port out every outgoing port. So all packets are seen everywhere. There is a lot of flooding and there are many chances for collision. The chance of collision of course, introduces additional latency in the network because collisions require other hosts or senders to back off and not send as soon as they see the other senders trying to send at the same time. LANs that are connected with hubs are also vulnerable to failures or misconfiguration because even one misconfigured device can cause problems for every other device on the LAN. Suppose that you had a misconfigured device that was sending a lot of rogue or unwanted traffic. Well, on a network that\u0026rsquo;s connected with hubs, every other host on the network would see that unwanted traffic. So, we need a way to improve on this broadcast medium by imposing some amount of isolation.\nSwitches Traffic Isolation So in contrast, switches perform some amount of traffic isolation so that the entire LAN doesn\u0026rsquo;t become one broadcast medium. But instead, we can partition the LAN into separate broadcast domains or collision domains. Now a switch might break the subnet into multiple LAN segments. Typically a frame that is bound for a host in the same part or segment of the LAN is not forwarded to other segments. So, for example if we had a network with three hubs, all connected by a switch, then each of these would be its own broadcast domain. And if a host here wanted to send a frame to another destination in the same segment, well that frame would be broadcast within that domain. But the switch would recognize that the destination was in the same segment and would not forward the packet on output ports destined for other LAN segments where the destination was not. Now enforcing this kind of isolation, requires constructing some kind of switch table, or state, at the switch, which maps destination MAC addresses to output ports.\nLearning Switches Let\u0026rsquo;s take a quick look at how learning switches work. A learning switch maintains a table between destination addresses and output ports on the switch, so that when it receives a frame destined for a particular place it knows what output port to forward the frame. Initially the forwarding table is empty, so if there\u0026rsquo;s no entry in the forwarding table the switch will simply flood. Let\u0026rsquo;s look at a quick example. If host A sends a frame destined for host C, then initially the switch has nothing in its table to determine where that frame should be sent, so it will flood the frame on all of its outgoing ports. On the other hand, because the frame has a source address of A, and arrived on input port one, the switch can now make an association between address A and port one. In other words, it knows that the host with address A is attached to port one, so that in future, when it sees frames destined for host A, it no longer needs to flood, but can instead send the frames directly to port one. So, for example, when C replies with a frame destined for A, the switch now has an entry that tells it that it doesn\u0026rsquo;t need to flood that packet. But instead, can simply send the packet directly to the output port. Note also that when C replies, the switch learns another association between address C and port three. So future frames destined for host C, no longer need to be flooded, either. They can simply be forwarded to output port three. So, in summary, if a learning switch has no entry in the forwarding table, it must flood the frame on all outgoing ports. But otherwise, it can simply send that frame to the corresponding output port in the table. Note that learning switches do not eliminate all forms of flooding. The learning switch must still flood in cases where there is no corresponding entry in the forwarding table, and also, these switches must forward broadcast frames, such as ARP queries. Now because learning switches still sometimes need to flood, we still have to take care when the network topology has loops. Now most underlying physical topologies have loops for reasons of redundancy. If any particular link fails, you\u0026rsquo;d still like hosts on the LAN to remain connected.\nBut let\u0026rsquo;s see what happens when the underlying physical topology has a loop. Let\u0026rsquo;s suppose a host on the upper LAN broadcasts a frame. Each learning switch will hear that frame and broadcast it on all of its outgoing ports. When that broadcast occurs, the other learning switches that are in the topology that contains a loop will hear the rebroadcast. They in turn will not know that they shouldn\u0026rsquo;t rebroadcast the packet that they just heard. So each of those switches will in turn rebroadcast the packet on their outgoing ports. And, of course, this process will continue, creating both packet loops and what are known as broadcast storms. So, cycles in the underlying physical topology can create the potential for learning switches to introduce forwarding loops and broadcast storms. So we need some kind of solution to ensure that even if the underlying physical topology has cycles, which it often needs for redundancy, that the switches themselves don\u0026rsquo;t always flood all packets on all outgoing ports. In other words, we need some kind of protocol to create a logical forwarding tree on top of the underlying physical topology.\nLearning Switches Quiz So, as a quick quiz about learning switches, let\u0026rsquo;s suppose that initially the switch forwarding table is empty and host D sends a frame that is destined for host B. Fill out the entry in the switch forwarding table that is populated as a result of this message.\nLearning Switches Solution When the switch sees the frame from host D, destined for host B, it doesn\u0026rsquo;t know what to do with the frame, so it forwards that frame on all of its output ports. However, because it sees a frame arrive from source D, it knows that future frames that are destined for source D should be output on port four.\nSpanning Trees The solution to this problem is to construct what\u0026rsquo;s called a Spanning Tree, which is a loop-free typology that covers every node in the graph. The set of edges, shown in blue, constitutes what\u0026rsquo;s known as a Spanning Tree. The collection of edges in the blue typology covers every node in the underlying physical typology, and yet, there are no loops in the blue topology. Now, instead of flooding a frame, a switch in this topology would simply forward packets along the spanning tree. So for example, this switch would only send a frame along the port corresponding to the blue edge and would not forward the frame out any edges that were not part of the spanning tree. Other switches that receive the frame, would flood in the same fashion, along all edges that were part of the spanning tree, while omitting edges that were not members of the spanning tree.\nLet\u0026rsquo;s take a look at how to construct the spanning tree. First the collection of switches must elect a root, since every tree must have a root. Typically this is the switch with the smallest ID. In this case, the switch at the top of the topology is the root. Then each switch must decide which of its links to include in the spanning tree. And it excludes any link if that link is determined to be not on the shortest path to the root. For example, let\u0026rsquo;s consider the switch in the lower right. It has three lengths, this length takes it on a path that\u0026rsquo;s three hops from the root. This length takes it on a path that\u0026rsquo;s two hops to the root, and this length takes it on a path, that\u0026rsquo;s one hop to the root. Any link that\u0026rsquo;s not on a shortest path to the route is excluded and any link that\u0026rsquo;s on a shortest path on a route is included. Similarly here, this edge is on a path that\u0026rsquo;s one hop away from the route and this edge is on a path that\u0026rsquo;s two hops away. So this node will include this link from the spanning tree. Now, each switch repeats this process to exclude links from the underlying topology. And ultimately, this yields a forwarding topology that looks like the blue graph. And of course there is an issue which is how do we determine the root in the first place? Well initially, every node think it\u0026rsquo;s the root. And the switches run an election process to determine which switch has the smallest ID. And if they learn of a switch with a smaller ID, they update their view of the root, and they compute the distance to the new root. Whenever a switch updates its view of the root, it also determines how far it is from that root. So that when other neighboring nodes receive those updates they can determine their distance to the new root simply by adding one to any message that they receive.\nSpanning Tree Example Let\u0026rsquo;s take a quick example. Suppose the message format is as follows. Y, d and x, where x is the origin of the message, Y is the node being claimed as root, and d is the distance of the particular node sending this message, x, from a claimed root. So, initially every switch in the network broadcasts a message like x,0,x to indicate that the node that it thinks itself is the root. When other switches hear this message, they compare the ID of the sender to their owner ID, and they update their opinion of who the root is based on the comparison of these IDs. Let\u0026rsquo;s suppose that we have the following graph and switch number 4 thinks it\u0026rsquo;s the root. So we will send a message 4, 0, 4 to nodes 2 and 7. But 2 also thinks it is the root, so 4 is going to receive the message 2,0, from node 2, and then it\u0026rsquo;s going to realize that 4 is just one hop away from node 2. So node 4 will update its view of the root to be node 2. Eventually 4 will also hear a message 2,1,7 from node 7; indicating that node 7 thinks it is one hop away from its view of the root, which is node 2. It will realize that the path through node 7 is a longer path to the root, and it will remove the link 4-7 from the tree. We can repeat this process and ultimately we will end up with a spanning tree.\nSwitches vs Routers Let\u0026rsquo;s do a quick comparison of switches and routers. Switches typically operate at layer two. A common protocol at layer two is Ethernet. Switches are typically automatically configuring, and forwarding tends to be quite fast since packets only need to be processed through layer two on flat look ups. Routers, on the other hand, typically operate at layer three where IP is the common protocol. And router level topologies are not restricted to a spanning tree. One can even have multipath routing, where a single packet could be sent along one of multiple possible paths in the underlying router level topology. So, in many ways Ethernet, or layer two switching, is a lot more convenient, but one of the major limitations is broadcast. The spanning tree protocol messages and ARP queries both impose a fairly high load on the network. So this raises the question of whether it\u0026rsquo;s possible to get many of the benefits of the auto configuration and fast forwarding of layer two without facing these broadcast limitations. As it turns out, there are ways to strike this balance. And in the third part of the course, when we talk about network management, we will look at some ways to scale Ethernet to very large topologies. For example, in data center networks. We\u0026rsquo;ll also explore how an emerging technology called Software Defined Networking, or SDN, is effectively blurring the boundary between the layer two and layer three.\nBuffer Sizing So in this lesson, we\u0026rsquo;ll look at an important question in switch design which is, how much buffering do routers and switches need? It\u0026rsquo;s fairly well known that routers and switches do need packet buffers to accommodate for statistical multiplexing. But it\u0026rsquo;s less clear how much packet buffering is really necessary. Now given that queuing delay is really the only variable part of packet delay on the internet, you\u0026rsquo;d think we\u0026rsquo;d know the answer to this question already. And for quite some time there have been some well understood rules of thumb but it turns out that we\u0026rsquo;ve recently revisited this question and come up with some different answers. So let\u0026rsquo;s first look at the universally applied rule of thumb. Now for the sake of the examples in this lesson, I\u0026rsquo;m going to use routers and switches interchangeably because it doesn\u0026rsquo;t really matter. All that matters here is that we have a network device that\u0026rsquo;s a \u0026lsquo;store and forward\u0026rsquo; packet device that has the capability of storing a frame or a packet and then later sending it on. So let\u0026rsquo;s suppose that we have a path between a source and a destination, and the round-trip propagation delay is 2T and the capacity to bottleneck link is C. Now the commonly held view is that this router needs a buffer of 2T times C. It should be clear why this rule of thumb exists. C is the capacity to the bottleneck link in say, bits per second and T is the time of units second, so this works out to bits, and the meaning of this quantity is simply the number of bits that could be outstanding along this path at any given time. It effectively represents the maximum amount of outstanding data that could be on this path between the source and destination at any time. Now this rule of thumb guideline was mandated in many backbone and edge routers for many years. It appears in RFCs and ITF Architectural guidelines and it has major consequences for router design simply because this can be a lot of router memory and memory can be expensive. The other thing of course is that the bigger these buffers, not only the bigger the cost but also the bigger the queuing delay that could exist at any given router. And hence, the more delay the interactive traffic may experience and the more delay that feedback about congestion will experience. The longer these delays are, the longer it will take for the source to hear about congestion that might exist in the network. Now to understand why this guideline is incorrect, let\u0026rsquo;s first re-derive the rule of thumb a bit more formally and then we\u0026rsquo;ll understand why it does not always apply in practice.\nBuffer Sizing for a TCP Sender Let\u0026rsquo;s suppose that we have a TCP sender that\u0026rsquo;s sending packets, where the sending rate is controlled by the window W, and it\u0026rsquo;s receiving ACKs (acknowledgements). Now at any time if the window is W, only W unacknowledged packets may be outstanding. So the sender\u0026rsquo;s sending rate, R, is simply the TCP window, W, divided by the round trip time (RTT) of the path. So the rate is W over RTT. Now remember that TCP uses additive increase, multiplicative decrease, or AIMD, congestion control. So for every W ACKs received, we send W plus one packets, and our TCP saw tooth will look something like this. We\u0026rsquo;ll start at a rate W_max over 2, increase the window to W_max and then when we see a drop we will apply multiplicative decrease and reduce the sender\u0026rsquo;s sending rate to W_max over 2 again. So here, right at the point of a packet drop, this represents the maximum number of packets that can be in flight. So again, the required buffer is the maximum number of packets that can be in flight, or simply the height of this TCP saw tooth. Now we know the rate is W over RTT, and we\u0026rsquo;d like the sender to send at a common rate, R. And if we\u0026rsquo;d like the sender to be sending at the same rate before and after it experiences a loss, then we know that the rate before the drop must equal the rate after the drop. So then we can set these two rates equal. We know that the RTT is part transmission delay T, and part queuing delay which is the maximum buffer size of the bottleneck link, divided by the capacity of the bottleneck link. We also know that after reducing the window, the queuing delay is zero. So we can replace the term on the left with W_old over 2T plus B over C and we can replace the term on the right with W_old over 2, because the congestion window has been reduced half divided by 2T, simply the propagation delay with no queuing delay. Now if we solve this equation we find that the required buffering is simply 2T times C. Now the rule of thumb makes sense for a single flow, but a router in a typical backbone network has more than 20,000 flows. And it turns out that this rule of thumb only really holds if all of the those 20,000 flows are perfectly synchronized. If the flows are desynchronized, then it turns out that this router can get away, with much less buffering.\nIf TCP Flows are Synchronized Now, if TCP flows are synchronized, the dynamics of the aggregate window as shown in the upper part of the graph, would have the same dynamics as any individual flow. The quantities on the Y axis here would simply be different. Specifically, the number of pockets occupying the buffer would be the sum of all of the TCP flows windows, rather than the window of any individual flow. Now if there are only a small number of flows in the network then these flows may tend to stay synchronized, and the aggregate dynamics might mimic the dynamics of any single flow, as shown. But as the network supports an increasingly large number of flows, these individual TCP flows become de-synchronized. So instead of all of the flows lining up with the saw tooth as shown in the bottom part, individual flows might see peaks at different times. As a result, instead of seeing a huge saw tooth that\u0026rsquo;s the sum of a bunch of synchronized flows, the aggregate instead might look quite a bit more smooth, as a result of the individual flows being desynchronized. And we can represent this sum, which is the buffer occupancy, as a random variable. At any given time, it\u0026rsquo;s going to take a particular range of values. The range of values that this buffer occupancy takes can actually be analyzed in terms of the central limit theorem.\nThe central limit theorem tells us that the more variables that we have, and, in this case the number of variables are the number of unique congestion windows of flows that we have, the narrower the Gaussian will be. In this case, the Gaussian is the fluctuation of the sum of all of the congestion windows. In fact, the width decreases as 1 over root N, where N is the number of unique, congestion windows of flows that we have. And therefore, instead of the required buffering, needing to be 2T times C, we can get away with much less buffering, in particular, 2T times C divided by the square root of N, where N, is the number of flows, passing through the router.\n"
},
{
	"uri": "/7646/essential-economics/",
	"title": "Essential Economics",
	"tags": [],
	"description": "",
	"content": " Essential Economics This chapter discusses terminology, stock market dynamics, and important indicators in economics. This will allow us to more accurately judge the value of an economic decision and make predictions on the market.\nFunds We’ll discuss three different types of funds: Exchange-Traded Fund (ETF), mutual funds, and hedge funds. Different types of funds are governed under different rules. ETFs are similar to stocks in that they are bought and sold at will like stocks- very liquid. However, ETFs typically represent baskets of stocks, and it is known to the trader what the fund represents. Mutual funds can only be bought and sold at the end of the day, and the holdings within a mutual fund are only disclosed every quarter. The least transparent holdings are that of a hedge fund. Before investors can buy shares in the fund, they must sign a long term agreement and holdings are rarely disclosed.\nAbout Funds For stocks and ETFs, having a large ”cap” means that the total value of stocks (number of stocks × price of a stock) in a company is worth many billions of dollars. Moreover, the price of a stock doesn’t reflect the value of a company, but the price at which they are selling shares. ETFs, like stocks, can easily be traded through individuals alone, whereas shares in mutual funds require a broker and hedge fund shares require more of a one on one relationship. Managers of ETFs and mutual funds are compensated based on expense ratios, which denote a percentage of Assets Under Management (AUM). For an ETF, expense ratios range from 0.01% to 1%, and in mutual funds from 0.5% to 3%. Hedge funds follow a ”two and twenty” policy, where managers get 2% of the AUM and 20% of the profits.\nThe type of a fund can be more easily recognized by how it’s named. For example, an ETF has a ticker, or stock symbol, with three or four letters, like AAPL. A mutual fund has five letters, like VTINX, and a hedge fund doesn’t have a ticker because shares are much less liquid. How much money is managed by a fund is known as the AUM, and shares represent percentages of the AUM.\nIt’s fairly clear to see that hedge funds are very different from ETFs and mutual funds. Hedge funds typically have no more than 100 investors, whereas ETFs and mutual funds have thousands. Those that invest in hedge funds are typically very wealthy individuals, institutions, and funds of funds. Funds of funds typically take large sums of money from potentially many places and invest in several hedge funds. This is a bridge for smaller investors to participate in hedge fund. The goal of a hedge fund typically falls along the lines of two ideals. The hedge fund may be out to beat a bench mark which is to say that the hedge fund aims to outperform an index of stocks. A hedge fund could also aim for absolute return, which translates to net positive profit no matter what, but usually takes more time and has fewer returns as a trade off for stability. We’ll be focusing on hedge funds because they are the most computationally demanding.\nFund Metrics and Operations Measuring the performance of a fund is vital for making financial decisions in the market, so here we’ll discuss a few. Overall success can be measured by cumulative return, which is the percentage of an original value made in a given time: $\\dfrac{\\textrm{end - start}}{start}$. However, this means little if the portfolio is rapidly and wildly fluctuating. Hence it’s also useful to measure the volatility of a portfolio. This is simply measured by the standard deviation of daily returns; it’s best to have low volatility. Another important measure is the return on risks. This is done by calculating the Sharpe Ratio (SR), also called risk-adjusted reward.\n$SR = \\sqrt{252} \\dfrac{mean(\\textrm{daily returns} - \\textrm{risk free rate})} {volatility}$  The factor of $\\sqrt{252}$ comes from the number of trading days in a year. These factors can give us an idea of how well a portfolio is performing.\nAs previously mentioned, hedge funds are very computationally intensive environments. Let’s delve into the details of how a typical hedge fund works. Central to the operation of a hedge fund is its trading algorithms. Normally, a target portfolio is decided upon, then historical stock data and the target portfolio are fed to the trading algorithms to produce orders. The orders are sent to the market to alter the live portfolio, which is again fed back into trading algorithms.\nTrading algorithms work to place certain orders at the proper time. For example, an order for everything in the target portfolio shouldn’t be placed all at once because the price of the stock will go up and more money is spent than if strategic ordering were implemented. Additionally, there is another set of computational structures for determining the target portfolio.\nHistorical data, current portfolio, and prediction algorithms are fed into an optimization program to produce a target portfolio. The majority of machine learning comes into play when determining the market forecast.\nMarket Mechanics Ordering The live portfolio is altered by giving orders to a broker in the stock market, so it serves to know what exactly is in an order. The broker needs to know whether to buy or sell and of which stock(s) by their market symbols. The order must also contain the number of shares and the type of order. Stock exchanges only consider limit orders and market orders, but note that orders can be increasingly complex based on instructions given to a broker. A market order is an order at the current market price, and ordering at a limit price tells the broker to only buy or sell at a certain price; for example, some may not want to buy beyond some value or sell below some price. Of course, a limit order must also include the desired price.\nAfter the broker sends the order to the stock exchange, it is made public in the style of an ”order book”. Others can see the stocks and collective bids that have been made on them, but not who has placed the orders. The order book contains a list for each stock of the orders within it including whether the order asks for others to buy or bids on the stock. Both types include a price at which orders are allowed to be bought/sold at and the size of the order. Orders of the same type and price are lumped together. Market orders always sell at the highest bid and buy at the lowest asking price.\nThe example order book suggests that the price of the stock will decrease because there is much more selling pressure- more are selling than buying.\nDynamics of Exchange There are many stock exchanges, and each has its own order book. When an order is placed, say by an individual, the order is sent to the broker and the broker chooses between the stock exchanges to execute that order. The broker takes information from all the stock exchanges and makes a transaction based on which one has the stock with the best price. Fees are associated with making transactions in stock exchanges and with using a broker. A broker typically has many clients; the broker can observe clients who want to buy and sell at the same price and circumvent stock exchanges entirely. The law ensures that this trade can only happen if both the buyer and seller get prices that are at least as good as at an exchange. Even if this transaction cuts out the stock exchange, it must still be registered with one, and it’s usually with the exchange the stock is housed.\nOrders can be handled internally or moved through what’s called a ”dark pool”. A dark pool is a place where investors can trade without the transparency of an order book outside of a stock exchange. The results of the trade, like internal trades, still need to be registered with public exchanges after they’ve occured. A dark pool can act as an intermediary between all types of investors that may want to escape the transparency of stock exchanges. Brokers like this because they don’t have to pay fees associated with trading at a stock exchange. They also argue that it’s fair because clients are getting prices that are as good as at the market. However, hedge funds and dark pools can heavily exploit this system as it stands if they have well-placed, fast computers.\nExploiting the Market These days the market is entirely digital and computers automate transactions all across the country. As a result, orders can be processed in fractions of a second, and timing is everything. A hedge fund may pay stock exchanges enough money to house computers very close to the exchanges, which gives them a distinct advantage. For example, let’s say that someone places an order for a stock, and it’s sent to multiple markets. A hedge fund close to those markets can see that order reach one of them first and buy up that stock from the other exchanges through the high speed infrastructure they have in place. Then when that order reaches other exchanges, the hedge fund has already bought those shares and sells it back at a higher price. This is one of many strategies in High Frequency Trading (HFT) that takes place on the order of milliseconds.\nThis can also happen on an international scale. A hedge fund may have computers collocated with international markets rapidly observing and comparing order books between them. If a difference occurs in a stock between the two markets, all that needs to be done is to sell in the market with a higher price and buy in the market with a lower one. This happens very quickly, so the price of the stock is not very different at different markets. HFT strategies usually trade high volumes to turn a large profit in small price differences.\nThose that operate on HFT strategies can not only manipulate a transparent market, but also a dark one. First, let’s explain why someone would want to use a dark pool. For example, an investor who wants to sell a high volume of shares in a transparent market would want to do so in small chunks so as not to upset the price all at once and get less for the shares. However, others will see this and lower their bids knowing that a high volume is to be sold and the investor still gets less for their shares. In a dark pool, others can’t see those that want to buy or sell, so the investor may get a better price. There are many ways to exploit a dark pool, but it always stems from information leakage. Knowing the order book of a dark pool means a world of advantage. Dark pool operators or constituents may secretly participate in their own pool or leak information about it to others for a price. The private nature of a dark pool allows those who operate it to make their own rules about who can participate and how trading works. Since information is at the discretion of the operator, it’s fairly easy for those with direct access to exploit a dark pool. Those that don’t have direct access can ”game” the pool by probing it with many small volume orders. This yields some idea of the size and prices of bids, which gamers can exploit by selling when they find the bids are highest and buying when asking is lowest.\nOther Orders and Shorting Although exchanges only take market and limit orders, other orders can be made through a broker. Often a broker implements them for clients without their knowledge to benefit both themselves and the client. The most simple order above a limit order is a stop-loss order. The broker holds stock until the price drops below a certain price, then sells at market price. Similarly, a stop-gain order waits until the price climbs to a point at which the client wants to sell. Slightly more complex is the trailing stop. Here the broker sets a stop-loss criteria trailing a stock that’s increasing in price. As the price increases, so does the stop-loss criteria; when that stock starts to decrease, the stop-loss criteria is met and stocks are sold.\nWhat if someone wanted to bet that a stock will decrease and still profit? Instead of just selling high and buying low, those stocks can be borrowed and sold, so the value of those stocks is gained at a high point but the stocks are still owed. Then when the price of the stock decreases, the stock can be bought at a lower value, and the shares returned to whom they were borrowed while a profit on the difference was made. This is called shorting. As long as the price of the stock goes down, this is a good strategy; however, if the price goes up, then the difference results in a net loss.\nWorth: Company Valuation The price of a company’s stock is intended to reflect the value of the company. Ergo, if the price of a company’s stock deviates significantly from its predicted value based on the company’s predicted worth, then there’s a profitable opportunity for when it returns to reflect the company’s worth. The value of the company can be estimated several different ways. One way is to estimate its intrinsic value, which is based on the future dividends the company will give; these are annual payments to stockholders. This doesn’t really describe what the company has though. The book value of a company is founded in the company’s assets like its facilities and resources. A company’s market cap is yet another way to estimate a company’s worth, and it’s easiest to calculate. This is effectively what the stock market thinks what the company is worth and it’s the value of a stock multiplied by the total number of stocks.\nIntrinsic value may not make sense if we try to imagine the value of a company that will pay dividends consistently as long as it stands. However, the company can never be 100% reliable, so the value of its dividends amount to the value of a promise. The promise of some money in a year is worth less than the same amount given right now because of this principle. Thus, the value of a those promised dividends decreases as the time they’re promised is longer, so the total value will converge to a calculable value. Similarly, we can calculate the present value (PV) of a dollar that is promised after a certain time. It makes sense that the PV of a dollar promised right now is a dollar, but what about in a year? The PV is some fraction of its future value (FV) based interest rate (IR) and the length of time, $t$.\n$PV = \\dfrac{FV}{(1\u0026#43;IR)^t}$  In this way we have a conversion between the present value and future value of some amount of money. The interest rate is also called the discount rate and it reflects the risk involved with investment. A more stable company will have a lower discount rate because they’re more reliable. The intrinsic value, IV, of a company can be calculated knowing its discount rate and dividend payments by\n$IV = \\dfrac{FV}{IR}$  Thus, if a hypothetical company pays dividends of \\$5 a year and has a discount rate of 1%, then the value of this company is $ \\dfrac{$5}{0.01} = \\$500 $. Book value of a company is simple to calculate because it is just what the company has versus what the company owes. If a company only has a factory worth \\$1 million, a patent worth \\$500,000, and a loan of \\$200,000, then the company is worth \\$1 million − 200,000 = \\$0.8 million. The patent is considered an intangible asset and isn’t counted in calculating the book value.\nNews about companies can drastically change some of these measures. Investors reflect their opinions on the worth of a company through stocks- if they feel the company is worth less, they will sell and vice versa. Let’s say bad news about a company comes up; investors will see that as increased risk in investing in the company. The company will have to increase their IR to appease investors and the intrinsic value of the company will reduce. This would also reduce the stock price of the company, which decreases the market capitalization of the company. News can affect singular companies, sectors of business, and the market as a whole depending on the scope of the news.\nMarket strategies are based on deviations in the estimated values of a company. For example, if the intrinsic value of a company drops and the stock price is relatively high given its history, then it would probably be a good idea to short that stock because the price will almost certainly go down. The book value of a company provides somewhat of a minimum for the market cap; that is because if the market cap goes below the book value, then a predatory buyer typically buys the whole company, breaks it apart, and sells its parts for the book value to turn a profit.\nThe Capital Assets Pricing Model The Capital Assets Pricing Model (CAPM) is a model that is used to predict the return of stocks. To understand this model, a portfolio must be understood in more depth. The term portfolio has been used throughout this text, but has yet to be clearly defined; a portfolio is a set of weighted assets, namely stocks. A portfolio is a set of stocks that are weighted by their value, and all the weights add to 1. Some stocks might be shorted, so technically their portfolio value is negative and really the sum of the absolute value of their weights is mathematically written, where $ w_i$ is the weight of a stock in a portfolio\n$\\sum_{i} |w_i| = 1$  and the return of the portfolio for a given day is\n$\\sum_{i} w_i r_i$  where $r_i$ is the return for a stock in a day. As an example, lets say a portfolio is composed of two stocks, A and B, and their respective weights are 0.75 and -0.25 because stock B is shorted. Then if on a given day, stock A increases by 1% and stock B decreases by 2%, the result is a portfolio return of (0.75)(0.01) + (−0.25)(−0.02) = 1.25%.\nA similar portfolio can be made for entire markets. Although, it’s typically limited to an index which includes the largest companies in a market, like the S\u0026amp;P 500 for the US market. These companies are weighted by their market caps, so a company’s weight in the market is approximately that company’s cap, c, divided by the sum of all caps.\n$w_i = \\dfrac{c_i}{\\sum_{j} c_i}$  The CAPM predicts the return on stocks within a certain market with the simple equation\n$r_i[t] = \\beta_i r_m[t] \u0026#43; \\alpha_i[t]$  This says that the return of a stock is largely based on the return of the market as a whole, $r_m$. The degree to which a stock is affected is based on that stocks particular $\\beta$ value. Fluctuations that deviate from this are represented in the (theoretically) random variable, $\\alpha$, which (theoretically) has an expected value of zero. The $\\beta$ and $\\alpha$ values of a stock are calculated based on the historical data of daily returns. The daily returns of a stock are plotted against that of the market and the slope of the fitted line constitutes the $\\beta$ value. The y-intercept and random deviations describe the $\\alpha$ value.\nCAPM and Related Theories The nature of CAPM suggests a specific strategy when approaching the market. CAPM says that the relationship of stocks to the market is linear with an average fluctuation of zero from this relationship. This suggests that the best tactic is to simply choose a set of stocks that will perform well with a certain market environment and sit on them. Active management is a way of thinking that believes the $\\alpha$ value is not entirely random and can be predicted. This mindset promotes carefully choosing and trading stocks on a regular basis depending on predicted $\\alpha$ values. This is the dichotomy between active and passive portfolio management. If we assume that $\\alpha$ is entirely random, then the only way we can beat the market is by predicting the return on the market. However, this is not entirely true, and CAPM will be used to eliminate market risk entirely.\nCAPM gives $\\beta$ values to each stock, but there are other theories that say it’s more complicated. One of these is the Arbitrage Pricing Theory (APT), which says that there is not a contribution based on the whole market, but based on its sectors. Moreover, a stock is affected by what happens in the ten different sectors of the economy, and the CAPM equation becomes\n$r_i = \\sum_{j} \\beta_{ij}r_j[t] \u0026#43; \\alpha_i[t]$  where $j$ denotes the different sectors. This provides a more in-depth prediction of stock return.\nUsing the CAPM Now we want to use the CAPM to create a lucrative portfolio. If we say that the return on the market can never be predicted, then any component associated with market return is risk. Applying the CAPM equation to get an overall portfolio return yields\n$r_p = \\sum_{i} w-I[\\beta_ir_m(t)\u0026#43;\\alpha_i(t)] $  The only way to remove the market return component is to choose weights such that $\\sum_{i}w_i\\beta_i = 0$. At this point it may seem that there will not be an expected return for the portfolio because CAPM predicts $\\alpha$ to be random. It is here that the assumptions of CAPM are wrong. Using some information, we can predict whether a stock will perform better or worse than the market, which will yield a profit regardless of which way the market goes. This information can come from expertise, some analysis, or, in our case, machine learning.\nTechnical Analysis There are effectively two kinds of analysis: fundamental and technical. Fundamental analysis looks at the properties of a company to determine market decisions. Technical analysis looks at patterns in stock prices to make market decision; since technical analysis is clearly more useful aside from predatory buying, that’s what will be discussed.\nTechnical analysis only focuses on stock price and volume. Using these to calculate statistics gives indications on what economic decisions to make. Technical analysis is most useful on shorter time scales and when combinations of indicators point to the same decision. HFT trade at the millisecond timescale and fundamental analysis firms operate at the timescale of years; humans and computers tend to work together at a timescale in between.\nSome Good Indicators It’s useful to develop strategies based on time-series indicators, so here are a few. Momentum is a scale dependent indicator that suggests an upward or downward trend depending on slope. Moreover, an n-day momentum, $m$, for a stock with price function, $p[t]$, is calculated as\n$m = \\dfrac{p[t]-p[t-n]}{p[t-n]} = \\dfrac{p[t]}{p[t-n]} - 1$  This is simply the difference in price as a ratio to the price n days ago. Typically 5, 10, or 20 day momentum is used with values ranging from -0.5 to 0.5. Another indicator is a simple moving average (SMA); SMA is also scale dependent looking over an n-day window. The price of a stock over n days is averaged and plotted over time where points are placed at the leading end of the window. However, to get a useful value, this needs to be compared to the real time price. Like momentum, it is done as a ratio\n$SMA[n] = \\dfrac{p[t]-E(p[t-n:t])}{E(p[t-n:t])} = \\dfrac{p[t]}{E(p[t-n:t])} - 1$  Momentum and SMA together often prove to be strong indicators. For example, if there is strong positive momentum and a crossing of the price from the price being lower than the average to above- this is a good indicator the price will increase. Larger than normal deviations from the moving average are expected to return back to the average and indicate opportunities. Thus, if SMA is positive, it’s a selling opportunity, and a buying opportunity if SMA is negative.\nHow those decision are made depends on the state of the stock, or its volatility. The standard deviation of the stock’s fluctuations provides an excellent measure for when to make decisions. It depends on how certain we want to be that the price is an outlier. The farther the decision threshold is from the SMA, the more certain we are that the price is an anomaly. From basic statistics, if the decision threshold is placed two standard deviations away from the SMA, then we are 95% sure the price is an anomaly. These bands, typically at two standard deviations away from SMA, are called Bollinger bands. The way this is written mathematically is, again, a ratio, which is between the price difference and the $2\\sigma$ length where $\\sigma$ is the standard deviation.\n$BB[t] = \\dfrac{p[t] - SMA[t]}{2\\sigma}$  When making economic decisions, a Bollinger band value greater than 1 denotes a selling opportunity, and less than -1 denotes a buying opportunity. However, it’s better to trade when the price crosses the band for the second time because that signals the price moving in a profitable direction.\nWhen using these values in a machine learner, it’s important that indicators are normalized. To normalize values, follow\n$normal = \\dfrac{value-mean}{\\sigma}$  This provides a z-score by which to compare everything.\nAdjusted Prices Analysis of historical data is crucial for determining patterns and making economic decisions, but some things drastically change the price of stocks without having any effect on the real value of the stock. Dividends and stock splits are two things that do just that. The adjusted price accounts for these events and corrects the computational problems that would occur if only the price were taken into account.\nStock splits occur when the price of a stock is too high and the company decides to cut the price, but increase the volume so that the overall market cap is the same. This is a problem when dealing with data because it’s seen as a large drop in price. The adjusted price is calculated going backwards in time; moreover, the given and adjusted price are the same for a certain starting present day and adjusted going backward in time. If the price is ever split, say by 3, then at the time of the split, the price is divided by 3 so that there is no discontinuity.\nAt the time a company announces the date for payment of dividends, the price of the stock will increase by the amount of a dividend until they’re paid at which point the price rapidly decreases by that amount. This is adjusted looking back in time, and on the day a dividend is paid, the prices preceding are decreased by the proportion of the dividend payment.\nAs a note for machine learning, the data that is chosen for the learner is very important. If the stocks from today are chosen and analyzed starting from 7 years ago, then those stocks will of course do well because they’ve survived. That’s using a biased strategy, so what needs to be done is to take index stocks from 7 years ago and run with those. For adjusted price, it’s also important to note that the adjusted price will be different depending on where the starting point is chosen, and that should also be taken into account.\nEfficient Markets Hypothesis Until now, we’ve been assuming, for technical analysis, that there is information in historical data that we can exploit to determine what the market is going to do. The same goes for fundamental analysis in terms of fundamental data. However, the Efficient Markets Hypothesis says that we’re wrong on both accounts!\nHere are some assumptions of the Efficient Markets Hypothesis:\n Large number of investors: The most important assumption of EMH is that there are a large number of investors for-profit. They have incentive to find where the price of a stock is out of line with its true value. Because there are so many investors, any time new information comes out, the price is going to change accordingly. New information arrives randomly Prices adjust quickly Prices reflect all available information  The three forms of the EMH There are 3 forms of the EMH, ranging from weak to strong.\n Weak: Future prices cannot be predicted by analyzing historical prices. This leaves room for fundamental analysis, however. Semi-strong: Prices adjust rapidly to new public information Strong Prices: Prices reflect all information, public and private  Is the EMH correct? If the EMH is correct, a lot of what we’re trying to do is impossible, so we should cut our losses and go home. Luckily, there is evidence for why certain versions of the hypothesis are incorrect. The existence of hedge funds indicates that you can profit by investing in stocks other than the market portfolio.\nThe strong version is the weakest of the three, considering there are many examples of insiders using esoteric information for their own benefit. And in many cases, these people have gone to jail!\nThere is also data that shows that the semi-strong version isn’t too likely to be correct. You can see trends in 20-year annualized returns versus 10-year P/E ratio data, which means that you most likely can use fundamentals to predict future performance.\nThe Fundamental Law of Active Portfolio Management Richard Grinold was trying to find a way of relating performance, skill, and breadth. For example, you might have lots of skill to pick stocks well, but you might not have the breadth to use that skill. So he developed the following relationship:\n$performance = skill\\sqrt{breadth}$  So we need some way of measuring skill and breadth. Performance is summarized by something called the information ratio:\n$IR = IC\\sqrt{BR}$  where IC is the information coefficient, and BR is the number of trading opportunities we have.\nThe Coin-flipping Casino As a thought experiment, instead of buying and selling stocks, we’re going to flip coins, and bet on the outcome. This is analogous to buying a stock and holding it– either you earn or lose money.\nThe coin is biased (like $\\alpha$) to P(heads) = .51. The uncertainty of the outcome is like $\\beta$.\nBetting: Betting works by betting on N coins. If we win, we now have 2N coins. If we lose, we now have 0 coins, so this is an even-money bet (you either gain N coins or lose N coins.)\nThe Casino: The casino has 1000 tables, each with a biased coin, and you have 1000 tokens, so you can bet them in any way you like: 10 tokens each on 100 tables, 1 token on each table, or 1000 tokens on 1 table. Once bets have been placed, the coins are all flipped in parallel, and for each game you either lose your chips or win. So now the question is what scenario is going to net you the best outcome? Let’s take the following two bets for example:\n 1000 tokens on one table and 0 on the other 999 1 token on each of 1000 tables  Which is better? Or are they the same? In fact, the expected return of both bets are the same, but bet 2 is much less risky, as with bet 1, if you lose, you lose all of your money, but with bet 2, you might lose around half your money. In fact, the chance of losing all of your money (if you bet tails) is:\n$(.49)^{1000} \\approx 10^{-310}$  To determine which is best, we need to consider risk and reward. In this case, the reward is our expected return. If $P_w$ is the chance we would win, and $P_l$ is the chance that we would lose, and $W$ is the amount we would win, whereas $L$ is the amount we’d lose, expected return for a single bet is calculated as follows:\n$E[R] = P_wW \u0026#43; P_lL$  For the biased coin where we place all of our bets on one table, this would be:\n$E[R] = .51(\\$1000) \u0026#43; .49(−\\$1000) = \\$20$  If we placed 1 token on each table, the expected return would be:\n$E[R] = \\sum_{i=1}^{1000} .51(\\$1) \u0026#43; \\sum_{i=1}^{1000} .49(−\\$1000) = \\$20$  So, in terms of reward, neither is better or worse. So how do we choose how to allocate the tokens? It turns out that the risk makes it easy to choose.\nFirst, what’s the chance that we lose it all? For the case where all of our tokens are on one table, the chance is 49%. For the second table, it’s around $10^{-308}%, which is quite a bit smaller\u0026hellip;\nAnother way to look at the risk is by looking at the standard deviation of the bets. An example of the outcomes for situation 2 is:\n$−1, 1, 1, 1, −1, −1, 1, −1, ..., 1$  The standard deviation of which is just 1. Now, for the case where we put all of our tokens on one table, the outcomes look like this:\n$1000, 0, 0, 0, 0, ..., 0$  or\n$−1000, 0, 0, 0, 0, ..., 0$  The standard deviation (risk) in both cases is $\\sqrt{1000} \\approx 31.62$, which is much higher than the standard deviation for putting bets on each table. Now, we can create a risk-adjusted reward (Sharpe Ratio) for the single-bet case:\n$R_s = \\dfrac{\\$20}{\\$31.62} = 0.63$  For the multi-bet scenario, it’s:\n$R_m = \\dfrac{\\$20}{\\$1} = 20.0$  Clearly, the second case wins based on this ratio. Something interesting about these results is that:\n$20 = .63\\sqrt{1000}$  It turns out that this can be generalized to\n$SR_{multi} = SR_{single}\\sqrt{bets}$  Which shows that as we increase the number of bets (diversify), the Sharpe Ratio increases. This is the relationship described in the Fundamental Law of Active Portfolio Management; to increase performance, you can either increase skill or diversify (bets), although diversification only goes as the square root.\nNow, back to equation 3.1. Let’s define what information ratio means. If we consider the CAPM equation:\n$r_p[t] = \\beta_p r_m[t] \u0026#43; \\alpha_p[t]$  we can associate the first term, $\\beta_p r_m[t]$ with the market, and the second term, $\\alpha_p[t]$ with skill. Information ratio is defined as:\n$IR = \\dfrac{\\overline{\\alpha_p[t]}} {\\sigma_{\\alpha_p[t]}}$  Information ratio can be thought of as a Sharpe Ratio of excess return (due to skill). Now, the information coefficient, IC, is the correlation of forecasts to returns. IC can range from 0 (no skill) to 1 (perfect skill). BR, or breadth, is the number of trading opportunities per year. For example, if you are Warren Buffet, and hold only 120 stocks for a whole year, BR is just 120. However, if you have 120 stocks and trade them daily, $BR = 120∗365$.\nLet’s do an example: say that James Simons and Warren Buffet both have the same information ratio, and that Simons’ algorithm is 1\u0026frasl;1000 as smart as Buffet’s. Buffet only trades 120 times per year. How many trades per year must Simons execute to have the same information ratio (performance)?\n$IR_S = IC_S\\sqrt{BR_S}$  $IR_B = IC_B\\sqrt{BR_B}$  $IR_S = 1/1000IC_B$  $\\dfrac{IR_B}{IR_S} = \\dfrac{IC_S\\sqrt{BR_S}}{IC_B\\sqrt{BR_B}} = 1$  $= \\dfrac{\\dfrac{1}{1000}\\sqrt{BR_S}}{\\sqrt{BR_B}}$  $\\Rightarrow BR_S = (1000)^2 BR_B$  $= 120,000,000$ \nSo Simons must execute 120 million trades, whereas Buffet only needs to execute 120. That’s quite a difference! Indeed, skill is an extremely important factor in performance.\nPortfolio Optimization Now we wish to optimize a portfolio, and what this means is minimizing risk for a given target return. Risk is largely defined as the volatility of a stock. A portfolio is composed of some stocks that individually have their own return-risk ratios, but it is possible to weight them such that the return-risk ratio of the portfolio is higher than that of any individual stock.\nThis is done through combining correlated and anti-correlated stocks to highly reduce volatility. In the case of a highly correlated group of stocks, their combination results in a similar volatility, but if they’re combined with highly anti-correlated stocks, then with accurate weighting, fluctuations cancel out and volatility is minimal while yielding similar returns. A useful algorithm to find the best weighting is mean variance optimization (MVO). This algorithm is not explained, but we should find it. MVO and similar algorithms find the minimal risk for a given target return, and if this is plotted over all target returns, we get a curve called the efficient frontier. On a return-risk plot, a line tangent to the efficient frontier with an intercept at the origin also points to the portfolio with the minimal Sharpe ratio.\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/6242/",
	"title": "6242",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/routing/",
	"title": "Routing",
	"tags": [],
	"description": "",
	"content": " Routing Overview With your head wrapped around routing we\u0026rsquo;ll now take a look at the nuts and bolts that make routing possible: naming, addressing and forwarding.\nAnd you\u0026rsquo;ll start your first significant Mininet project. In the project you\u0026rsquo;ll investigate switched buffer sizing which can have an important effect on network performance.\nInternet Routing The next few lessons will cover internet routing. Contrary to what you might think, the internet is not a single network, but rather a collection of tens of thousands of independently operated networks, or autonomous systems, sometimes simply called ASes. Networks such as Comcast, Georgia Tech, and Google, are different types of autonomous systems. An autonomous system might be internet service provider, a content provider, a campus network, or any other independently operated network. Now when you\u0026rsquo;re sitting at home on Comcast and trying to reach content in Google or Georgia Tech, your traffic actually traverses multiple autonomous systems. This process of internet routing actually involves two distinct types of routing. One is intradomain routing, which is the process by which traffic is routed inside any single autonomous system. The other is interdomain routing, which is the process of routing traffic between autonomous systems. So computing a path between a node in an ISP like Comcast and another node in a network like Georgia Tech\u0026rsquo;s involves computation of both intradomain paths and interdomain paths. In this part of the lesson we\u0026rsquo;ll look at intradomain routing. Then we\u0026rsquo;ll study interdomain routing, as well as the business relationships that make interdomain routing so complicated. So let\u0026rsquo;s jump into our study of intradomain routing and topology.\nAutonomous Systems (AS) AS Quiz As a quick quiz, which of the following types of routing protocols are responsible for routing within an autonomous system?\nAS Solution Intradomain routing protocols are responsible for routing within an autonomous system. Interdomain routing protocols, on the other hand, are responsible for routing traffic between autonomous systems.\nIntra AS Topology Before we jump into intradomain routing, let\u0026rsquo;s take a look at what a topology might look like inside a single autonomous system. A topology inside an AS consists of nodes and edges that connect them. The nodes are sometimes called points of presence, or PoPs. A PoP is typically located in a dense population center, so that it can be close to the PoPs of other providers for easier interconnection and also close to other customers for cheaper backhaul to customers that may be purchasing connectivity from this particular AS. The edges between pops are typically constrained by the location of fiber paths, which for the sake of convenience typically parallel major transportation routes such as railroads and highways.\nHere\u0026rsquo;s an example of a single AS topology which is the Abilene Network, which is a research network in the United States. Each of these locations would be considered a PoP, and each of these PoPs may have one or more edges between them. Georgia Tech is an autonomous system that connects at the Atlanta PoP of the Abilene Network.\nHere\u0026rsquo;s a close up of the Abilene Network in the south eastern U.S. The Abilene network connects to other universities in the southeast near Atlanta and an internet exchange point called SOX, or southern crossroads. Now, thus far we\u0026rsquo;ve just talked about the topology of an autonomous system, which essentially defines the graph. The next step is to compute paths over that topology, a process called routing. Routing is the process by which nodes discover where to forward traffic so that it reaches a certain node. There are two types of intradomain routing. One is called distance vector, and the other is called link state. In the rest of this lesson we\u0026rsquo;ll explore the two different types of intradomain routing and the advantages and disadvantages of each of them. Let\u0026rsquo;s first take a look at distance vector routing.\nDistance Vector Routing In distance vector routing, each node sends multiple distance vectors to each of its neighbors, essentially amounting to copies of its own routing table. Routers then compute costs to each destination in the topology based on shortest available path. Distance vector routing protocols are based on the Bellman-Ford algorithm. A node X\u0026rsquo;s forwarding table is based on the solution to the following equation. Suppose that node X is trying to find a shortest cost route to node Y. In this case node X is trying to find a path through some intermediate node, V, that minimizes the cost between X and V, and the already known shortest cost path between V and Y. Again, the solution to this equation for all destinations, Y, in the topology is X\u0026rsquo;s forwarding table. Let\u0026rsquo;s now take a look at distance vector routing by way of example.\nExample of Distance Vector Routing 1 Let\u0026rsquo;s suppose that we have a three node network with the costs on the edges as shown. Initially, each node has a single distance vector representing the shortest path cost to each other incident node in the graph. For example, the distance between x and x is obviously zero. And the shortest known distance between x and y from x\u0026rsquo;s perspective is one, the direct path. Similarly, the shortest known distance between x and z to x at the outset is five because all it knows is the direct path. Note that a shorter path between x and z exists via y, but x simply doesn\u0026rsquo;t know about it yet. Now in distance vector routing, every node send its vectors to every other adjacent node. And each node then updates its routing table according to the Bellman-Ford equation. Let\u0026rsquo;s look at what happens when node x learns of y\u0026rsquo;s distance vectors. Well in this case, the distance from x to z will be computed as the minimum of the sums of all distances to z through any intermediate node. So the cost between x and y is one, and the distance between y and z as discovered by y\u0026rsquo;s distance vector is two. Therefore, x can update its shortest cost distance to z as three. Similarly, x will receive a distance vector from z, five two zero, but of course, when it uses the Bellman-Ford equation to update its distances, again the distance between z and x will be updated from five to three. We can repeat this exercise at other nodes, as they receive distance vectors from other nodes in the topology, and quickly, every node in the network has a complete routing table. Now when costs decrease, the network converges quickly, but one problem is that when failures occurs, bad news can actually travel slowly.\nExample of distance Vector Routing 2 Let\u0026rsquo;s look at a different example. So for the sake of illustration, I\u0026rsquo;ve increased the cost between x and z to 50, and now everyone starts with a different set of initial distance vectors. Now eventually, after running the distance vector protocol, we would see the tables converge as such. Let\u0026rsquo;s suppose that the cost of the link between x and y suddenly increased from 1 to 60. Well now in this case, y would need to update its view of the shortest path between y and x. Now it\u0026rsquo;s no longer one, but it\u0026rsquo;s not 60 either. To see why let\u0026rsquo;s go back to our Bellman-Ford equation. We can see that y thinks it can get to z with a cost of two, and that z can get to x with a cost of three. So in fact it\u0026rsquo;s going to update this entry from one to five. Then it will tell it\u0026rsquo;s neighbor z its new distance vector. In other words, that now its distance to x is no longer one but five. At this point, z needs to re-compute it\u0026rsquo;s shortest path to x. Now, it knows that it can get to y with a cost of two but it thinks still that y can get to x with a cost of five. Therefore, this entry is no longer three but seven. And now z sends its new distance vector back to y. Y then updates it\u0026rsquo;s distance vector for z and this process continues. So, then y thinks it is now nine units away from x. So z has to do this all over again and now z thinks that its shortest path is two plus nine or 11. Now this process repeats of course until z finally realizes that it has a shorter path of 50 directly through x after this counting up process exceeds the value of 50.\nThis problem is called the count to infinity problem, and the solution is called poison reverse. The idea here is that if y must route through z to get to x in its table, as it did here, then y advertizes an infinite cost for the destination x to z. So instead of sending five, zero, two, y would send infinity, zero, two. This would thus prevent z from routing back through y, and immediately, it would choose the shortest path to x, of path cost 50.\nRouting Information Protocol An example of a distance vector routing protocol is the routing information protocol or RIP. The first version of RIP was defined in 1982 where edges had unit cost, and infinity for the count to infinity problem was 16. Table refreshes occur every 30 seconds and when an entry changes, it sends a copy of that update to all of its neighbors except for the one that induced the update. This rule is sometimes called the split horizon rule. The small value for infinity ensures that the count to infinity doesn\u0026rsquo;t take very long and every round has a time out limit of 180 seconds which is basically reached when a router hasn\u0026rsquo;t received an update from a next hop for six 30 second periods. In practice, when a router or link fails in RIP, things can often take minutes to stabilize. So because of problems such as slow convergence and count to infinity, protocol designers look to other alternatives.\nLink State Routing The prevailing alternative and the one that is used in most operational networks today is link state routing. In link state routing, each node distributes a network map to every other node in the network and then each node performs a shortest path computation between itself and all other nodes in the network. So, initially each node adds the cost of its immediate neighbors, D(v), and every other distance to a node that is infinite. Then each node floods the cost between nodes u and v to all of its neighbors. And the distance to any node v becomes the minimum of the cost between u and w plus the cost to w, or the current shortest path to v. The shortest path computation is often called the Dijkstra shortest path routing algorithm. Two common link state routing protocol are open shortest paths first or OSPF and intermediate system- intermediate system or IS-IS. In recent years, IS-IS has gained increasing use in large internet service providers and is the more commonly used link state routing protocol in large transit networks today. One problem with link state routing is scale. The complexity of a link state routing protocol grows as n cubed where n is the number of nodes in the network.\nCoping with Scale Hierarchy One way of coping with scale is to introduce hierarchy. OSPF has a notion of areas, and IS-IS has an analogous notion of levels. In a backbone network, the network\u0026rsquo;s routers may be divided into levels, or areas, and the backbone itself may have its own area. In OSPF, the backbone area is called area zero, and each area in the backbone that\u0026rsquo;s not in area zero has an area zero router. The area zero routers perform shortest path computations and the routers in each of the other areas independently perform shortest path computations. Now paths are computed by computing the shortest path within an area, or, if the path must leave an area, it\u0026rsquo;s computed by stitching together the shortest path to the area zero backbone router, and then the shortest path across area zero followed by another intra-area shortest path.\nInterdomain Routing We\u0026rsquo;re now moving on to cover interdomain routing or routing between ASes. Recall that internet routing consists of routing between tens of thousands of independently operated networks, or autonomous systems. Each of these networks operates in their own self-interest and have independent economic and performance objectives, and yet they must cooperate to provide global connectivity so that when you\u0026rsquo;re sitting at home, you can retrieve content that might be hosted at the Georgia Tech network.\nNow, each independently operated network is called an autonomous system, or AS. And each AS advertises reachability to some destination by sending what are called route advertisements or announcements.\nThe protocol that ASes use to exchange these route advertisements is called the Border Gateway Protocol, or simply, BGP. A route advertisement has many important attributes, but for now, let\u0026rsquo;s just talk about three. Now a router here, let\u0026rsquo;s say on the Comcast network, might receive a route advertisement, typically from its neighboring AS. That route advertisement might contain a destination prefix, such as the IP prefix for Georgia Tech. Then it might contain what\u0026rsquo;s called a next hop IP address, which is the IP address of the router that the Comcast router must send traffic to, to send traffic along that route. Typically that next hop IP address is the IP address for the first router in the neighboring network. And the Comcast router knows how to reach that next hop IP address because its border router and the border router in the neighboring AS are on the same subnet. Typically this might be a /30 subnet, therefore this IP address is reachable from Comcast\u0026rsquo;s border. A third important attribute is what\u0026rsquo;s called the AS path, which is a sequence of what are called AS numbers that describe the route to the destination. Now strictly speaking, the AS path is nothing more than the sequence of ASes that the route traversed to reach the recipient AS. So for example, Georgia Tech\u0026rsquo;s AS number is 2637 and Abilene\u0026rsquo;s is 10578 so the AS path that Comcast would hear if it received a route advertisement from Abilene for Georgia Tech, would be 10578 followed by 2637. So in the remainder of the lesson we\u0026rsquo;ll look at other BGP route attributes. But these are essentially the three most important because they describe how to stitch together an interdomain path to a global destination. So we have the destination IP prefix for the destination that a router might want to send traffic to; the next hop, which is the IP address for the router for the next hop along the path; and finally, the AS path, which is the sequence of ASes that the route traversed en route to the AS that\u0026rsquo;s hearing the announcement. The last AS number on the AS path is often called the origin AS, because that is the AS that originated the advertisement for this IP prefix. In this case, the origin AS is 2637, or Georgia Tech, because it is the AS that originated the announcement for this prefix.\nInterdomain Routing 2 Now thus far, we\u0026rsquo;ve talked about interdomain routing BGP, or the border gateway protocol, as consisting of route advertisements solely between border routers of adjacent autonomous systems. In fact, this is a specific type of BGP called external BGP, or eBGP. But in fact, as we know, each one of these autonomous systems has routers of its own, inside. Those routers also need to learn routes to external destinations. The protocol that is used to transmit routes inside an autonomous system for external destinations, is called internal BGP or iBGP. Okay, so to review, external BGP is responsible for transmitting routing information between border routers of adjacent ASes about external destinations. And internal BGP is responsible for disseminating BGP route advertisements about external destinations to routers inside any particular AS. Note the distinction between iBGP and an intra-domain routing protocol or an IGP.\nIGP vs iBGP The IGP or the intra-domain routing protocol, disseminates routes inside an AS to internal destinations whereas iBGP or internal- border gateway protocol, disseminates routes inside an AS to external destinations. So let\u0026rsquo;s suppose that a router inside AS A is trying to reach a destination inside AS B. AS A would learn the route via eBGP and the next hop of course, at this router, would be the border router at B. And now a router inside autonomous system A would learn the route to B via iBGP. Now the BGP next stop, would be the border router. And so, this router inside AS A, needs to use the IGP, to reach the iBGP next hop.\nProtocol Quiz So as a quick quiz, which routing protocol is responsible for disseminating routes inside an AS to external destinations? Is it the IGP? Is it iBGP. Or is it eBGP?\nProtocol Solution iBGP is responsible for disseminating routes inside an AS about destination IP prefixes that are located outside that AS. The iBGP next hop is typically a next hop IP address that is reachable via the ASes intradomain routing protocol, or IGP.\nBGP Route Selection Let\u0026rsquo;s now take a quick look at BGP route selection. It is often the case that a router on a particular autonomous system might learn multiple routes to the same destination. In this case, a router on autonomous system one, might learn a route to a destination in AS4 via both AS2 and AS3. In this situation. The router in AS one must select a single best route to the destination among the choices. The selection among multiple alternatives is known as the BGP route selection process. Let\u0026rsquo;s now take a quick look at that process.\nBGP Route Selection Process The first step in the BGP route selection process is to prefer a route with the higher local preference value. The local preference value is simply a numerical value that a network operator in the local AS can assign to a particular route. This attribute is purely local. It does not get transmitted between autonomous systems, so it is dropped in eBGP route advertisements. But it allows a local network operator the ability to explicitly state that one route should be preferred over the other. Among routes with equally high local preference values, BGP prefers routes with shorter AS path length. The idea is that a path might be better if it traverses a fewer number of autonomous systems. The third step involves comparison of multiple routes advertised from the same autonomous system. The multi-exit discriminator (MED) value allows one AS to specify that one exit point in the network is more preferred than another. So lower MED values are preferred, but this step only applies to compare routes that are advertised from the same autonomous system. Because the neighboring AS sets the MED value on routes that it advertises to a neighbor, MED values are not inherently comparable across routes advertised from different ASes. Therefore this step only applies to routes advertised from the same AS. Fourth, BGP speaking routers inside an autonomous system will prefer a BGP route with a shorter IGP path cost to the IGP next up. The idea here is that if a router inside an autonomous system learns two routes via iBGP then it wants to prefer the one that results in the shortest path to the exit of the network. This behavior results in what is called \u0026ldquo;hot potato\u0026rdquo; routing, where an autonomous system sends traffic to the neighboring autonomous system via a path that traverses as little of its own network as possible. Finally, if there are multiple routes with the highest possible local preference, the shortest AS path and the shortest IGP path, the router uses a tiebreak to pick a single breaking route. This tiebreaking step is arbitrary. It might be the most stable, or the route that\u0026rsquo;s been advertised the longest. But often, to induce determinism, operators typically prefer that this tie breaking step is performed based on the route advertisement from the router with the lowest router ID, which is typically the neighboring router\u0026rsquo;s IP address. Let\u0026rsquo;s now take a closer look into local preference, AS path length, multi-exit discriminator, and hot potato routing. Now as I mentioned, the first step in the router selection process is for routers to prefer routes with higher local preference values. Now an operator can actually set the local preference value on incoming BGP route advertisements to affect which route a router ultimately selects. Let\u0026rsquo;s see how this works.\nLocal Preference Now, a router in AS1 might learn two routes to a destination, one via the AS path 2-4 and the other via the AS path 3-4. Local preference, or simply, local pref, allows an operator to configure the router to assign different preference values to each of the routes that it learns. The default local preference value is 100. But if the operator prefers that this router select the path through AS two, it can configure the router to set a higher local preference for that route such as 110. This results in this router selecting the route through AS two and sending traffic to the destination in AS four via AS two. In this way an operator can adjust local preference values on incoming routes to control outbound traffic or to control how traffic leaves its autonomous system en route to a destination. This is extremely useful in configuring primary and back up routes. For example, here the route though AS two might be the primary route ,and the route through AS three, is the backup route. Now typically, as I mentioned, local preference is used to control outbound traffic. But sometimes autonomous systems can attach what\u0026rsquo;s called a BGP community to a route to affect how a neighboring autonomous system sets local preference. A community is nothing more but a fancy jargon word for a tag on a route. So let\u0026rsquo;s suppose that AS four wanted to control inbound traffic by affecting how AS two or AS three set local preference. In this case, let\u0026rsquo;s suppose that AS two wanted traffic to arrive via AS three, its primary, rather than by AS two, its backup. In this case, AS two might advertise its BGP routes with primary and backup communities. The backup community value might cause a router in AS two to adjust its local preference value, thus affecting how AS two\u0026rsquo;s outbound traffic choices are made. So, again local preference is used to control outbound traffic, in this case AS two\u0026rsquo;s outbound traffic decision. But the use of a BGP community on the route advertisement can sometimes be used to cause a neighboring AS to make different choices regarding it\u0026rsquo;s outbound traffic, thereby, allowing an AS to specify a primary or back up path for incoming traffic. This type of arrangement requires prior agreement.\nMultiple Exit Discriminator Let\u0026rsquo;s suppose that two autonomous systems connect in two different cities, San Francisco and New York. Let\u0026rsquo;s further suppose that AS 1 wants traffic to destination d to enter via New York City, rather than via the peering link in San Francisco. Well, remember that all things being equal, routers inside AS 2 will select the BGP route with the shortest IGP path cost to the next hop, resulting in hot potato routing. So some routers will select the San Francisco egress, and other routers might select the New York egress. To override this default hot potato routing behavior, AS1 might advertise its BGP routes to AS2 with MED values. For example, if the MED value on the route learned at the border router in New York was 10, and the MED value from the route learned from the router in San Francisco was 20, then instead of performing hot potato routing, all of these routers that would ordinarily be closer to the San Francisco egress, would instead pick the route learned via the New York egress because the preference for a lower MED value comes before the preference for a next hop with the lower IGP path process. So all of these routes would instead be carried over AS 2\u0026rsquo;s backbone network and exit via New York. Thus MED overrides hot potato routing behavior allowing an AS to explicitly specify that it wants another neighboring AS to carry the traffic on its own backbone network, rather than dumping the traffic at the closest egress and forcing traffic across the neighbor\u0026rsquo;s backbone. MEDs are typically not used in conventional business relationships, but they\u0026rsquo;re sometimes used, for example, if AS 1 does not want AS2 free riding on AS 1\u0026rsquo;s backbone network. So effectively MED allows AS 1 to say, yes, I will connect or peer with you, but it is your job to carry the traffic long distances across the country. This mechanism is sometimes used when a transit provider peers with a content provider, and the transit provider doesn\u0026rsquo;t want the content provider essentially getting free transit through the neighboring AS.\nIn the absence of MED overriding any behavior, typically what will happen is a router inside AS 2 would learn multiple routes via internal BGP to different egress points for the same destination d, and it would simply pick the next hop, or the egress router with the lowest IGP path cost, in this case, 5. It\u0026rsquo;s very common practice to set these IGP costs in accordance with distance, or propagation delay, thus resulting in routers inside the AS picking shorter paths. Now one problem with this notion of hot potato routing is that a very small change in IGP path cost can result in a lot of BGP routing changes. Remember that it\u0026rsquo;s probably not just one destination that\u0026rsquo;s being routed through the San Francisco egress, but maybe tens of thousands of routes. So a single IGP path cost change can result in rerouting of tens of thousands of IP prefixes in BGP. People have looked at various ways to improve the stability of BGP routing by decoupling the IGP and the BGP in this part of the route selection process.\nInterdomain Routing Business Models So now we\u0026rsquo;re going to look at Interdomain Routing Business Models. So the one thing to remember about interdomain routing is that it\u0026rsquo;s really all about routing money. Let\u0026rsquo;s consider this AS that wants to send traffic to a particular destination. Well, in the internet there are two different types of business relationships: a customer-provider business relationship, where money flows from customer to provider regardless of the direction that traffic flows; the other type of business relationship is a peering relationship where an AS can exchange traffic with another AS free of charge. This is sometimes also called settlement-free peering. So already you can see given three possible ways to reach the destination. This AS is first going to prefer a route through its customer, because regardless of the direction of traffic on this link, money is always flowing from the customer. The peering link is the second most preferable because it\u0026rsquo;s free. And the least preferable route is through the provider, because the AS has to pay money every time it sends traffic on this link. This leads to the basic rules of preference in interdomain routing, where customer routes are preferred over peer routes, which are in turn preferred over provider routes.\nThe other consideration that an AS has to make is filtering, or export decisions. In other words, given that an AS learns a route from its neighbor, to whom should it re-advertise that route? To understand filtering and export decisions, let\u0026rsquo;s add a couple more AS\u0026rsquo;s to the graph. Let\u0026rsquo;s add another peer, and let\u0026rsquo;s add another provider. Let\u0026rsquo;s call this AS in the middle of the picture Cox Communications. This ISP might have smaller regional customers and it might also buy transit connectivity from other providers. Now let\u0026rsquo;s suppose that this AS learns routes to a destination via its customer, its peer, and its provider. Now we already have established that it would prefer the customer route, so that it can make money by sending traffic to that destination. But what about filtering decisions? Well, routes that are learned from a customer, Cox of course would want to re-advertise to everyone else, because the more people use that route, the more money Cox makes. Therefore a route that\u0026rsquo;s learned from a customer, gets advertised to everybody else. On the other hand, a route that\u0026rsquo;s learned from a provider, if it were actually selected, would of course, only be advertised to customers. It wouldn\u0026rsquo;t make any sense to take a route like this and advertise it to another provider. The reason, of course, is that money is flowing in the direction of the providers. So any route that\u0026rsquo;s learned from a provider would never be advertised to another provider, because it would result in Cox essentially becoming a transit provider between two of its own providers and paying them both for the privilege of carrying that traffic. So routes learned from a provider would only ever be advertised to other customers. And similarly, routes from peers would only be advertised to other customers, not to other peers or other providers. So to summarize, interdomain routing has both ranking rules, where, given multiple choices, an AS might prefer a customer route over a peer route over a provider route. And then, given that it selected a particular route from either a customer, a provider, or a peer, it makes different decisions about where to re-advertise that route to other neighboring ASes. Now as it turns out, if every AS in the internet followed these rules exactly, then routing stability is guaranteed. Now you might wonder, isn\u0026rsquo;t routing stability guaranteed already? And it turns out that it isn\u0026rsquo;t.\nInterdomain Routing Can Oscillate! In fact, interdomain routing can oscillate indefinitely. To see why, consider the following 4 AS topology, where each AS specifies preferred paths, presumably via local preference. So each AS prefers the AS in the clockwise direction, rather than the shorter, direct path. Now it\u0026rsquo;s pretty easy to see that there\u0026rsquo;s no stable solution. Let\u0026rsquo;s suppose that we started off with everybody selecting the direct path. Well, in this case, any one of these ASes would notice that it has a more preferred path. So for example, AS 1 would see that because AS 3 has picked the direct path, then, in fact, it could prefer a situation where oscillations can occur indefinitely. Similarly, here now AS 3 sees that it has a more preferred path, 3 2 0, so it might switch to that.\nIn doing so, it breaks AS 1\u0026rsquo;s path. 1 3 0 no longer works. So AS1 has to switch back to its less preferred direct path, but now we\u0026rsquo;re in the same situation all over again because now AS2\u0026rsquo;s preferred path becomes available via 1, so AS 2 now reroutes, and AS 3\u0026rsquo;s most preferred path, 3 2 0, no longer works so it must switch to the direct path.\nNow, it\u0026rsquo;s very easy to see that this oscillation continues ad infinitum. This particular pathology was first discovered by Varadhan, Govindan, and Estrin, in a paper called persistent route oscillation in interdomain routing, in 1996. Later, Tim Griffin formalized this pathology and derived conditions for stability. Those stability conditions came to be known as a BGP correctness property called safety. It turns out that if ASes follow the ranking and export rules that we discussed, that safety is guaranteed. But, there are various times when those rules are violated. Business relationships, such as regional peering and paid peering, can occasionally cause those conditions to be violated. So as it turns out, to this day, BGP is not guaranteed to be stable in practice, and many common practices result in the potential for this type of oscillation to occur.\n"
},
{
	"uri": "/7646/machine-learning-algorithms/",
	"title": "Machine Learning Algorithms",
	"tags": [],
	"description": "",
	"content": " Machine Learning Algorithms In most cases, machine learning algorithms are focused on building a model. Then, the model can be used to take inputs and give outputs based on the model. The model is a tool to predict outputs based on the inputs. In our case, we will be using models to take information about stocks and predict their future prices.\nSo we use machine learning to take historical data and generate a model. When we want to use it, we give the model observations, $\\vec{x_i}$, and it gives us predictions, $y$. Examples of good inputs (predictive factors) are:\n Price momentum Bollinger value Current price  while examples of outputs would be:\n Future price Future return  We’ll first talk about Supervised Regression Learning.\nRegression and Modeling Supervised Regression Learning means that we’ll provide (supervised learning) a bunch of example data $(x, y)_i$ and allow the model to make a numerical prediction (regression). There are two main types of regression techniques:\n Linear regression (parametric) k-nearest neighbor (kNN) (instance-based), the more popular approach Decision trees Decision forests  Assessing a Model Assessing a model is much like predicting prices as it uses indicators to judge the effectiveness of the model. The first indicator is root mean square error (RMSE), which is as follows\n$\\sqrt{\\dfrac{\\sum (ytest - ypredict)^2}{N}}$  The error that is important is that of test data, which is outside of the training data. Typically 60% of the data is used for training, and 40% is used for testing. However, sometimes there isn’t enough data to adequately evaluate a learning algorithm, in which case a method called cross validation is used. This method slices the data into chunks, typically fifths. One is chosen as test and the rest are for training, then a different chunk is chosen to be the test and another trial is run. For financial data, we don’t want to accidentally look forward in time, so we would only use roll forward cross validation. This simply demands that all the training data is before the test data.\nThe second metric for how well an algorithm is working is the correlation of the test data and predicted values. Strong correlation, close to $\\pm1$, indicates a good algorithm whereas a weak correlation, close to zero, indicates a poor algorithm. Correlation and RMSE are excellent indicators on how well an algorithm is doing, but we might also want to fine tune an algorithm; we want to answer the question ”when are we trying too hard to fit data?”. This is where overfitting comes into play. Overfitting is the point at which error for training data is decreasing while error for test data is increasing.\nTypes of Learners Ensemble Learners Ensemble learners are composed of several different learners, which could include kNN, regression, and decision tree learners in one. The output of this learner is then simply a combination of the learners’ answers, which is typically an average of the outputs.\nBagging and Boosting Boot-strap aggregating, or bagging, only uses one algorithm, but many different models. If the training data is separated into learning instances where there’s a total of $n$ instances, then each model is fed a bag of these instances. Each bag is composed of $n\u0026rsquo;$ learning instances that are randomly selected with replacement, so instances may show up more than once in the same bag. These are used to train m models and the result is the average of all the outputs. Boosting builds each subsequent bag based on the results of the last. Training data is also used to test the models, and a model’s predicted data showing significant error is weighted to more likely be in the next bag for the next model. The process is continued for the desired number of models, and the results are averaged. Although this could be advantageous in predicting outliers, it’s also more susceptible to overfitting.\nReinforcement Learning As seen in figure 4.1, reinforcement learning describes the interaction of a robot with its environment. The robot performs an action, which has an effect on the environment and changes its state. The robot observes the change in state and its associated reward and makes decisions to maximize that reward.\nReinforcement learning also describes the problem that is how to go about maximizing the reward. In the stock market, the reward is return on trades, and we want to find out how to maximize returns. This problem is complicated by time constraints. The value of future gains diminishes with time, so it’s unreasonable to use an infinite horizon on which to base returns. However, optimizing returns over too short a time may limit rewards from seeing a much larger overall gain.\nMarkov Decision Problems What we’ve been talking about is called a Markov decision problem. Here’s how the problem is formalized.\nWe have:\n Set of states $S = {s1, \u0026hellip; , s_n}$ Set of actions $A = {a1, \u0026hellip; , a_n}$ Transition function $T[s, a, s_0]$ Reward function $R[s, a]$  What we’re trying to find is a policy $\\pi(s)$ that will maximize the reward. Unfortunately, we don’t know the $T$ or $R$, since that’s defined by the environment. So, the learner has to interact with the world and see what happens. Based on the reward, it can start generating policies.\nA way to encode this information is using experience tuples. Experience tuples are as follows: given a state $s_1$ and an action $a_1$ that we took, we were put into state $s'_1$ and got reward $r_1$. The tuple is shown like this:\n$\\langle s_1, a_1, s_1\u0026#39;, r_1 \\rangle$  Now, we can rename $s_1\u0026rsquo;$ to $s_2$, since that’s our new state, and then take a new action and see what happens, and we get a new tuple:\n$\\langle s_2, a_2, s_2\u0026#39;, r_2 \\rangle$  And we repeat this for many different combinations of states and actions, and then we’ll use the tuples to generate the policy. There are two ways to generate the policy:\n Model-based: For this method, we generate a model of $T[s, a, s_0]$ based on statistical analysis of the tuples. We look at a particular state and a particular action and see the probability of transitioning to another state. Same thing with $R[s, a]$. Then, we can use policy or value iteration to solve it. Model-free: This method keeps the data around and uses the original tuples to determine what the new state will be for a certain action. This is Q-learning.  Q-Learning Q-Learning is a model-free approach, which means that it doesn’t need to have any sort of model of the transition function $T$ or the reward function $R$. It builds a table of utility values as the agent interacts with the world. These are the Q values. At each state, the agent can then use the Q values to select the best action. Q-Learning is guaranteed to give an optimal policy, as it is proven to always converge.\n$Q$ represents the value of taking action $a$ in state $s$. This value includes the immediate reward for taking action $a$, and the discounted (future) reward for all optimal future actions after having taken $a$.\nHow do we use Q? What we want to find for a particular state is what policy, $\\prod(s)$ we should take. Using Q values, all we need to do is find the maximum $Q$ value for that state.\n$\\prod(s) = \\underset{a}{\\arg\\max}(Q[s,a])$  So, we go through each action a and see which action has the maximum $Q$ value for state $s$. Eventually, after learning enough, the agent will converge to the optimal policy, $\\pi^*(s)$, and optimal $Q$ table, $Q^*[s, a]$.\nHow do we get Q? To use the Q table, first we must generate it by learning. How do we go about that? Well it’s similar to previous learning algorithms, in that we provide it training data for which we know the outcomes. We then iterate over time and take actions based on the current policy (Q values), and generate experience tuples $\\langle s, a, s\u0026rsquo;, r \\rangle$ and generate the Q values based on the experience.\nIn a more detailed fashion:\n Initialize the Q table with small random values Compute $s$ Select $a$ Observe $r, s$ Update $Q$ Step forward in time, then repeat from step 2.  To update Q, we first need a formula to decide what it should be. What we can do is assign a learning rate, $\\alpha$, to weight the new observations. We can therefore update $Q$like this:\n$Q\u0026#39;[s, a] = Q[s, a] \u0026#43; \\alpha(\\textrm{improved estimate} - Q[s, a])$  As you can see, $Q$ converges as the improved estimate is the same as the current estimate (we’re at the best $Q$). Now, we need to know what the improved estimate is:\n$\\textrm{improved estimate} = \\textrm{immediate returns} \u0026#43; (\\textrm{discounted future rewards})$  Then, replacing discounted future rewards with the actual way of calculating it, and rearranging to only use the current value of $Q$ once, we find that the formula to calculate the new value of $Q$ for a state-action pair hs, ai, the formula is:\n$Q\u0026#39;[s, a] = (1 - \\alpha)Q[s, a] \u0026#43; \\alpha(r \u0026#43; \\gamma Q[s\u0026#39;, \\underset{a\u0026#39;}{\\arg\\max}(Q[s\u0026#39;, a\u0026#39;])])$  where:\n $r = R[s, a]$ is the immediate reward for taking an action $a$ in the state $s$ $\\gamma \\in [0, 1]$ is the discount factor to reduce the value of future rewards $s\u0026rsquo;$ is the resulting next state $\\underset{a\u0026rsquo;}{\\arg\\max}(Q[s\u0026rsquo;, a\u0026rsquo;])$ is the action which maximizes the Q-value among all possible actions $a\u0026rsquo;$ from $s\u0026rsquo;$, and $\\alpha \\in [0, 1]$ is the learning rate used to vary the weight given to new experiences compared to past Q-values. It’s typically around 0.2.  Exploration The success of a Q-learning algorithm depends on the exploration of the state-action space. If you only explore a small subset of it, you might not find the best policies. One way to ensure that you explore as much as possible is to introduce randomness into selecting actions during the learning phase. So basically, you see first whether you want to take the action with the maximal Q value or choose a random action, then if you take a random action, each action gets a probability which decreases over subsequent iterations.\nQ-Learning for Trading Now that we know what Q-learning is, we need to figure out how to apply it to the context of trading. That means that we need to define what state, action, and reward mean. Actions are straightforward, as there are basically three of them:\n Buy Sell Do Nothing  Our rewards can be daily returns or cumulative returns after a trade cycle (buy→sell). However, using daily returns will allow the agent to converge on a Q value more quickly, because if it waited until a sell, then it would have to look at all of the actions backwards until the buy to get that reward.\nNow, we just need to figure out how to determine state. Some good factors to determine state are:\n Adjusted Close/Simple Moving Average Bollinger Band value P/E ratio Holding stock (whether or not we’re holding the stock) Return since entry  Discretization Our state must be a single number so we can look it up in the table easily. To make it simpler, we’ll confine the state to be an integer, which means we need to discretize each factor and then combine them into an overall state. Our state space is discrete, so the combined value is the overall state. Say we have a state like this: The discretized state could be: 2950.\nTo discretize, what we do is take the data for a factor over its range, then divide it into n bins. Then we find the threshold by iterating over the data by the step size and taking the value at each position.\nstepsize = size(data) / n data.sort() for i in range(0, steps): threshold[i] = data[(i +1) * stepsize]  Problems with Q-Learning One main problem with Q-Learning is that it takes a lot of experience tuples to converge to the optimal Q value. This means the agent has to take many real interactions with the world (execute trades) to learn. The way this has been addressed is by using Dyna.\nDyna-Q Dyna is designed to improve the convergence of Q learning by building a model of $T$ and $R$ and then using Q learning to make decisions based on the model. However, the Q learning portion is still model-free, so it’s a mix of both.\nSo we do the Q-Learning steps, but after we take an action, we update the model of $T$ and $R$ with the new data, simulate a bunch of experiences based on the model, then update $Q$ based on these simulated experiences. To simulate the experiences, we basically generate random states and actions, and then find the new states/rewards based on the transition function and reward function.\nLearning T To figure out a model for $T$, what we can do is count the number of times that a transition to $s\u0026rsquo;$ by using the action $a$ in state $s$ occurred, then divide that by the total number of transitions to figure out the probability where $T_c$ is the number of times the transition occurred.\n$T[s, a, s\u0026#39;] = \\dfrac{T_c[s, a, s\u0026#39;]}{\\sum_{i} T_c[s, a, i]}$  Learning R To finalize the model, we need to find our expected reward, $R[s, a]$. Whenever we interact with the world, we get an immediate reward, $r$. We can use this to update our model for $R$ in a similar way to updating the $Q$ values where $\\alpha$ is again the learning rate:\n$R\u0026#39;[s, a] = (1 - \\alpha)R[s, a] \u0026#43; \\alpha r$  Conclusion So a summary of how Dyna-Q works is the following:\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/6601/",
	"title": "6601",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/7641/",
	"title": "7641",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6476/",
	"title": "6476",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6750/",
	"title": "6750",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6460/",
	"title": "6460",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/",
	"title": "6250",
	"tags": [],
	"description": "",
	"content": " About:  todo  Resources:  todo Reading list:\u0026hellip;  "
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " OMS:Notes About This project has two goals:\n Accelerate the rate of learning for all Provide resources to help those either 1) new to OMSCS or 2) new to a technical domain   If you want to go fast, go alone. If you want to go far, go together. \u0026ndash; AFRICAN PROVERB\n Contributing Contributions are very welcome. This is first and foremost a community effort by OMSCS for OMSCS.\n If you would like to make an occasional edit, please do so with the tab on the relevent page. If you would like to contribute more, please see the contributing guide \u0026ndash; coming soon  If any material here is out of date or can be improved, click on the \u0026ldquo;Improve this page\u0026rdquo; tab.\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]