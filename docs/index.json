[
{
	"uri": "/6250/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction: Welcome to Computer Networking We\u0026rsquo;ll be covering advanced concepts in networking such as software defined networking (SDN), data center networking (DCN) and content distribution. You\u0026rsquo;ll complete projects using a state of the art network emulator called mini-net to understand and explore these advanced concepts leading up to a final project replicating actual networking research.\nComputer Networking Welcome to the graduate course on computer networking. The primary goal of this course is to provide a survey of the necessary tools, techniques, and concepts to perform research in computer communications. This is a project based course, and there will be significant emphasis on hands-on experience. In networking, perhaps more than many other subjects, realization is key. You can read about concepts or techniques in a textbook, but really the most effective way to learn networking is by doing. So, you\u0026rsquo;ll gain a lot of hands on experience in this course through the assignments. In comparison to an introductory networking course which you may have taken, this course will provide more in depth coverage of networking topics, and it will also offer a crash course in some of the available tools that are now available for performing research in computer networking. You will gain experience with many of these tools through the project based assignments in the course.\nTwo Components The course has essentially two components. In the lectures you will learn about cutting edge research problems in computer networking and you\u0026rsquo;ll also gain the ability to come up with your own problems. We\u0026rsquo;ll pick up the basics along the way as necessary. In addition to the lectures there are also a number of problem sets or assignments that you will work through as you work your way through the course. The problem sets and assignments in the course will give you proficiency with the tools and technologies that are state of the art in the research community. That will allow you to follow through on the research ideas that you may come up with as we work through various topics in the course. There are tons of exciting tools to use, and the problem sets and assignments will help you gain proficiency with them.\nWhat the Course is NOT About It\u0026rsquo;s also worth bearing in mind what this course is not about. The course is not an introduction to networking, so there are a number of basic topics that won\u0026rsquo;t be covered in this course. In particular, we\u0026rsquo;ll assume that you\u0026rsquo;re already familiar with the basics of things like TCP, Socket programming, and so forth. Anything that you might have picked up in an introductory networking course, we are just going to assume as a prerequisite for this course. So before you proceed, it may be worth revisiting some of your old undergraduate networking course material. The course is also not providing any introduction to programming. However, many assignments in the course will make use of some amount of programming. So some knowledge of scripting languages like Ruby, Python or Perl will certainly be helpful in some assignments. We\u0026rsquo;ll be making a lot of use of a network emulation toolkit called Mininet, and to use that tool most effectively, you will certainly want to learn some Python if you don\u0026rsquo;t already know it. Don\u0026rsquo;t worry if you don\u0026rsquo;t know these languages already, though. There\u0026rsquo;s plenty of time to learn in the course since the deadlines are fairly spread out. And the assignments aren\u0026rsquo;t focused on knowledge of programming per say, but rather, the concepts that you are going to realize in the programming languages.\nCourse Structure The course is broken into three smaller sub-courses. The first course will cover topics including architectural principles, switching, routing, naming, addressing, and forwarding. The second part of the course will cover congestion control, streaming, rate limiting, and content distribution. And the third part of the course will have modules on software defined networking, traffic engineering, and network security. There will be about three assignments per sub course, plus a final project.\n"
},
{
	"uri": "/7646/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": " Python: Numpy \u0026amp; Pandas Dataframes A dataframe is a data structure in pandas that allows multiple datasets to be mapped to the same indices. For example, a data frame that maps dates to closing prices could be:\n    SPY AAPL GOOG GLD     2000-01-09 101.01 50.89 NaN NaN   2000-01-10 100.05 50.91 NaN NaN   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   2015-12-31 200.89 600.25 559.50 112.37    The indices in this dataframe are the dates on the left, and the closing prices for that date are stored in each column. The ”NaN”s appear because GOOG and GLD were not publicly traded during those periods.\nReading CSVs into Dataframes To begin using the dataframes, you need data first. Historical stock data from Yahoo is provided in the form of a CSV file, which can be easily read into a dataframe using pandas’s function read csv().\nimport pandas as pd def test_run(): df = pd.read_csv(\u0026quot;data/AAPL.csv\u0026quot;) print df if __name__ == \u0026quot;__main__\u0026quot;: test_run()  This example reads in a CSV corresponding to the historic data for AAPL (Apple, Inc) into the variable df. df is a DataFrame object, which means any DataFrame methods may be used on it.\nAn example of a method that can be used is max(), which returns the maximum value in the range.\nimport pandas as pd def get_max_close(symbol): df = pd.read_csv(\u0026quot;data/{}.csv\u0026quot;.format(symbol)) return df ['Close'].max() def test_run(): for symbol in ['AAPL','IBM'] print \u0026quot;Max close \u0026quot; print symbol, get_max_close(symbol) if __name__ == \u0026quot;__main__\u0026quot;: test_run()  Plotting Matplotlib can be used to plot the data in the dataframes, as pandas can conveniently tap into the matplotlib API. Plotting data in a dataframe is as simple as calling plot() on one of the series in the frame.\nExample: Plotting the Adjusted Closea price of AAPL\nimport pandas as pd import matplotlib.pyplot as plt def test_run(): df = pd.read_csv('data/AAPL.csv') print df['Adj Close'] df[['Adj Close', 'Close']].plot() plt.title('Comparison') plt.show() if __name__ == '__main__': test_run()     index AdjClose     0 669.79   1 660.59   2 662.74   3 680.44   4 676.27   \u0026hellip; \u0026hellip;   3173 24.60   3174 24.96    Issues There are some issues with the data that need to be solved to effectively use it in the way we want.\n Trading days: The NYSE only trades for a certain number of days per year, which means that indexing by dates will return some results when the exchanges were not open. This poses problems for trying to pull out certain date ranges from the dataframe. Multiple stocks: One of the dataframe’s powers is to be able to contain multiple ranges, which means that we need to be able to retrieve multiple datasets and store them into the dataframe. Date order: The data in the Yahoo CSV are in reverse chronological order (most recent at the top), so any analysis on the dataframe will be going backwards in time, which is not ideal.  Solution to the issues To solve the trading days problem, we’ll use an Exchange-Traded Fund (ETF) called SPY (S\u0026amp;P 500) to serve as a basis for what days the stock market is open. The only days that exist in the dataset for this ETF are the days the stock market traded, so if we use this as a reference and use joining on the dataframes, we can recover data on only the days which had trading.\nExample: Using joins to get only traded days\nstart_date = '2010-01-22' end_date = '2010-01-26' dates = pd.date_range(start_date, end_date) df1 = pd.DataFrame(index=dates) # build empty dataframe  If we were to print out df1, the output would be:\nEmpty DataFrame Columns : [] Index : [2010-01-22 00:00:00, 2010-01-23 00:00:00, 2010-01-25 00:00:00, 2010-01-26 00:00:00]  This empty dataframe will be the basis for the data we want to retrieve. The next step is to join this dataframe with a dataframe with the data for SPY. This will keep only indices of the SPY dataframe that also exist in the empty one.\nExample: Reading in the new dataframe and joining them\ndfSPY = pd.read_csv(\u0026quot;data/SPY.csv\u0026quot;, index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=[ 'Date','Adj Close'], na_values =['nan']) df1 = df1.join(dfSPY)  The output would now be:\n Adj Close 2010-01-22 104.34 2010-01-23 NaN 2010-01-24 NaN 2010-01-25 104.87 2010-01-26 104.43  To get rid of the ”NaN”s, you can call dropna() on the newly joined dataframe, but there is a better way of joining them such that the ”NaN”s don’t appear in the first place. The join type is called an inner join, which joins at the intersection of the two dataframes. This way, only the dates which are in both will be kept as indices. Everything else will be thrown away.\nExample: The inner join\ndf1 = df1.join(dfSPY, how='inner')  Multiple stocks Reading in multiple stocks is as easy as just adding a for loop:\nExample: Reading in multiple stocks into a single dataframe\ndfSPY = dfSPY.rename(columns={'Adj Close': 'SPY'}) df1 = df1.join(dfSPY, how='inner') symbols = ['GOOG', 'IBM', 'GLD'] for symbol in symbols: df_tmp = pd.read_csv(\u0026quot;data/{}.csv\u0026quot;.format(symbol), index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=['Date', 'Adj Close'], na_values=['nan']) # rename to prevent name clash (multiple columns with same name) df_tmp = df_tmp . rename(columns={'Adj Close': symbol}) df1 = df1.join(df_tmp, how='inner')  Here’s an example of reading and plotting multiple stocks’ closing price on one plot\nExample: Reading and plotting multiple stocks\nimport os import pandas as pd import matplotlib.pyplot as plt def plot_selected(df, columns, start_ind, end_ind): print df.ix[start_ind: end_ind, columns] plot_data(df.ix[start_ind:end_ind, columns]) def symbol_to_path(symbol, base_dir=\u0026quot;data\u0026quot;): return os.path.join(base_dir, \u0026quot;{}. csv\u0026quot;. format(str(symbol))) def get_data(symbols, dates): df = pd.DataFrame(index=dates) if 'SPY' not in symbols: symbols.insert(0, 'SPY') for symbol in symbols: tmp = pd.read_csv(symbol_to_path(symbol), index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=['Date', 'Close'], na_values=['nan']) tmp = tmp.rename(columns={'Close': symbol}) df = df.join(tmp) if symbol == 'SPY': df = df.dropna(subset=['SPY']) return df def plot_data(df, title=\u0026quot; Stock prices\u0026quot;): ax = df.plot(title=title, fontsize=12) ax.set_xlabel(\u0026quot;Date\u0026quot;) ax.set_ylabel(\u0026quot;Closing price\u0026quot;) plt.show() def test_run(): dates = pd.date_range('2010-01-01', '2010-12-31') symbols = ['IBM ', 'GLD '] df = get_data(symbols, dates) plot_selected(df, ['SPY', 'IBM '], '2010-03-01', '2010-04-01') if __name__ == '__main__': test_run()  Normalizing Sometimes when plotting, the values of a stock will be significantly different from the other stocks such that it becomes difficult to tell some of them apart. Normalizing the data allows all of them to start at the same point and then show divergences from the initial point, making it easier to compare them at the same time.\nNormalizing the dataframe is as simple as dividing the entire dataframe by its first row\nExample: Normalizing a dataframe\ndef normalize_data(df): df = df / df.ix[0 ,:] return df  NumPy The actual data in the dataframe is actually an ndarray in NumPy (A multidimensional homogeneous array). That means we can do operations on the data using NumPy. For example, if you have a dataframe df1, the ndarray would be extracted by doing\nnd1 = df1.values  Accessing a cell in the array is as simple as:\nval = nd1[row, col]  You can also access subarrays by indexing with the colon:\nsub = nd1[0:3, 1:3]  would capture the rectangular subarray from the first to the third rows and the second to third columns.\nIndexing Note that the second part of the index is 1 past the actual index that will be the last, so 0:3 only pulls out 0, 1, and 2. Like in MATLAB, you can pull out everything using just the colon. For example:\nsub = nd1[3 ,:]  would retrieve all columns of row 3.\nNegative indexing To get the last index, you can use negative numbers (the last index would be -1, second to last -2, etc.)\nsub = nd1[-1, 1:3]  would get columns 1,2 of the last row\nBoolean indexing/masking Suppose we want to get the values in an array, a, which are all less than the mean. NumPy’s masking feature makes it really intuitive, as all you need to do is:\nlessThanMean = a[a \u0026lt; a.mean()]  The array a \u0026lt; a.mean() would be a boolean array, which might look like\n[[ True, True, False, False]]  Assignment Assigning values in an array is easy using the NumPy notation. For example, say we wanted to replace the values in the first 2x2 square of nd1 with the 2x2 square in nd2 with columns 2 and 3, and rows 3, and 4. The operation would be:\nnd1[0:2, 0:2] = nd2[-2:, 2:4]  Creating an array Creating a numpy array is as easy as passing in a normal python list into the array method:\nimport numpy as np print np.array([1, 2, 3])  Creating a 2D m x n array is as simple as passing in a m-long list of n-tuples.\nprint np.array([(1, 2, 3), (4, 5, 6)])  would output\n[[1 ,2 ,3] [4 ,5 ,6]]  More initializers You can also create arrays with certain initial values.\nnp.empty((5, 3, 2))  initializes an ”empty” 5x3x2 dimensional array. The values in the array are actually whatever was in the memory locations of the array pointers, so the output could look like garbage.\nnp.ones((5, 4), dtype=np.int)  creates a 5x4 array, where the value in each cell is the integer 1.\nnp.random.random((5, 4))  creates a 5x4 array with random numbers from a uniform distribution in [0.0,1.0). An example result could be:\n[[ 0.82897637 0.36449978 0.91209931 0.96307279] [ 0.63777312 0.24482194 0.5817991 0.18043012] [ 0.85871221 0.98874123 0.68491831 0.53831711] [ 0.52908238 0.81083147 0.97440602 0.81032768] [ 0.98566222 0.38902445 0.16922005 0.0873198 ]]  Other methods or fields, such as sum() or size() can be looked up in online documentation.\n"
},
{
	"uri": "/42/",
	"title": "42",
	"tags": [],
	"description": "",
	"content": " About Georgia Tech\u0026rsquo;s online Master of Science in Computer Science (OMS CS) comprises a curriculum of courses taught by the world-class faculty in the Georgia Tech College of Computing.\nGetting Started TODO: Resources/guidance for new students\u0026hellip;\nCommunities  Slack Reddit Facebook  Resources:  OMSCS FAQ: here and here Course Reviews Awesome-OMSCS Repo  "
},
{
	"uri": "/6250/architecture-and-principles/",
	"title": "Architecture and Principles",
	"tags": [],
	"description": "",
	"content": " Architecture \u0026amp; Principles We\u0026rsquo;ll begin our foray into networking by reviewing the history of the internet and its design principles. Networking today is an eclectic mix of theory and practice in large part because the early internet architects set out with clear goals and allowed flexibility in achieving them.\nWith all that flexibility, does that mean we\u0026rsquo;ll see the rollout of IPv6 soon? Only in your dreams.\nA Brief History of the Internet In this lesson we will cover a brief history of the internet. The internet has its roots in the ARPA Net which was conceived in 1966 to connect big academic computers together. The first operational ARPA Net nodes came online in 1969 at UCLA, SRI, UCSB, and Utah. Around the same time, the National Physical Laboratory in the UK also came online. By 1971 there were about 20 ARPANet Nodes and the first host-to-host protocol. There were two cross country links, and all of the links were at 50 KBPS.\nHere is a rough sketch of the ARPANet as drawn by Larry Roberts in the late 1960s. You can see the four original Nodes here, as well as some other well known players such as Berkeley, the MAC project at MIT, BBN, Harvard, Carnegie-Mellon, Michigan, Illinois, Dartmouth, Stanford, and so forth. This is what the ARPANET looked like in the late 1960s.\nHere\u0026rsquo;s a picture of the ARPANET in June 1974. And you can see not only some additional networks that have come online, but also a diagram of the machines that are connected at each of the universities. You can also see a connection here between the ArpaNet and MPLnet. Of course, the ArpaNet wasn\u0026rsquo;t the only network. There were other networks at the time. Sat Net operated over satellite. There were packet radio networks, and there were also Ethernet local area networks. Work started in 1973 on replacing the original network control protocol with TCP/IP where IP was the Internetwork Protocol and TCP was the Transmission Control Protocol.\nTCP/IP was ultimately standardized from 1978 to 1981 and included in Berkley UNIX in 1981. And on January 1st, 1983 the internet had one of its flag days, where the ArpaNet transitioned to TCP/IP. Now the internet continued to grow, but the number of computers on the internet really didn\u0026rsquo;t start to take off until the mid 90s. You can see here that around August 1995 there were about 10 million hosts on the internet, and five years later there was an order of magnitude more hosts on the internet—more than 100 million. During this period the Internet experienced a number of technical milestones. In 1982 the internet saw the rollout of the domain name system which replaced the host.txt file containing all the world\u0026rsquo;s machine names with a distributed name lookup system. 1988 saw the rollout of TCP Congestion Control after the net suffered a series of congestion collapses. 1989 saw the NSF net and BGP inter-domain routing including support for routing policy. The 90s, on the other hand, saw a lot of new applications. In approximately 1992 we started to see a lot of streaming media including audio and video. Web was not soon after, in 1993, which allowed users to browse a mesh of hyperlinks. The first major search engine was Altavista, which came online in December of 1995, and peer to peer protocols and applications including file sharing, began to emerge around 2000.\nProblems and Growing Pains Now, today\u0026rsquo;s internet is experiencing considerable problems and growing pains, and it\u0026rsquo;s worth bearing some of these in mind and thinking about them, as many of them give rise to interesting research problems to think about as we work through the material in the course. One of the major problems is that we\u0026rsquo;re running out of addresses. The current version of the internet protocol, IPV4, uses 32-bit addresses, meaning that the IPV4 internet only has 2 to the 32 IP addresses, or about 4 billion IP addresses. Furthermore, these IP addresses need to be allocated hierarchically and many portions of the IP address space are not allocated very efficiently. For example, the Massachusetts Institute of Technology has one two fifty sixth of all the Internet address space. Another problem is congestion control. Now congestion control\u0026rsquo;s goal is to match offered load to available capacity. But one of the problems with today\u0026rsquo;s congestion control algorithms is that they have insufficient dynamic range. They don\u0026rsquo;t work very well over slow and flaky wireless links and they don\u0026rsquo;t work very well over very high speed intercontinental paths. Now, some solutions exist but change is hard and all solutions that are deployed must interact well with one another. And deployment in some sense requires some amount of consensus. A third major problem is routing. Routing is the process by which those on the internet discover paths to take to reach another destination. Today\u0026rsquo;s interdomain routing protocol, BGP, suffers a number of ills, including a lack of security, ease of misconfiguration, poor convergence, and non-determinism. But it sort of works and it\u0026rsquo;s the most critical piece of the internet infrastructure in some sense because it\u0026rsquo;s the glue that holds all of the internet service providers together. Another major problem in today\u0026rsquo;s internet is security. Now while we\u0026rsquo;re reasonably good at encryption and authentication, we are not actually so good at turning these mechanisms on. And we\u0026rsquo;re pretty bad at key management, as well as deploying secure software and secure configurations. The fifth major problem is denial of service. And the internet does a very good job of transmitting packets to a destination even if the destination doesn\u0026rsquo;t want those packets. This makes it easy for an attacker to overload servers or network links to prevent the victim from doing useful work. Distributed denial of service attacks are particularly commonplace on today\u0026rsquo;s Internet. Now, the thing that all of those problems have in common is that they all require changes to the basic\ninfrastructure, and changing basic infrastructure is really difficult. It\u0026rsquo;s not even clear what the process is to achieve consensus on changes. So as we work our way through the course, it will be interesting to see the problems that we encounter in each of these areas, various solutions that have been proposed, and also to think about ways in which new protocols and technologies can be deployed. In later parts of the course we\u0026rsquo;ll learn about a new technology called software defined networking, or SDN. That makes it easier to solve some of these problems by rolling out new software technologies, protocols, and other systems to help manage some of these issues.\nArchitectural Design Principles In this lecture we will talk about the Internet\u0026rsquo;s original design principles. These design principles were discussed in the paper reading for today, the Design Philosophy of the DARPA Internet Protocols, by Dave Clark, dated 1988. The paper has many important lessons, and we will go through many of them as we revisit many of the design decisions. Before we jump into any details let\u0026rsquo;s talk about some of the high level lessons. One of the most important conceptual lessons is that the design principles and priorities were designed for a certain type of network. And as the internet evolves, we are feeling some of the growing pains of some of those choices. In the last lesson we talked about a number of the problems and growing pains of the internet. And it\u0026rsquo;s worth bearing in mind that many of the problems that we are seeing now, are a result of some of the original design choices. Now that\u0026rsquo;s not to say that some of these design choices are right or wrong, but rather that they simply reflect the nature of our understanding at the time, as well as the environment and constraints that the designers faced for the particular network that existed at that time. Now needless to say, some of the technical lessons from the original design have turned out to be fairly timeless. One concept is packet switching, which we will discuss in this lesson. And another is the notion of fate sharing, or soft state, which we will discuss in a subsequent lesson in the course.\nGoal The fundamental design goal of the internet was multiplexed utilization of existing interconnected networks. There are two important aspects to this goal. One is multiplexing or sharing. So one of the fundamental challenges that the internet technologies needed to solve was the shared use of a single communications channel. The second major part of this fundamental goal is the interconnection of existing networks. These two sub problems had two very important solutions. Statistical multiplexing, or packet switching, was invented to solve the sharing problem, and the narrow waist was designed to solve the problem of interconnecting networks. Let\u0026rsquo;s talk about each of these now in turn. We\u0026rsquo;ll first talk about packet switching\nPacket Switching In packet switching, the information for forwarding traffic is contained in the destination address of every datagram or packet. Similar to how you would write a letter and specify the destination to where you want the letter sent, and that letter might wend its way through multiple intermediate post offices en-route to the recipient, packet switching works much the same way. There is no state established ahead of time, and there are very few assumptions made about the level of service that the network provides. This assumption about the level of service that the network provides, is sometimes called best effort. So how does packet switching enable sharing? Just as if you were sending a letter, many senders can send over the same network at the same time, effectively sharing the resources in the network. A similar phenomenon occurs in packet switching when multiple senders send network traffic or packets over the same set of shared network links. Now this is in contrast to the phone network, where if you were to make a phone call, the resources for the path between you and the recipient are dedicated and are allocated until the phone call ends. The mode of switching that the conventional phone network uses is called circuit switching, where a signaling protocol sets up the entire path, out-of-band. So this notion of packet switching and statistical multiplexing, allowing multiple users to share a resource at the same time, was really revolutionary. And it is one of the underlying design principles of the internet that has persisted. Now, an advantage of statistical multiplexing of the links and the network means that the sender never gets a busy signal. The drawbacks include things like variable delay and the potential for lost or dropped packets. In contrast, circuit switching provides resource control, better accounting and reservation of resources, and the ability to pin paths between a sender and receiver. Packet switching provides the ability to share resources and potentially better resilience properties.\nPacket Switching vs Circuit Switching Quiz Let\u0026rsquo;s take a quick quiz on packet switching versus circuit switching. Which of the following are characteristics of packet switching and circuit switching: variable delay, busy signals, sharing of network resources like an end-to-end path among multiple recipients, and dedicated resources between the sender and receiver? Each of these options only has one correct answer.\nPacket Switching vs Circuit Switching Solution Variable delay is a property of statistical multiplexing, or packet switching. Circuit switch networks can have busy signals. Packet switch networks share network resources. And circuit switch networks typically have dedicated resources along a path between the sender and receiver\nNarrow Waist Let\u0026rsquo;s now take a look at the second important fundamental design goal on the internet, interconnection, and how interconnection is achieved with the design principle called the Narrow Waist. Let\u0026rsquo;s keep in mind that one of the main goals was to interconnect many existing networks, and to hide the underlying technology of interconnection from applications. This design goal was achieved using a principle called the narrow waist. The internet architecture has many protocols that are layered on top of one another. At the center is an interconnection protocol called IP, or the internet protocol. Now every internet device must speak IP or have an IP stack. Given that a device implements the IP stack, it can connect to the internet. This layer of the network is sometimes called the network layer. Now this layer provides guarantees to the layers above. On top of the network layer sits the transport layer. The transport layer includes protocols like TCP and UDP. The network layer provides certain guarantees to the transport layer. One of those guarantees is end to end connectivity. For example, if a host has an IP address, then the network layer, or IP, provides the guarantee that a packet with that host destination IP address should reach the destination with the corresponding address with best effort. On top of the transport layer sits the application layer. The application layer includes many protocols that various internet applications use. For example, the web uses a protocol called the hypertext transfer protocol or HTTP. And mail uses a protocol called SMTP or simple mail transfer protocol. Transport layer protocols provide various guarantees to the application layer including reliable transport or congestion control. Now below the network layer we have other protocols. The link layer provides point-to-point connectivity, or connectivity on a local area network. A common link layer protocol is Ethernet. Below that, we have the physical layer, which includes protocols such as sonnet or optical networks and so forth. The physical layer is sometimes called layer 1. The link layer is sometimes called layer 2 and the network layer is sometimes called layer 3. We tend to not refer to layers above the network layer by number. The most critical aspect of this design is that the network layer essentially only has one real protocol in use, and that\u0026rsquo;s IP. That means that every device on the network must speak IP, but as long as the device speaks IP it can get on the internet. This is sometimes called IP over anything, or anything over IP, now the advantage to the narrow waist, as I mentioned, is that it is fairly easy to get a device on the network if it runs IP, but the drawback is that because every device is running IP, it\u0026rsquo;s very difficult to make any changes at this layer. However, people are trying to do so, and later in the course, when we discuss software defined networking, we will explore how various changes are being made to both the IP layer, and other layers that surround it.\nGoals: Survivability So we talked about how the internet satisfies the goals of sharing and interconnection and now let\u0026rsquo;s talk about some of the other goals that are discussed in the DARPA Design Philosophy Paper. As we discuss some of these other goals it\u0026rsquo;s worth considering and thinking about how well the current internet satisfies these other design goals in the face of evolving applications, threats, and other challenges. One of the goals discussed is survivability, which states that the network should continue to work if even some devices fail, are comprised, and so forth. There are two ways to achieve survivability. One is to replicate. So one could keep state at multiple places in the network, such that when any node crashes there\u0026rsquo;s always a replica or hot standby waiting to take over for the failure. Another way to design the network for survivability is to incorporate a concept called fate sharing. Fate sharing says that it\u0026rsquo;s acceptable to lose state information for some entity, if that entity itself is lost. For example, if a router crashes all of the state on the router, such as the routing tables, are lost. If we can design the network to sustain these types of failures, where the state of a particular device shares the fate of the device itself, then we can withstand failures better. So fate sharing makes it easier to withstand complex failure scenarios and engineering is also easier. Now it\u0026rsquo;s worth asking whether the current internet still satisfies the principle of fate sharing. In a subsequent lesson, we\u0026rsquo;ll talk about network address translation and how it violates the notion of fate sharing. There are other examples where the current internet\u0026rsquo;s design violates fate sharing and it\u0026rsquo;s worth thinking about those.\nHeterogeneity The internet supports heterogeneity through the TCP/IP protocol stack. TCP/IP was designed as a monolithic transport, where TCP provided flow control and reliable delivery, and IP provided universal forwarding. Now it became clear that not every application needed reliable, in-order delivery. For example, streaming voice and video often perform well, even if not every packet is delivered. And the domain name system, which converts domain names to IP addresses, often also doesn\u0026rsquo;t need completely reliable, in-order delivery. Fortunately, the narrow waste of IP allowed the proliferation of many different transport protocols, not just TCP. The second way that the internet\u0026rsquo;s design accommodates Heterogeneity is through a best-effort service model, whereby the network can lose packets, deliver them out of order, and doesn\u0026rsquo;t really provide any quality guarantees. It also doesn\u0026rsquo;t provide information about failures, performance, et cetera. On the plus side, this makes for a simple design, but it also makes certain kinds of debugging and network management more difficult.\nDistributed Management Another goal of the internet was distributed management. And there are many examples where distributed management has played out. In addressing, we have routing registries. For example, in North America we have ARIN, or the American Registry for Internet Numbers. And in Europe that same organization is called RIPE. DNS allows each independent organization to manage its own names and BGP allows each independently operated network to configure its own routing policy. This means that no single entity needs to be in charge and thus allows for organic growth and stable management. On the downside, the internet has no single owner or responsible party. And as Clark said, some of the most significant problems with the internet relate to the lack of sufficient tools for distributed management, especially in the area of routing. In such a network where management is distributed it can often be very difficult to figure out who or what is causing a problem, and worse, local action such as misconfiguration in a single local network can have global effects. The other three design goals that Clark discusses are cost effectiveness, ease of attachment, and accountability. It\u0026rsquo;s reasonable to argue that the network design is fairly cost effective as is and current trends are aiming to exploit redundancy even more. For example, we will learn about content distributions and distributed web caches that aim to achieve better cost effectiveness for distributing content to users. Ease of attachment was arguably a huge success. IP is essentially plug and play. Anything with a working IP stack can connect to the internet. There\u0026rsquo;s a really important lesson here, which is that if one lowers the barrier to innovation, people will get creative about the types of devices and applications that can run on top of the internet. Additionally, the narrow waist of IP allows the network to run on a wide variety of physical layers ranging from fiber, to cable, to wireless and so forth. Accountability, or the ability to essentially, bill, was mentioned in some of the early papers on TCP/IP but it really wasn\u0026rsquo;t prioritized. Datagram networks can make accounting really tricky. Phone networks had a much easier time figuring out how to bill users. Payments and billing on the internet are much less precise, and we\u0026rsquo;ll talk about these more in later lectures.\nWhat\u0026rsquo;s Missing It\u0026rsquo;s also worth noting what\u0026rsquo;s missing from Clark\u0026rsquo;s paper. There\u0026rsquo;s no discussion of security. There\u0026rsquo;s no discussion of availability. There\u0026rsquo;s no discussion of mobility or support for mobility. And there\u0026rsquo;s also no mention of scaling. There are probably a lot of other things that are missing and it\u0026rsquo;s worth thinking about on your own, some of the other things that current internet applications demand, that are not mentioned in Clark\u0026rsquo;s original design paper.\nDARPA Paper Quiz So as a quick quiz, can you quickly check all of the design goals in the list that were mentioned in Clark\u0026rsquo;s original design goals paper? Security, support for heterogeneity, support for interconnection, support for sharing and support for mobility.\nDARPA Paper Solution Clark\u0026rsquo;s original design goals, paper, mentions the need to support heterogeneity, interconnection and sharing.\nEnd-to-End Argument In this lesson, we\u0026rsquo;ll cover the End-to-End Argument as discussed in the paper, End-to-End Arguments in System Design by Saltzer, Reed, and Clark in 1981. In a nutshell, the End-to-End Argument reads as follows, \u0026ldquo;The function in question can completely and correctly be implemented only with the knowledge and application standing at the end points of the communication system. Therefore, providing that questioned function as a feature of the communication system itself is not possible.\u0026rdquo; Essentially, what the argument says is that the intelligence required to implement a particular application on the communication system should be placed at the endpoints, rather than in the middle of the network. Commonly used examples of the end-to-end argument include error handling and file transfer, encrypting end-to-end versus hop-by-hop in the network, and the partition of TCP and IP of error handling, flow control, and congestion control. Sometimes the end-to-end argument is summarized as, \u0026ldquo;the network should be dumb and minimal and the end points should be intelligent.\u0026rdquo; Many people argue that the end- to-end argument allowed the internet to grow rapidly, because innovation took place at the edge in applications and services, rather than in the middle of the network, which can be hard to change sometimes. Let\u0026rsquo;s look at one example of the end-to-end argument, error handling in file transfer.\nFile Transfer Let\u0026rsquo;s suppose that computer A wants to send a file to computer B. The file transfer program on A asks the file system to read the file from the disk. The communication system then sends the file, and finally the communication system sends the packets. On the receiving side, the communication system gives the file to the file transfer program on B, and that file transfer program asks to have the file written to disk. So what can go wrong in this simple file transfer setup? Well, first, reading and writing from the file system can result in errors. There may be errors in breaking up and reassembling the file. And, finally, there may be errors in the communication system itself. Now, one possible solution is to ensure that each step has some form of error checking, such as duplicate copies, redundancy, time out and retry, so forth. One might even do packet error checking at each hop of the network. One could send every packet three times. One might acknowledge packet reception at each hop along the network. But the problem is that none of these solutions are complete. They still require application level checking. Therefore it may not be economical to perform redundant checks at different layers and at different places of this particular operation. Another possible solution is an end-to-end check and retry where the application commits or retries based on the check sum of the file. If errors along the way are rare, this will most likely finish on the first try. Now, this is not to say that we shouldn\u0026rsquo;t take steps to correct errors at any one of these stages. Error correction at lower levels can sometimes be an effective performance booster. And the trade off here is based on performance, not correctness. So whether or not one should implement additional correctness checks at these layers depends on whether or not the amount of effort put into the reliability gains are worth the extra trouble. Another example where the intend argument applies is with encryption, where keys are maintained by the end applications, and cipher text is generated before the application sends the message across the network. Now one of the key questions in the end-to-end argument is identifying the ends. The end-to-end argument says that the complexity should be implemented at the ends but not in the middle, but the ends may vary depending on what the application is. So for example, if the application or protocol involves Internet routing, the ends may be routers, or they might be ISPs. If the application or protocol is a transport protocol, the ends might be end hosts. So, identifying the ends in the end-to-end argument is always a thorny question that you have to answer first.\nEnd-to-End Argument Violations Now, when talking about the end-to-end argument, it is worth remembering that the end-to-end argument is just that. It\u0026rsquo;s an argument. Not a theorem, or a principle, or a law. And there are many things that have come to violate the end-to-end principle. Network address translators, which we\u0026rsquo;ll talk about in the next lesson, violate the end-to-end argument. VPN tunnels, which tunnel traffic between intermediate points on a network, violate the end-to-end argument. Sometimes TCP connections are split at an intermediate node along an end-to-end path, particularly when the last hop of the end-to-end path is wireless. This is sometimes done to improve the performance of the connection because loss on the last hop lossy wireless hop may not necessarily reflect congestion, and we don\u0026rsquo;t necessarily want TCP to react to losses that are not congestion related. Even spam, in some sense, is a violation of the end-to-end argument. For e-mail the end user is generally considered to be a human, and by the end-to-end argument, the network should deliver all mail to the user. Does this mean that spam control mechanisms are in violation of end-to-end, and if so are these violations appropriate? What about peer to peer systems where files are exchanged between two nodes on the Internet but are assembled in chunks that are often traded among peers? What about caches, and in-network aggregation? So, when considering the end-to-end argument, it\u0026rsquo;s worth asking whether or not the argument is still valid today and in what cases. There are questions about what\u0026rsquo;s in versus out, certainly, and what functions belong in the dumb minimal network. For example, routing is currently in the dumb minimal network. Do we really believe that it belongs? What about multicast? Mobility quality of service? What about NAT\u0026rsquo;s? And it\u0026rsquo;s worth considering whether the end-to-end argument is constraining innovation of the infrastructure by preventing us from putting some of the more interesting or helpful functions inside the network. In the third course, we will talk about software defined networking, which in some sense reverses many aspects of this end-to-end argument.\nViolation NAT Part 1 A fairly pervasive violation of the end-to-end argument are home gateways, which often perform something called network address translation. Now on a home network we have many devices that connect to the network, but when we buy service from our internet service provider we\u0026rsquo;re typically only given one public IP address. And yet we have a whole variety of devices that we may want to connect. Now the idea behind network address translation is that we can give each of these devices a private IP address and there are designated regions of the IP address space that are for private IP addresses. One of those is 192.168.0.0/16 and there are others, which you can go read about in RFC 3130. Each one of these devices in the home gets its own private IP address. The public internet, on the other hand, sees a public IP address which typically is the IP address provided by the internet service provider. When packets traverse the home router, which is often running a network address translation process, the source address of every packet is rewritten to the public IP address. Now when traffic comes back to that public IP address, the network address translator needs to know which device behind the NAT the traffic should be sent to. So it uses a mapping of port numbers to identify which device the return traffic should be sent to in the home network. So the NAT or the network address translator maintains a table that says packets with the source IP address of 192.168.1.51 and source port 1000 should be rewritten to a source address of the public IP address and a source port of 50878. Similarly, packets with a source IP address of 192.168.1.52 and source port of 1000 should be rewritten to the public IP address and a source port of 50879. Then when traffic returns to the NAT to one of these addresses the NAT knows that it needs to rewrite the destination address on the return traffic to the appropriate destination IP address and port that\u0026rsquo;s in the private network. So for outbound traffic, the NAT device creates a table entry mapping the computer\u0026rsquo;s local IP address and port number to the public IP address at a different port number and replaces the sending computer\u0026rsquo;s non-routable IP address with the gateway or the NAT public IP address. It also replaces the sender\u0026rsquo;s source port with a different source port that allows it to de-mutiplex the packets sent to this return address and port. For inbound traffic to the home network, the NAT checks the destination port on the packet, and based on the port, it rewrites the destination IP address and port to the private IP address in the table before forwarding the traffic to a local device in the home network.\nViolation NAT Part 2 Now the NAT clearly violates the end-to-end principle, because machines behind the NAT are not globally addressable, or routable, and other hosts on the public Internet cannot initiate inbound connections to these devices behind the NAT. Now there are ways to get around this, there\u0026rsquo;re various protocols. One is called STUN, or signaling and tunneling through UDP-enabled NAT devices. And in these types of protocols, the device sends an initial outbound packet somewhere, simply to create an entry in the NAT table and once that entry is created we now have a globally routable address and port to which devices on a public Internet can send traffic. Now these devices somehow have to learn that public IP address and port that corresponds to that service and this might be done using DNS for example. It\u0026rsquo;s also possible to statically configure these tunnels or mappings on your NAT device at home. Needless to say, even with these types of hacks and workarounds for NAT, it\u0026rsquo;s clear that network address translation is a violation of the end-to-end principle because by default two hosts on the Internet, one on the home network and one on the public Internet, cannot communicate directly by default.\n"
},
{
	"uri": "/7646/statistical-analysis-of-time-series-data/",
	"title": "Statistical Analysis of Time Series Data",
	"tags": [],
	"description": "",
	"content": " Statistical Analysis of Time Series Data Pandas makes it simple to perform statistical analysis on dataframes, which is extremely important in determining different indicators and acting as inputs to the learning algorithms.\nGlobal statistics For example, if you had a dataframe df1 which had the closing prices for various stocks over a given time period, you can retrieve an ndarray with the mean of the columns by just calling df1.mean().\nFigure 2.1: Example output array for mean() (called on closing prices for January 2010 through December 2012)\nIn addition to mean, there around 32 other global statistics that are available in pandas.\nRolling statistics Instead of doing analysis on the entire dataset, you might want to do a rolling analysis, which only looks at certain snapshots of the data to sample. For example, you could have a 20-day moving mean, which you would calculate day-by-day by averaging the last 20 days’ data. In later sections, this moving average will be explained in more detail, but some critical points of interest are when the moving average crosses the data.\nBollinger bands Some analysts believe that significant deviations from the moving mean will result in movement back towards the mean. If the price dips far below the mean, then it might be a buy signal, whereas if it goes too high, it could indicate a time to sell. Bollinger bands are a way of measuring this deviation.\nBollinger observed that if you look at the volatility of the stock, and if it’s too large, then you discard the movements above and below the mean, but if it’s not, then it might be worth paying attention to.\nWhat he did was place two new moving means, one $2\\sigma$ above, and another $2\\sigma$ below the moving average. If you look at deviations near to $2\\sigma$, then they’re worth paying attention to. If the price drops below $2\\sigma$, and then rises back up through it, then it could be a buy signal. (the price is moving back towards the average).\nConversely, if the price rises above $2\\sigma$, then falls back down, it could be a sell signal.\nComputing rolling statistics in pandas Pandas provides some methods to easily calculate rolling mean (rolling mean()) and rolling standard deviation (rolling_std()).\nExample: Calculating a 20-day rolling mean\nrm_SPY = pd.rolling_mean(df['SPY'], window=20)  The Bollinger bands are calculated as follows:\ndef get_bollinger_bands(rm, rstd): return rm + 2*rstd, rm - 2*rstd  Daily returns Daily returns are how much a stock’s price went up or down on a given day. They are an extremely important statistic as they can be a good comparison between different stocks.\n$dailyReturn[t] = \\dfrac{price[t] - price[t - 1]}{price[t - 1]} = \\dfrac{price[t]}{price[t - 1]} - 1$  daily_ret = (df / df.shift(1).values) - 1 daily_ret.ix[0,:] = 0  Cumulative Returns Cumulative return is calculated by finding the gain from the beginning of the range to the current time, i.e.\n$CumlativeReturn[t] = \\dfrac{price[t]}{price[0]} - 1$  For example, if the price at the beginning was \\$125, and the current price is \\$142, then the gain/cumulative return is $\\dfrac{142}{125} - 1 = .136 = 13.6\\%$\nCumulative returns are essentially the original dataset normalized.\nIncomplete data People assume that financial data is extremely well-documented and that perfect data is recorded minute by minute. They also believe that there are no gaps or missing data points. However, for any particular stock, it might have different prices on different stock exchanges! It’s difficult to know who’s right all the time. Also, not all stocks trade every day (they might suddenly start trading or stop trading).\nYou might think you can just interpolate the data between breaks, but that’d cause statistical errors and a side-effect of ”looking into the future” when doing analysis on that subset of data. The better way of doing it to minimize error is to fill forward and backwards.\nFilling To fix the ”NaN”/empty data, you can use filling to maintain the last known value until known data is reached. For example, if you had a stock that didn’t have data until 2001 and then stopped having data in 2006 but then started having data again in 2012, you could fill forward from 2006-2012 and then fill backwards from 2001 back to whenever you want your data to start.\nExample: Filling in missing data using fillna()\ndf = get_data(symbols, dates) df.fillna(method=\u0026quot;ffill\u0026quot;, inplace=True) df.fillna(method=\u0026quot;bfill\u0026quot;, inplace=True)  Histograms and scatter plots It’s difficult to draw conclusions directly from daily returns plots, so histograms make it easier to see what’s going on. A histogram allows you to see how many occurrences of each return happens relative to other returns. This histogram typically follows a Gaussian over large periods of time.\nHistograms From the histogram we can determine a few key statistics: mean, standard deviation, and kurtosis. Kurtosis is a measure of how close the curve is to a Gaussian. In stock data, there are usually more occurrences at high deviations (causing sort of ”fat tails”), which would be reflected as a positive kurtosis. Skinny tails would mean a negative kurtosis.\nExample: Getting a histogram The histogram above was generated by just calling hist() on the daily returns dataframe as such:\ndaily_returns.hist(bins=20)  The bins parameter is essentially the resolution of the histogram. The domain is divided into 20 bins and anything within those bins counts for that bin’s count in the histogram. Other statistics like mean and standard deviation are easily calculated:\nmean = daily_returns['SPY'].mean() print \u0026quot; mean =\u0026quot;, mean std = daily_returns['SPY'].std() print \u0026quot;std deviation =\u0026quot;, std  Which outputs:\nmean = 0.000509326569142 std deviation = 0.0130565407719  We can now plot the mean and standard deviations on the plot to make analysis easier:\nplt.axvline(mean, color='w', linestyle='dashed', linewidth=2) plt.axvline(mean+std, color='r', linestyle='dashed', linewidth=2) plt.axvline(mean-std, color='r', linestyle='dashed', linewidth=2)  Showing this:\nprint daily_returns.kurtosis() \u0026gt;\u0026gt;\u0026gt; SPY 3.376644  which means that the data has fat tails since it’s positive.\nThe utility of these histograms comes when plotting them together. It’s easy to compare multiple stocks in terms of their returns and volatility. If stock A’s curve is skewed more positive and is thinner than stock B, then it has a low volatility with higher returns vs stock B.\nBy looking at this chart, you can see that SPY and XOM are about the same in volatility\n$\\sigma_{SPY} = 0.013057$, $\\sigma_{XOM} = 0.013647$. However, SPY would have higher returns since $R_{SPY} = 0.000509$ whereas $R_{XOM} = 0.000151$\nScatter plots Scatter plots are another way of visualizing the correlation between two stocks. Say you had a dataframe with the daily returns for SPY and XYZ. If you took just the ndarray containing the y-axis values, and then plotted SPY on the x-axis and XYZ on the y-axis, you would see a bunch of points that might have a certain trend.\nIf you take a linear regression of this data, the slope would be called the beta ($\\beta$) value. If the $\\beta$ value for SPY and XYZ is 1, it means that, on average, if SPY (the market) moves up by 1%, then XYZ also moves up by 1%.\nThe y-intercept of the line is called $\\alpha$. It describes how the stock on the y-axis performs with respect the stock on the x-axis. If the $\\alpha$ value of XYZ with respect to SPY is positive, then, on average, XYZ is returning more than the market overall.\nCorrelation: If there isn’t any correlation in the dataset, then the linear regression doesn’t tell you anything about the relationship. A common method for calculating the correlation is by finding the sample Pearson correlation coefficient, $r_{xy}$. It’s calculated by the following:\n$r_{xy} = \\dfrac{cov(X, Y)}{\\sigma_X\\sigma_Y} = \\dfrac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 }\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$  where $cov$ is the covariance. In this case, $X$ would be the daily return for SPY and $Y$ would be the daily return for XYZ. If $|r X,Y| = 1$, then the two are perfectly correlated (either positively or negatively, depending on the sign of $\\rho$). If $|r| \u0026lt; 1$, then there is possible correlation, but a value closer to 1 means better correlation. If $r = 0$, there is no correlation.\nExample: Plotting scatter plots, getting $\\alpha$ and $\\beta$ values, and determining correlation\n# scatter plot for XOM vs SPY daily_ret.plot(kind='scatter', x='SPY', y='XOM', title=\u0026quot;Scatterplot\u0026quot;) beta_XOM,alpha_XOM = np.polyfit(daily_ret['SPY'],daily_ret['XOM'],1) plt.plot(daily_ret['SPY'], beta_XOM*daily_ret['SPY'] + alpha_XOM,'-',color='r') plt.show() # scatterplotforGLDvsSPY daily_ret.plot(kind='scatter', x='SPY', y='GLD', title=\u0026quot;Scatterplot\u0026quot;) beta_GLD, alpha_GLD = np.polyfit(daily_ret['SPY'], daily_ret['GLD'], 1) plt.plot(daily_ret['SPY'], beta_GLD*daily_ret['SPY'] + alpha_GLD, '-', color='r') plt.show() print \u0026quot;BetaXOM: \u0026quot;, beta_XOM print \u0026quot;AlphaXOM: \u0026quot;, alpha_XOM print \u0026quot;BetaGLD: \u0026quot;, beta_GLD print \u0026quot;AlphaGLD: \u0026quot;, alpha_GLD # calculate correlation using pearson method print \u0026quot;Correlationmatrix:\\n\u0026quot;, daily_ret.corr(method='pearson')  Which results in:\nBeta XOM: 0.85753872112 Alpha XOM: -0.000285580653638 Beta GLD: 0.0663816850634 Alpha GLD: 0.000660583984316 Correlation matrix: SPY XOM GLD SPY 1.000000 0.820423 0.074771 XOM 0.820423 1.000000 0.079401 GLD 0.074771 0.079401 1.000000  Looking at the $\\beta$ values, you can see that XOM is more responsive to market changes, while GLD is relatively unresponsive. However, GLD tends to perform better than the market on average, since its $\\alpha$ is positive.\nBut these values are meaningless without seeing what their correlations are. Looking at the correlation matrix, XOM is pretty well correlated with SPY, whereas GLD has a very low correlation, so changes GLD aren’t really correlated with changes in the market.\nLooking at the plots, it’s easy to see that the points for XOM are more correlated and match the line better than do those of GLD.\nSharpe ratio and other portfolio statistics The portfolio is the collection of all stocks currently owned by a person. It’s important to know various statistics associated with the portfolio to make informed decisions on what to sell/buy.\nSuppose you begin with a portfolio p consisting of the following parameters:\nstart_val = 1000000 start_date = '2009-01-01' end_date = '2011-12-31' symbols = ['SPY','XOM','GOOG','GLD'] allocs = [0.4,0.4,0.1,0.1] # @ beginning , 40% to SPY , 40% to XOM, etc  Now suppose we want to find the value of this portfolio day-by-day. If we normalize the portfolio dataframe, we essentially have a dataframe containing cumulative returns for each index. If we multiply this by allocs, we get returns scaled by each percentage of the total portfolio. Then, multiply by start val to get each stock’s total value. Finally, take the sum of this penultimate dataframe to get a single-column dataframe with the total portfolio value at each point in time. In Python,\n# get cumulative returns df = get_data(symbols, pd.date_range(start_date,end_date)) df = normalize(df) # get changes for each stock by their percentages of the starting value alloced = df * allocs # get dollar value of changes vals = alloced * start_val # sum to get total value portfolio_value = vals.sum(axis=1)  We may now compute various statistics on the portfolio’s value.\n Daily returns: Obviously, daily returns of the entire portfolio would be an important statistic, as they indicate how the portfolio changes over time. For some statistics, we need to get rid of the 0 at the beginning of the daily return or else it’ll throw off the values.  daily_rets = daily_rets[1:]   Cumulative returns: The total cumulative return of the portfolio is another interesting statistic, as you can see if the overall gain was positive or negative.  cum_ret = (port_val[-1]/port_val.ix[0,:]) - 1   Avg. and Std. Deviation: These two are the main statistics that get thrown off by the 0 at the beginning. If it were there, the mean would be closer to 0, even though technically 0 isn’t actually one of the returns.  avg_daily_ret = daily_rets.mean() std_daily_ret = daily_rets.std()  Sharpe Ratio The Sharpe Ratio is a metric that adjusts return for risk. It enables a quantitative way to compare two stocks in terms of their returns and volatility. The Sharpe Ratio is calculated based on the assumption that, Ceteris paribus,\n Lower risk is better Higher return is better  Being an economic indicator, it also takes into account the opportunity cost/return of putting the money in a risk-free asset such as a bank account with interest. A sort of risk-adjusted return may be calculated as follows:\n$R_{adj} = \\dfrac{R_p-R_f}{\\sigma_p}$  where $R_p$ is the portfolio return, $R_f$ is the risk-free rate of return, and $\\sigma_p$ is the volatility of the portfolio return.\nThis ratio is a sort of basis for how the Sharpe Ratio is calculated. The Sharpe Ratio is as follows:\n$S = \\dfrac{E[R_p-R_f]}{std(R_p - R_f)}$  Since we’re looking at past data, the expected value is actually the mean of the dataset, so this becomes:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{std(R_p - R_f)}$  One question is where $R_f$ comes from. There are three main ways of getting the data for the risk-free rate:\n The London Inter-Bank Offer Rate (LIBOR) The interest rate on the 3-month Treasury bill 0% (what people have been using recently\u0026hellip;)  LIBOR changes each day, and the Treasury bill changes slightly each day, but interest in bank accounts are typically paid in 6-month or yearly intervals. Using this simple trick, you can convert the annual/biannual amount to a daily amount:\nSuppose the yearly interest rate is $I$. If we start at the beginning of the year with a value $P$, the new value after interest is paid will be $P\u0026rsquo;$. To find the equivalent daily interest value, $I_{eq}$,\n$P\u0026#39; = P(1 \u0026#43; I_{eq})^{252}$  $P(1 \u0026#43; I) = P(1 \u0026#43; I_{eq})^{252}$  $1 \u0026#43; I = (1 \u0026#43; I_{eq})^{252}$  $(1 \u0026#43; I_{eq}) = \\sqrt[252]{1\u0026#43;I}$  $I_{eq} = \\sqrt[252]{1\u0026#43;I} - 1$ \nTherefore, $R_f$, the daily risk-free rate, is just $\\sqrt[252]{1 + I} - 1$. The reason it’s 252 instead of 365 is because there are only 252 trading days in a year.\nSince we’re treating $R_f$ as constant, the standard deviation in the denominator just becomes $std(R_p)$, so the final equation for the Sharpe Ratio becomes:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{\\sigma_{R_p}}$  Sampling rate The Sharpe Ratio can vary widely depending on the sampling frequency. Since $SR$ is an annual measure, any calculations that are done with samples more frequent than yearly need to be scaled to get the annual ratio. To adjust the calculated Sharpe Ratio to be ”annualized”, you just multiply by a factor of $\\sqrt{\\textrm{#samples per year}}$. So if you sample daily, the Sharpe Ratio would become:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{\\sigma_{R_p}} \\sqrt{252}$  Example: Given 60 days of data with the following statistics:\n $R_p = 10bps$ $R_f = 2bps$ $\\sigma R_p = 10bps$,  what is the Sharpe Ratio? One bps is one hundredth of a percent.\nOptimizers An optimizer can: - Find minimum/maximum values of functions - Build parameterized models based on data - Refine allocations to stocks in portfolios For example, say you have the function $f(x) = (x - 1.5)^2 + .5$, and you want to find the minimum. It’s trivial to use calculus and find the minimum analytically, but you can’t always do so if you don’t have an analytical model of the data. Let’s put this in Python:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import scipy . optimize as spo def f(x): y = (x - 1.5)**2 + .5 print \u0026quot;x = {}, y = {}\u0026quot;.format(x, y) return y def test_run(): guess = 2.0 min_result = spo.minimize(f, guess, method='SLSQP', options={'disp': True}) print \u0026quot; minima found at:\u0026quot; print \u0026quot;x = {}, y = {}\u0026quot;.format(min_result.x, min_result.fun) if __name__ == \u0026quot; __main__ \u0026quot;: test_run()  outputs:\nx = [2.], y = [0.75] x = [2.], y = [0.75] x = [2.00000001], y = [0.75000001] x = [0.99999999], y = [0.75000001] x = [1.5], y = [0.5] x = [1.5], y = [0.5] x = [1.50000001], y = [0.5] Optimization terminated successfully. (Exit mode 0) Current function value: [0.5] Iterations: 2 Function evaluations: 7 Gradient evaluations: 2 minima found at: x = [1.5], y = [0.5]  Pitfalls Optimizers aren’t perfect, and since the method used above uses the gradient of the current point to move to the next point, it can be tripped up by various abnormalities in the function it’s trying to minimize, such as:\n Flat ranges: If a portion of the graph is flat (the slope is close to or is 0), then the solver will either take a lot of iterations to solve for the minimum or it might not ever be able to move to a new point, unless it can find a way out. Discontinuities: If there are discontinuities, the gradient might not be defined well enough for the solver to continue. Multiple minima: Say you have a function $f(x) = x^4 − 2x^2 + x^2$. This function has 2 minima at $(0, 0)$ and $(1, 0)$. If the solver starts at $x = 1.5$, it’ll find the minimum at $(1, 0)$, but it won’t ever reach the other minimum. Conversely, if the solver starts at $x = −1.5$, it’ll find the minimum at $(0, 0)$. Therefore, it’s easy to get trapped in a local minimum that may not be the actual global minimum.  Convex problems A real-valued function $f(x)$ defined on an interval is called convex if the line segment between any two points on the graph of $f(x)$ on that interval lies above the graph. Otherwise, it’s called non-convex.\nLeft: a convex function. Right: a non-convex function. It is much easier to find the bottom of the surface in the convex function than the non-convex surface. (Source: Reza Zadeh)\nBuilding a parameterized model If you have a set of data points representing rainfall and humidity that were gathered, you might want to find a function that best fits those points. Say you wanted to fit a line $f(x) = mx + b$ to the points. In this case, you can use linear algebra and find the leastsquares solution, but you can also use an optimizer to find the best parameters $m$ and $b$. What does ”best” mean? Well, we can devise a measure for the error for each point:\n$e_i = (y_i - f(x_i))^2 = (y_i - (mx_i \u0026#43; b))^2$  which is just the difference between the actual value and our model’s predicted value. The reason it’s squared is to ensure that negative errors don’t reduce the total error when we sum up every $e_i$.\n$E = \\sum_{i=1}^{n} (y_i - (mx_i \u0026#43; b))^2$  Now that we have what we want to minimize, $E$, we can use a minimizer to find the best $m$ and $b$. To make the parameters nicer to work with in Python (and allow generalization to higher degrees of polynomials), we’ll rename $m$ and $b$ to $C_0$ and $C_1$. Now, $f(x) = C_0x+C_1$.\nFor example in python:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import scipy . optimize as spo # line is a tuple (C0 , C1) def error(line, data): return np.sum((data[:, 1]-(line[0]*data[:, 0]+line[1]))**2) def fit_line(data, error_func): # initial guess for parameters l = np.float32([0, np.mean(data[:, 1])]) return spo.minimize(error_func, l, args=(data,), method='SLSQP', options={'disp': True}).x def test_run(): original = np . float32([4, 2]) print \u0026quot; original line : C0 = {} , C1 = {}\u0026quot;. format(original[0], original[1]) Xoriginal = np . linspace(0, 10, 40) Yoriginal = original[0] * Xoriginal + original[1] plt.plot(Xoriginal, Yoriginal, 'b--', linewidth=2.0, label=\u0026quot;Originalline\u0026quot;) # add some random noise to the data noise_sigma = 4.0 noise = np.random.normal(0, noise_sigma, Yoriginal.shape) data = np.asarray([Xoriginal, Yoriginal+noise]).T plt.plot(data[:, 0], data[:, 1], 'go', label=\u0026quot;Data points\u0026quot;) l_fit = fit_line(data, error) print \u0026quot; Fitted line : C0 = {}, C1 = {}\u0026quot;.format(l_fit[0], l_fit[1]) plt.plot(data[:, 0], l_fit[0]*data[:, 0] + l_fit[1], 'r- -', linewidth=2.0, label=\u0026quot; Fitted line \u0026quot;) plt.legend(loc='upperright') plt.show() if __name__ == '__main__': test_run()  Portfolio Optimization Now that wee have the tools to optimize a function, we can use it to optimize our portfolio! We can choose to optimize/minimize/maximize various measures, such as daily returns, cumulative returns, or Sharpe Ratio based on the percent allocation of all of the stocks in the portfolio.\nFraming the problem First we need three things:\n a function, $f(x)$, to minimize an initial guess for $x$ the optimizer  In our case, $x$ is actually the set of allocations for the stocks. Also, since we want to maximize Sharpe Ratio, we need to multiply $f(x)$ by $-1$ to call the minimizer.\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/7646/",
	"title": "7646",
	"tags": [],
	"description": "",
	"content": " About: This course is part of the OMSCS ML specialization and is taught by the Quantitative Software Research Group at Georgia Tech. It covers pythons and introductory numerical computing, computational investing, and applied machine learning.\nInstructors:  Tucker Balch David Byrd  Resources:  Course website Calendar Software Setup Q\u0026amp;A Study Guide  Credit: much of this section comes from Ryan Babaie \u0026amp; Neil Hardy\u0026rsquo;s guide.\n"
},
{
	"uri": "/6250/switching/",
	"title": "Switching",
	"tags": [],
	"description": "",
	"content": " Switching At its core, a network serves to route packets between machines on the network. Let\u0026rsquo;s take a look at how packets are moved across networks. It\u0026rsquo;s more complicated than it sounds at first, but quite fascinating.\nThat\u0026rsquo;s right. To even reach your screen, the packets that make up this video likely traveled across at least four or five networks, if not more.\nYou\u0026rsquo;ll learn how that works in the routing videos on BGP, a routing protocol.\nSwitching and Bridging In this lesson, we will learn about switching and bridging. In particular, we will learn about how hosts find each other on a subnet and how subnets are interconnected. We will also learn about the difference between switches and hubs, and the difference between switches and routers. And we\u0026rsquo;ll talk about the scaling problems with Ethernet and mechanisms that can be used to allow it to scale better.\nBootstrapping Networking Two Hosts To start let\u0026rsquo;s talk about how you would network two machines, each with a single interface, to each other. So Host 1 and Host 2 would be connected by two Ethernet adapters or network interfaces. And each of these would have a LAN, or physical, or MAC address. Now a host that wants to sent a datagram to another host can simply send that datagram via its Ethernet adapter with a destination MAC address of the other host that it wants to receive the frame. Frames can also be sent to a broadcast destination MAC address which would mean that the datagram would be sent to every host that it was connected to on the local area network. Now, of course, typically what happens is a host knows a DNS name or an IP address of another host, but it may not know the hardware or MAC address of the adapter on the host that it wants to send it\u0026rsquo;s datagram to. So we need to provide a way for a host to learn the MAC address of another host. The solution to this is a protocol called ARP or the address resolution protocol.\nARP: Address Resolution Protocol In ARP, a host queries with an IP address, broadcasting that query to every other node on the network. That query will be of a form, \u0026ldquo;who has a particular IP address,\u0026rdquo; such as 130.207.160.47, and that particular host who has that IP address on the LAN will respond with the appropriate MAC address. So the ARP query is a broadcast that goes to every host on the LAN from the host that wants the answer to the query and the response is a unicast response with the MAC address as the answer. That\u0026rsquo;s returned to the host that issued the query. When the host that issues the query receives a reply, it starts to build what\u0026rsquo;s called an ARP table. It\u0026rsquo;s ARP table then maps each IP address on the local area network to the corresponding MAC address. Now, instead of broadcasting a ARP query to discover the MAC address corresponding with this IP address, the host can simply consult its local ARP table.\nLet\u0026rsquo;s now take a look at what the host does with this information. When the host wants to send a packet to the destination with a particular IP address. It takes that IP packet and encapsulates it in an Ethernet frame with the corresponding destination MAC address. Essentially, it puts that IP packet inside of an Ethernet frame. So before it sends the IP packet with that destination IP address, it first puts the packet inside a larger Ethernet frame with its own source MAC address and the destination MAC address from its local ARP table.\nARP Quiz So let\u0026rsquo;s consider what we learned about ARP. So what are the formats of the queries and responses in ARP? Is the query a broadcast where a host is asking about an IP address, and the response is a unicast with a MAC address? Is the query a unicast message asking about an IP address and the response is broadcast with a MAC address? Or is the query a broadcast asking about a particular MAC address, where the response is a unicast with the response of a particular IP address?\nARP Solution The purpose of ARP is to allow a host to discover the MAC address corresponding to a particular IP address. And the host doesn\u0026rsquo;t know which host on the LAN owns that particular MAC address. So, ARP allows the host to send a broadcast query asking about who owns a particular IP address. And the response comes from the owner of that particular IP address and the response is the MAC address.\nInterconnecting LANs with Hubs The simplest way that a LAN can be connected is with something called a hub. Hubs are the simplest form of interconnection and in some sense they don\u0026rsquo;t even exist in networks anymore today, because you can build a switch for essentially the same price. But for the sake of example, let\u0026rsquo;s just take a look at how a LAN would be connected with a Hub. Now, a hub essentially creates a broadcast medium among all of the connected hosts where all packets on the network are seen everywhere. So if a particular host sends a frame that\u0026rsquo;s destined for some other host on the LAN, then a hub will simply broadcast that frame that it receives on an incoming port out every outgoing port. So all packets are seen everywhere. There is a lot of flooding and there are many chances for collision. The chance of collision of course, introduces additional latency in the network because collisions require other hosts or senders to back off and not send as soon as they see the other senders trying to send at the same time. LANs that are connected with hubs are also vulnerable to failures or misconfiguration because even one misconfigured device can cause problems for every other device on the LAN. Suppose that you had a misconfigured device that was sending a lot of rogue or unwanted traffic. Well, on a network that\u0026rsquo;s connected with hubs, every other host on the network would see that unwanted traffic. So, we need a way to improve on this broadcast medium by imposing some amount of isolation.\nSwitches Traffic Isolation So in contrast, switches perform some amount of traffic isolation so that the entire LAN doesn\u0026rsquo;t become one broadcast medium. But instead, we can partition the LAN into separate broadcast domains or collision domains. Now a switch might break the subnet into multiple LAN segments. Typically a frame that is bound for a host in the same part or segment of the LAN is not forwarded to other segments. So, for example if we had a network with three hubs, all connected by a switch, then each of these would be its own broadcast domain. And if a host here wanted to send a frame to another destination in the same segment, well that frame would be broadcast within that domain. But the switch would recognize that the destination was in the same segment and would not forward the packet on output ports destined for other LAN segments where the destination was not. Now enforcing this kind of isolation, requires constructing some kind of switch table, or state, at the switch, which maps destination MAC addresses to output ports.\nLearning Switches Let\u0026rsquo;s take a quick look at how learning switches work. A learning switch maintains a table between destination addresses and output ports on the switch, so that when it receives a frame destined for a particular place it knows what output port to forward the frame. Initially the forwarding table is empty, so if there\u0026rsquo;s no entry in the forwarding table the switch will simply flood. Let\u0026rsquo;s look at a quick example. If host A sends a frame destined for host C, then initially the switch has nothing in its table to determine where that frame should be sent, so it will flood the frame on all of its outgoing ports. On the other hand, because the frame has a source address of A, and arrived on input port one, the switch can now make an association between address A and port one. In other words, it knows that the host with address A is attached to port one, so that in future, when it sees frames destined for host A, it no longer needs to flood, but can instead send the frames directly to port one. So, for example, when C replies with a frame destined for A, the switch now has an entry that tells it that it doesn\u0026rsquo;t need to flood that packet. But instead, can simply send the packet directly to the output port. Note also that when C replies, the switch learns another association between address C and port three. So future frames destined for host C, no longer need to be flooded, either. They can simply be forwarded to output port three. So, in summary, if a learning switch has no entry in the forwarding table, it must flood the frame on all outgoing ports. But otherwise, it can simply send that frame to the corresponding output port in the table. Note that learning switches do not eliminate all forms of flooding. The learning switch must still flood in cases where there is no corresponding entry in the forwarding table, and also, these switches must forward broadcast frames, such as ARP queries. Now because learning switches still sometimes need to flood, we still have to take care when the network topology has loops. Now most underlying physical topologies have loops for reasons of redundancy. If any particular link fails, you\u0026rsquo;d still like hosts on the LAN to remain connected.\nBut let\u0026rsquo;s see what happens when the underlying physical topology has a loop. Let\u0026rsquo;s suppose a host on the upper LAN broadcasts a frame. Each learning switch will hear that frame and broadcast it on all of its outgoing ports. When that broadcast occurs, the other learning switches that are in the topology that contains a loop will hear the rebroadcast. They in turn will not know that they shouldn\u0026rsquo;t rebroadcast the packet that they just heard. So each of those switches will in turn rebroadcast the packet on their outgoing ports. And, of course, this process will continue, creating both packet loops and what are known as broadcast storms. So, cycles in the underlying physical topology can create the potential for learning switches to introduce forwarding loops and broadcast storms. So we need some kind of solution to ensure that even if the underlying physical topology has cycles, which it often needs for redundancy, that the switches themselves don\u0026rsquo;t always flood all packets on all outgoing ports. In other words, we need some kind of protocol to create a logical forwarding tree on top of the underlying physical topology.\nLearning Switches Quiz So, as a quick quiz about learning switches, let\u0026rsquo;s suppose that initially the switch forwarding table is empty and host D sends a frame that is destined for host B. Fill out the entry in the switch forwarding table that is populated as a result of this message.\nLearning Switches Solution When the switch sees the frame from host D, destined for host B, it doesn\u0026rsquo;t know what to do with the frame, so it forwards that frame on all of its output ports. However, because it sees a frame arrive from source D, it knows that future frames that are destined for source D should be output on port four.\nSpanning Trees The solution to this problem is to construct what\u0026rsquo;s called a Spanning Tree, which is a loop-free typology that covers every node in the graph. The set of edges, shown in blue, constitutes what\u0026rsquo;s known as a Spanning Tree. The collection of edges in the blue typology covers every node in the underlying physical typology, and yet, there are no loops in the blue topology. Now, instead of flooding a frame, a switch in this topology would simply forward packets along the spanning tree. So for example, this switch would only send a frame along the port corresponding to the blue edge and would not forward the frame out any edges that were not part of the spanning tree. Other switches that receive the frame, would flood in the same fashion, along all edges that were part of the spanning tree, while omitting edges that were not members of the spanning tree.\nLet\u0026rsquo;s take a look at how to construct the spanning tree. First the collection of switches must elect a root, since every tree must have a root. Typically this is the switch with the smallest ID. In this case, the switch at the top of the topology is the root. Then each switch must decide which of its links to include in the spanning tree. And it excludes any link if that link is determined to be not on the shortest path to the root. For example, let\u0026rsquo;s consider the switch in the lower right. It has three lengths, this length takes it on a path that\u0026rsquo;s three hops from the root. This length takes it on a path that\u0026rsquo;s two hops to the root, and this length takes it on a path, that\u0026rsquo;s one hop to the root. Any link that\u0026rsquo;s not on a shortest path to the route is excluded and any link that\u0026rsquo;s on a shortest path on a route is included. Similarly here, this edge is on a path that\u0026rsquo;s one hop away from the route and this edge is on a path that\u0026rsquo;s two hops away. So this node will include this link from the spanning tree. Now, each switch repeats this process to exclude links from the underlying topology. And ultimately, this yields a forwarding topology that looks like the blue graph. And of course there is an issue which is how do we determine the root in the first place? Well initially, every node think it\u0026rsquo;s the root. And the switches run an election process to determine which switch has the smallest ID. And if they learn of a switch with a smaller ID, they update their view of the root, and they compute the distance to the new root. Whenever a switch updates its view of the root, it also determines how far it is from that root. So that when other neighboring nodes receive those updates they can determine their distance to the new root simply by adding one to any message that they receive.\nSpanning Tree Example Let\u0026rsquo;s take a quick example. Suppose the message format is as follows. Y, d and x, where x is the origin of the message, Y is the node being claimed as root, and d is the distance of the particular node sending this message, x, from a claimed root. So, initially every switch in the network broadcasts a message like x,0,x to indicate that the node that it thinks itself is the root. When other switches hear this message, they compare the ID of the sender to their owner ID, and they update their opinion of who the root is based on the comparison of these IDs. Let\u0026rsquo;s suppose that we have the following graph and switch number 4 thinks it\u0026rsquo;s the root. So we will send a message 4, 0, 4 to nodes 2 and 7. But 2 also thinks it is the root, so 4 is going to receive the message 2,0, from node 2, and then it\u0026rsquo;s going to realize that 4 is just one hop away from node 2. So node 4 will update its view of the root to be node 2. Eventually 4 will also hear a message 2,1,7 from node 7; indicating that node 7 thinks it is one hop away from its view of the root, which is node 2. It will realize that the path through node 7 is a longer path to the root, and it will remove the link 4-7 from the tree. We can repeat this process and ultimately we will end up with a spanning tree.\nSwitches vs Routers Let\u0026rsquo;s do a quick comparison of switches and routers. Switches typically operate at layer two. A common protocol at layer two is Ethernet. Switches are typically automatically configuring, and forwarding tends to be quite fast since packets only need to be processed through layer two on flat look ups. Routers, on the other hand, typically operate at layer three where IP is the common protocol. And router level topologies are not restricted to a spanning tree. One can even have multipath routing, where a single packet could be sent along one of multiple possible paths in the underlying router level topology. So, in many ways Ethernet, or layer two switching, is a lot more convenient, but one of the major limitations is broadcast. The spanning tree protocol messages and ARP queries both impose a fairly high load on the network. So this raises the question of whether it\u0026rsquo;s possible to get many of the benefits of the auto configuration and fast forwarding of layer two without facing these broadcast limitations. As it turns out, there are ways to strike this balance. And in the third part of the course, when we talk about network management, we will look at some ways to scale Ethernet to very large topologies. For example, in data center networks. We\u0026rsquo;ll also explore how an emerging technology called Software Defined Networking, or SDN, is effectively blurring the boundary between the layer two and layer three.\nBuffer Sizing So in this lesson, we\u0026rsquo;ll look at an important question in switch design which is, how much buffering do routers and switches need? It\u0026rsquo;s fairly well known that routers and switches do need packet buffers to accommodate for statistical multiplexing. But it\u0026rsquo;s less clear how much packet buffering is really necessary. Now given that queuing delay is really the only variable part of packet delay on the internet, you\u0026rsquo;d think we\u0026rsquo;d know the answer to this question already. And for quite some time there have been some well understood rules of thumb but it turns out that we\u0026rsquo;ve recently revisited this question and come up with some different answers. So let\u0026rsquo;s first look at the universally applied rule of thumb. Now for the sake of the examples in this lesson, I\u0026rsquo;m going to use routers and switches interchangeably because it doesn\u0026rsquo;t really matter. All that matters here is that we have a network device that\u0026rsquo;s a \u0026lsquo;store and forward\u0026rsquo; packet device that has the capability of storing a frame or a packet and then later sending it on. So let\u0026rsquo;s suppose that we have a path between a source and a destination, and the round-trip propagation delay is 2T and the capacity to bottleneck link is C. Now the commonly held view is that this router needs a buffer of 2T times C. It should be clear why this rule of thumb exists. C is the capacity to the bottleneck link in say, bits per second and T is the time of units second, so this works out to bits, and the meaning of this quantity is simply the number of bits that could be outstanding along this path at any given time. It effectively represents the maximum amount of outstanding data that could be on this path between the source and destination at any time. Now this rule of thumb guideline was mandated in many backbone and edge routers for many years. It appears in RFCs and ITF Architectural guidelines and it has major consequences for router design simply because this can be a lot of router memory and memory can be expensive. The other thing of course is that the bigger these buffers, not only the bigger the cost but also the bigger the queuing delay that could exist at any given router. And hence, the more delay the interactive traffic may experience and the more delay that feedback about congestion will experience. The longer these delays are, the longer it will take for the source to hear about congestion that might exist in the network. Now to understand why this guideline is incorrect, let\u0026rsquo;s first re-derive the rule of thumb a bit more formally and then we\u0026rsquo;ll understand why it does not always apply in practice.\nBuffer Sizing for a TCP Sender Let\u0026rsquo;s suppose that we have a TCP sender that\u0026rsquo;s sending packets, where the sending rate is controlled by the window W, and it\u0026rsquo;s receiving ACKs (acknowledgements). Now at any time if the window is W, only W unacknowledged packets may be outstanding. So the sender\u0026rsquo;s sending rate, R, is simply the TCP window, W, divided by the round trip time (RTT) of the path. So the rate is W over RTT. Now remember that TCP uses additive increase, multiplicative decrease, or AIMD, congestion control. So for every W ACKs received, we send W plus one packets, and our TCP saw tooth will look something like this. We\u0026rsquo;ll start at a rate W_max over 2, increase the window to W_max and then when we see a drop we will apply multiplicative decrease and reduce the sender\u0026rsquo;s sending rate to W_max over 2 again. So here, right at the point of a packet drop, this represents the maximum number of packets that can be in flight. So again, the required buffer is the maximum number of packets that can be in flight, or simply the height of this TCP saw tooth. Now we know the rate is W over RTT, and we\u0026rsquo;d like the sender to send at a common rate, R. And if we\u0026rsquo;d like the sender to be sending at the same rate before and after it experiences a loss, then we know that the rate before the drop must equal the rate after the drop. So then we can set these two rates equal. We know that the RTT is part transmission delay T, and part queuing delay which is the maximum buffer size of the bottleneck link, divided by the capacity of the bottleneck link. We also know that after reducing the window, the queuing delay is zero. So we can replace the term on the left with W_old over 2T plus B over C and we can replace the term on the right with W_old over 2, because the congestion window has been reduced half divided by 2T, simply the propagation delay with no queuing delay. Now if we solve this equation we find that the required buffering is simply 2T times C. Now the rule of thumb makes sense for a single flow, but a router in a typical backbone network has more than 20,000 flows. And it turns out that this rule of thumb only really holds if all of the those 20,000 flows are perfectly synchronized. If the flows are desynchronized, then it turns out that this router can get away, with much less buffering.\nIf TCP Flows are Synchronized Now, if TCP flows are synchronized, the dynamics of the aggregate window as shown in the upper part of the graph, would have the same dynamics as any individual flow. The quantities on the Y axis here would simply be different. Specifically, the number of pockets occupying the buffer would be the sum of all of the TCP flows windows, rather than the window of any individual flow. Now if there are only a small number of flows in the network then these flows may tend to stay synchronized, and the aggregate dynamics might mimic the dynamics of any single flow, as shown. But as the network supports an increasingly large number of flows, these individual TCP flows become de-synchronized. So instead of all of the flows lining up with the saw tooth as shown in the bottom part, individual flows might see peaks at different times. As a result, instead of seeing a huge saw tooth that\u0026rsquo;s the sum of a bunch of synchronized flows, the aggregate instead might look quite a bit more smooth, as a result of the individual flows being desynchronized. And we can represent this sum, which is the buffer occupancy, as a random variable. At any given time, it\u0026rsquo;s going to take a particular range of values. The range of values that this buffer occupancy takes can actually be analyzed in terms of the central limit theorem.\nThe central limit theorem tells us that the more variables that we have, and, in this case the number of variables are the number of unique congestion windows of flows that we have, the narrower the Gaussian will be. In this case, the Gaussian is the fluctuation of the sum of all of the congestion windows. In fact, the width decreases as 1 over root N, where N is the number of unique, congestion windows of flows that we have. And therefore, instead of the required buffering, needing to be 2T times C, we can get away with much less buffering, in particular, 2T times C divided by the square root of N, where N, is the number of flows, passing through the router.\n"
},
{
	"uri": "/7646/essential-economics/",
	"title": "Essential Economics",
	"tags": [],
	"description": "",
	"content": " Essential Economics This chapter discusses terminology, stock market dynamics, and important indicators in economics. This will allow us to more accurately judge the value of an economic decision and make predictions on the market.\nFunds We’ll discuss three different types of funds: Exchange-Traded Fund (ETF), mutual funds, and hedge funds. Different types of funds are governed under different rules. ETFs are similar to stocks in that they are bought and sold at will like stocks- very liquid. However, ETFs typically represent baskets of stocks, and it is known to the trader what the fund represents. Mutual funds can only be bought and sold at the end of the day, and the holdings within a mutual fund are only disclosed every quarter. The least transparent holdings are that of a hedge fund. Before investors can buy shares in the fund, they must sign a long term agreement and holdings are rarely disclosed.\nAbout Funds For stocks and ETFs, having a large ”cap” means that the total value of stocks (number of stocks × price of a stock) in a company is worth many billions of dollars. Moreover, the price of a stock doesn’t reflect the value of a company, but the price at which they are selling shares. ETFs, like stocks, can easily be traded through individuals alone, whereas shares in mutual funds require a broker and hedge fund shares require more of a one on one relationship. Managers of ETFs and mutual funds are compensated based on expense ratios, which denote a percentage of Assets Under Management (AUM). For an ETF, expense ratios range from 0.01% to 1%, and in mutual funds from 0.5% to 3%. Hedge funds follow a ”two and twenty” policy, where managers get 2% of the AUM and 20% of the profits.\nThe type of a fund can be more easily recognized by how it’s named. For example, an ETF has a ticker, or stock symbol, with three or four letters, like AAPL. A mutual fund has five letters, like VTINX, and a hedge fund doesn’t have a ticker because shares are much less liquid. How much money is managed by a fund is known as the AUM, and shares represent percentages of the AUM.\nIt’s fairly clear to see that hedge funds are very different from ETFs and mutual funds. Hedge funds typically have no more than 100 investors, whereas ETFs and mutual funds have thousands. Those that invest in hedge funds are typically very wealthy individuals, institutions, and funds of funds. Funds of funds typically take large sums of money from potentially many places and invest in several hedge funds. This is a bridge for smaller investors to participate in hedge fund. The goal of a hedge fund typically falls along the lines of two ideals. The hedge fund may be out to beat a bench mark which is to say that the hedge fund aims to outperform an index of stocks. A hedge fund could also aim for absolute return, which translates to net positive profit no matter what, but usually takes more time and has fewer returns as a trade off for stability. We’ll be focusing on hedge funds because they are the most computationally demanding.\nFund Metrics and Operations Measuring the performance of a fund is vital for making financial decisions in the market, so here we’ll discuss a few. Overall success can be measured by cumulative return, which is the percentage of an original value made in a given time: $\\dfrac{\\textrm{end - start}}{start}$. However, this means little if the portfolio is rapidly and wildly fluctuating. Hence it’s also useful to measure the volatility of a portfolio. This is simply measured by the standard deviation of daily returns; it’s best to have low volatility. Another important measure is the return on risks. This is done by calculating the Sharpe Ratio (SR), also called risk-adjusted reward.\n$SR = \\sqrt{252} \\dfrac{mean(\\textrm{daily returns} - \\textrm{risk free rate})} {volatility}$  The factor of $\\sqrt{252}$ comes from the number of trading days in a year. These factors can give us an idea of how well a portfolio is performing.\nAs previously mentioned, hedge funds are very computationally intensive environments. Let’s delve into the details of how a typical hedge fund works. Central to the operation of a hedge fund is its trading algorithms. Normally, a target portfolio is decided upon, then historical stock data and the target portfolio are fed to the trading algorithms to produce orders. The orders are sent to the market to alter the live portfolio, which is again fed back into trading algorithms.\nTrading algorithms work to place certain orders at the proper time. For example, an order for everything in the target portfolio shouldn’t be placed all at once because the price of the stock will go up and more money is spent than if strategic ordering were implemented. Additionally, there is another set of computational structures for determining the target portfolio.\nHistorical data, current portfolio, and prediction algorithms are fed into an optimization program to produce a target portfolio. The majority of machine learning comes into play when determining the market forecast.\nMarket Mechanics Ordering The live portfolio is altered by giving orders to a broker in the stock market, so it serves to know what exactly is in an order. The broker needs to know whether to buy or sell and of which stock(s) by their market symbols. The order must also contain the number of shares and the type of order. Stock exchanges only consider limit orders and market orders, but note that orders can be increasingly complex based on instructions given to a broker. A market order is an order at the current market price, and ordering at a limit price tells the broker to only buy or sell at a certain price; for example, some may not want to buy beyond some value or sell below some price. Of course, a limit order must also include the desired price.\nAfter the broker sends the order to the stock exchange, it is made public in the style of an ”order book”. Others can see the stocks and collective bids that have been made on them, but not who has placed the orders. The order book contains a list for each stock of the orders within it including whether the order asks for others to buy or bids on the stock. Both types include a price at which orders are allowed to be bought/sold at and the size of the order. Orders of the same type and price are lumped together. Market orders always sell at the highest bid and buy at the lowest asking price.\nThe example order book suggests that the price of the stock will decrease because there is much more selling pressure- more are selling than buying.\nDynamics of Exchange There are many stock exchanges, and each has its own order book. When an order is placed, say by an individual, the order is sent to the broker and the broker chooses between the stock exchanges to execute that order. The broker takes information from all the stock exchanges and makes a transaction based on which one has the stock with the best price. Fees are associated with making transactions in stock exchanges and with using a broker. A broker typically has many clients; the broker can observe clients who want to buy and sell at the same price and circumvent stock exchanges entirely. The law ensures that this trade can only happen if both the buyer and seller get prices that are at least as good as at an exchange. Even if this transaction cuts out the stock exchange, it must still be registered with one, and it’s usually with the exchange the stock is housed.\nOrders can be handled internally or moved through what’s called a ”dark pool”. A dark pool is a place where investors can trade without the transparency of an order book outside of a stock exchange. The results of the trade, like internal trades, still need to be registered with public exchanges after they’ve occured. A dark pool can act as an intermediary between all types of investors that may want to escape the transparency of stock exchanges. Brokers like this because they don’t have to pay fees associated with trading at a stock exchange. They also argue that it’s fair because clients are getting prices that are as good as at the market. However, hedge funds and dark pools can heavily exploit this system as it stands if they have well-placed, fast computers.\nExploiting the Market These days the market is entirely digital and computers automate transactions all across the country. As a result, orders can be processed in fractions of a second, and timing is everything. A hedge fund may pay stock exchanges enough money to house computers very close to the exchanges, which gives them a distinct advantage. For example, let’s say that someone places an order for a stock, and it’s sent to multiple markets. A hedge fund close to those markets can see that order reach one of them first and buy up that stock from the other exchanges through the high speed infrastructure they have in place. Then when that order reaches other exchanges, the hedge fund has already bought those shares and sells it back at a higher price. This is one of many strategies in High Frequency Trading (HFT) that takes place on the order of milliseconds.\nThis can also happen on an international scale. A hedge fund may have computers collocated with international markets rapidly observing and comparing order books between them. If a difference occurs in a stock between the two markets, all that needs to be done is to sell in the market with a higher price and buy in the market with a lower one. This happens very quickly, so the price of the stock is not very different at different markets. HFT strategies usually trade high volumes to turn a large profit in small price differences.\nThose that operate on HFT strategies can not only manipulate a transparent market, but also a dark one. First, let’s explain why someone would want to use a dark pool. For example, an investor who wants to sell a high volume of shares in a transparent market would want to do so in small chunks so as not to upset the price all at once and get less for the shares. However, others will see this and lower their bids knowing that a high volume is to be sold and the investor still gets less for their shares. In a dark pool, others can’t see those that want to buy or sell, so the investor may get a better price. There are many ways to exploit a dark pool, but it always stems from information leakage. Knowing the order book of a dark pool means a world of advantage. Dark pool operators or constituents may secretly participate in their own pool or leak information about it to others for a price. The private nature of a dark pool allows those who operate it to make their own rules about who can participate and how trading works. Since information is at the discretion of the operator, it’s fairly easy for those with direct access to exploit a dark pool. Those that don’t have direct access can ”game” the pool by probing it with many small volume orders. This yields some idea of the size and prices of bids, which gamers can exploit by selling when they find the bids are highest and buying when asking is lowest.\nOther Orders and Shorting Although exchanges only take market and limit orders, other orders can be made through a broker. Often a broker implements them for clients without their knowledge to benefit both themselves and the client. The most simple order above a limit order is a stop-loss order. The broker holds stock until the price drops below a certain price, then sells at market price. Similarly, a stop-gain order waits until the price climbs to a point at which the client wants to sell. Slightly more complex is the trailing stop. Here the broker sets a stop-loss criteria trailing a stock that’s increasing in price. As the price increases, so does the stop-loss criteria; when that stock starts to decrease, the stop-loss criteria is met and stocks are sold.\nWhat if someone wanted to bet that a stock will decrease and still profit? Instead of just selling high and buying low, those stocks can be borrowed and sold, so the value of those stocks is gained at a high point but the stocks are still owed. Then when the price of the stock decreases, the stock can be bought at a lower value, and the shares returned to whom they were borrowed while a profit on the difference was made. This is called shorting. As long as the price of the stock goes down, this is a good strategy; however, if the price goes up, then the difference results in a net loss.\nWorth: Company Valuation The price of a company’s stock is intended to reflect the value of the company. Ergo, if the price of a company’s stock deviates significantly from its predicted value based on the company’s predicted worth, then there’s a profitable opportunity for when it returns to reflect the company’s worth. The value of the company can be estimated several different ways. One way is to estimate its intrinsic value, which is based on the future dividends the company will give; these are annual payments to stockholders. This doesn’t really describe what the company has though. The book value of a company is founded in the company’s assets like its facilities and resources. A company’s market cap is yet another way to estimate a company’s worth, and it’s easiest to calculate. This is effectively what the stock market thinks what the company is worth and it’s the value of a stock multiplied by the total number of stocks.\nIntrinsic value may not make sense if we try to imagine the value of a company that will pay dividends consistently as long as it stands. However, the company can never be 100% reliable, so the value of its dividends amount to the value of a promise. The promise of some money in a year is worth less than the same amount given right now because of this principle. Thus, the value of a those promised dividends decreases as the time they’re promised is longer, so the total value will converge to a calculable value. Similarly, we can calculate the present value (PV) of a dollar that is promised after a certain time. It makes sense that the PV of a dollar promised right now is a dollar, but what about in a year? The PV is some fraction of its future value (FV) based interest rate (IR) and the length of time, $t$.\n$PV = \\dfrac{FV}{(1\u0026#43;IR)^t}$  In this way we have a conversion between the present value and future value of some amount of money. The interest rate is also called the discount rate and it reflects the risk involved with investment. A more stable company will have a lower discount rate because they’re more reliable. The intrinsic value, IV, of a company can be calculated knowing its discount rate and dividend payments by\n$IV = \\dfrac{FV}{IR}$  Thus, if a hypothetical company pays dividends of \\$5 a year and has a discount rate of 1%, then the value of this company is $ \\dfrac{$5}{0.01} = \\$500 $. Book value of a company is simple to calculate because it is just what the company has versus what the company owes. If a company only has a factory worth \\$1 million, a patent worth \\$500,000, and a loan of \\$200,000, then the company is worth \\$1 million − 200,000 = \\$0.8 million. The patent is considered an intangible asset and isn’t counted in calculating the book value.\nNews about companies can drastically change some of these measures. Investors reflect their opinions on the worth of a company through stocks- if they feel the company is worth less, they will sell and vice versa. Let’s say bad news about a company comes up; investors will see that as increased risk in investing in the company. The company will have to increase their IR to appease investors and the intrinsic value of the company will reduce. This would also reduce the stock price of the company, which decreases the market capitalization of the company. News can affect singular companies, sectors of business, and the market as a whole depending on the scope of the news.\nMarket strategies are based on deviations in the estimated values of a company. For example, if the intrinsic value of a company drops and the stock price is relatively high given its history, then it would probably be a good idea to short that stock because the price will almost certainly go down. The book value of a company provides somewhat of a minimum for the market cap; that is because if the market cap goes below the book value, then a predatory buyer typically buys the whole company, breaks it apart, and sells its parts for the book value to turn a profit.\nThe Capital Assets Pricing Model The Capital Assets Pricing Model (CAPM) is a model that is used to predict the return of stocks. To understand this model, a portfolio must be understood in more depth. The term portfolio has been used throughout this text, but has yet to be clearly defined; a portfolio is a set of weighted assets, namely stocks. A portfolio is a set of stocks that are weighted by their value, and all the weights add to 1. Some stocks might be shorted, so technically their portfolio value is negative and really the sum of the absolute value of their weights is mathematically written, where $ w_i$ is the weight of a stock in a portfolio\n$\\sum_{i} |w_i| = 1$  and the return of the portfolio for a given day is\n$\\sum_{i} w_i r_i$  where $r_i$ is the return for a stock in a day. As an example, lets say a portfolio is composed of two stocks, A and B, and their respective weights are 0.75 and -0.25 because stock B is shorted. Then if on a given day, stock A increases by 1% and stock B decreases by 2%, the result is a portfolio return of (0.75)(0.01) + (−0.25)(−0.02) = 1.25%.\nA similar portfolio can be made for entire markets. Although, it’s typically limited to an index which includes the largest companies in a market, like the S\u0026amp;P 500 for the US market. These companies are weighted by their market caps, so a company’s weight in the market is approximately that company’s cap, c, divided by the sum of all caps.\n$w_i = \\dfrac{c_i}{\\sum_{j} c_i}$  The CAPM predicts the return on stocks within a certain market with the simple equation\n$r_i[t] = \\beta_i r_m[t] \u0026#43; \\alpha_i[t]$  This says that the return of a stock is largely based on the return of the market as a whole, $r_m$. The degree to which a stock is affected is based on that stocks particular $\\beta$ value. Fluctuations that deviate from this are represented in the (theoretically) random variable, $\\alpha$, which (theoretically) has an expected value of zero. The $\\beta$ and $\\alpha$ values of a stock are calculated based on the historical data of daily returns. The daily returns of a stock are plotted against that of the market and the slope of the fitted line constitutes the $\\beta$ value. The y-intercept and random deviations describe the $\\alpha$ value.\nCAPM and Related Theories The nature of CAPM suggests a specific strategy when approaching the market. CAPM says that the relationship of stocks to the market is linear with an average fluctuation of zero from this relationship. This suggests that the best tactic is to simply choose a set of stocks that will perform well with a certain market environment and sit on them. Active management is a way of thinking that believes the $\\alpha$ value is not entirely random and can be predicted. This mindset promotes carefully choosing and trading stocks on a regular basis depending on predicted $\\alpha$ values. This is the dichotomy between active and passive portfolio management. If we assume that $\\alpha$ is entirely random, then the only way we can beat the market is by predicting the return on the market. However, this is not entirely true, and CAPM will be used to eliminate market risk entirely.\nCAPM gives $\\beta$ values to each stock, but there are other theories that say it’s more complicated. One of these is the Arbitrage Pricing Theory (APT), which says that there is not a contribution based on the whole market, but based on its sectors. Moreover, a stock is affected by what happens in the ten different sectors of the economy, and the CAPM equation becomes\n$r_i = \\sum_{j} \\beta_{ij}r_j[t] \u0026#43; \\alpha_i[t]$  where $j$ denotes the different sectors. This provides a more in-depth prediction of stock return.\nUsing the CAPM Now we want to use the CAPM to create a lucrative portfolio. If we say that the return on the market can never be predicted, then any component associated with market return is risk. Applying the CAPM equation to get an overall portfolio return yields\n$r_p = \\sum_{i} w-I[\\beta_ir_m(t)\u0026#43;\\alpha_i(t)] $  The only way to remove the market return component is to choose weights such that $\\sum_{i}w_i\\beta_i = 0$. At this point it may seem that there will not be an expected return for the portfolio because CAPM predicts $\\alpha$ to be random. It is here that the assumptions of CAPM are wrong. Using some information, we can predict whether a stock will perform better or worse than the market, which will yield a profit regardless of which way the market goes. This information can come from expertise, some analysis, or, in our case, machine learning.\nTechnical Analysis There are effectively two kinds of analysis: fundamental and technical. Fundamental analysis looks at the properties of a company to determine market decisions. Technical analysis looks at patterns in stock prices to make market decision; since technical analysis is clearly more useful aside from predatory buying, that’s what will be discussed.\nTechnical analysis only focuses on stock price and volume. Using these to calculate statistics gives indications on what economic decisions to make. Technical analysis is most useful on shorter time scales and when combinations of indicators point to the same decision. HFT trade at the millisecond timescale and fundamental analysis firms operate at the timescale of years; humans and computers tend to work together at a timescale in between.\nSome Good Indicators It’s useful to develop strategies based on time-series indicators, so here are a few. Momentum is a scale dependent indicator that suggests an upward or downward trend depending on slope. Moreover, an n-day momentum, $m$, for a stock with price function, $p[t]$, is calculated as\n$m = \\dfrac{p[t]-p[t-n]}{p[t-n]} = \\dfrac{p[t]}{p[t-n]} - 1$  This is simply the difference in price as a ratio to the price n days ago. Typically 5, 10, or 20 day momentum is used with values ranging from -0.5 to 0.5. Another indicator is a simple moving average (SMA); SMA is also scale dependent looking over an n-day window. The price of a stock over n days is averaged and plotted over time where points are placed at the leading end of the window. However, to get a useful value, this needs to be compared to the real time price. Like momentum, it is done as a ratio\n$SMA[n] = \\dfrac{p[t]-E(p[t-n:t])}{E(p[t-n:t])} = \\dfrac{p[t]}{E(p[t-n:t])} - 1$  Momentum and SMA together often prove to be strong indicators. For example, if there is strong positive momentum and a crossing of the price from the price being lower than the average to above- this is a good indicator the price will increase. Larger than normal deviations from the moving average are expected to return back to the average and indicate opportunities. Thus, if SMA is positive, it’s a selling opportunity, and a buying opportunity if SMA is negative.\nHow those decision are made depends on the state of the stock, or its volatility. The standard deviation of the stock’s fluctuations provides an excellent measure for when to make decisions. It depends on how certain we want to be that the price is an outlier. The farther the decision threshold is from the SMA, the more certain we are that the price is an anomaly. From basic statistics, if the decision threshold is placed two standard deviations away from the SMA, then we are 95% sure the price is an anomaly. These bands, typically at two standard deviations away from SMA, are called Bollinger bands. The way this is written mathematically is, again, a ratio, which is between the price difference and the $2\\sigma$ length where $\\sigma$ is the standard deviation.\n$BB[t] = \\dfrac{p[t] - SMA[t]}{2\\sigma}$  When making economic decisions, a Bollinger band value greater than 1 denotes a selling opportunity, and less than -1 denotes a buying opportunity. However, it’s better to trade when the price crosses the band for the second time because that signals the price moving in a profitable direction.\nWhen using these values in a machine learner, it’s important that indicators are normalized. To normalize values, follow\n$normal = \\dfrac{value-mean}{\\sigma}$  This provides a z-score by which to compare everything.\nAdjusted Prices Analysis of historical data is crucial for determining patterns and making economic decisions, but some things drastically change the price of stocks without having any effect on the real value of the stock. Dividends and stock splits are two things that do just that. The adjusted price accounts for these events and corrects the computational problems that would occur if only the price were taken into account.\nStock splits occur when the price of a stock is too high and the company decides to cut the price, but increase the volume so that the overall market cap is the same. This is a problem when dealing with data because it’s seen as a large drop in price. The adjusted price is calculated going backwards in time; moreover, the given and adjusted price are the same for a certain starting present day and adjusted going backward in time. If the price is ever split, say by 3, then at the time of the split, the price is divided by 3 so that there is no discontinuity.\nAt the time a company announces the date for payment of dividends, the price of the stock will increase by the amount of a dividend until they’re paid at which point the price rapidly decreases by that amount. This is adjusted looking back in time, and on the day a dividend is paid, the prices preceding are decreased by the proportion of the dividend payment.\nAs a note for machine learning, the data that is chosen for the learner is very important. If the stocks from today are chosen and analyzed starting from 7 years ago, then those stocks will of course do well because they’ve survived. That’s using a biased strategy, so what needs to be done is to take index stocks from 7 years ago and run with those. For adjusted price, it’s also important to note that the adjusted price will be different depending on where the starting point is chosen, and that should also be taken into account.\nEfficient Markets Hypothesis Until now, we’ve been assuming, for technical analysis, that there is information in historical data that we can exploit to determine what the market is going to do. The same goes for fundamental analysis in terms of fundamental data. However, the Efficient Markets Hypothesis says that we’re wrong on both accounts!\nHere are some assumptions of the Efficient Markets Hypothesis:\n Large number of investors: The most important assumption of EMH is that there are a large number of investors for-profit. They have incentive to find where the price of a stock is out of line with its true value. Because there are so many investors, any time new information comes out, the price is going to change accordingly. New information arrives randomly Prices adjust quickly Prices reflect all available information  The three forms of the EMH There are 3 forms of the EMH, ranging from weak to strong.\n Weak: Future prices cannot be predicted by analyzing historical prices. This leaves room for fundamental analysis, however. Semi-strong: Prices adjust rapidly to new public information Strong Prices: Prices reflect all information, public and private  Is the EMH correct? If the EMH is correct, a lot of what we’re trying to do is impossible, so we should cut our losses and go home. Luckily, there is evidence for why certain versions of the hypothesis are incorrect. The existence of hedge funds indicates that you can profit by investing in stocks other than the market portfolio.\nThe strong version is the weakest of the three, considering there are many examples of insiders using esoteric information for their own benefit. And in many cases, these people have gone to jail!\nThere is also data that shows that the semi-strong version isn’t too likely to be correct. You can see trends in 20-year annualized returns versus 10-year P/E ratio data, which means that you most likely can use fundamentals to predict future performance.\nThe Fundamental Law of Active Portfolio Management Richard Grinold was trying to find a way of relating performance, skill, and breadth. For example, you might have lots of skill to pick stocks well, but you might not have the breadth to use that skill. So he developed the following relationship:\n$performance = skill\\sqrt{breadth}$  So we need some way of measuring skill and breadth. Performance is summarized by something called the information ratio:\n$IR = IC\\sqrt{BR}$  where IC is the information coefficient, and BR is the number of trading opportunities we have.\nThe Coin-flipping Casino As a thought experiment, instead of buying and selling stocks, we’re going to flip coins, and bet on the outcome. This is analogous to buying a stock and holding it– either you earn or lose money.\nThe coin is biased (like $\\alpha$) to P(heads) = .51. The uncertainty of the outcome is like $\\beta$.\nBetting: Betting works by betting on N coins. If we win, we now have 2N coins. If we lose, we now have 0 coins, so this is an even-money bet (you either gain N coins or lose N coins.)\nThe Casino: The casino has 1000 tables, each with a biased coin, and you have 1000 tokens, so you can bet them in any way you like: 10 tokens each on 100 tables, 1 token on each table, or 1000 tokens on 1 table. Once bets have been placed, the coins are all flipped in parallel, and for each game you either lose your chips or win. So now the question is what scenario is going to net you the best outcome? Let’s take the following two bets for example:\n 1000 tokens on one table and 0 on the other 999 1 token on each of 1000 tables  Which is better? Or are they the same? In fact, the expected return of both bets are the same, but bet 2 is much less risky, as with bet 1, if you lose, you lose all of your money, but with bet 2, you might lose around half your money. In fact, the chance of losing all of your money (if you bet tails) is:\n$(.49)^{1000} \\approx 10^{-310}$  To determine which is best, we need to consider risk and reward. In this case, the reward is our expected return. If $P_w$ is the chance we would win, and $P_l$ is the chance that we would lose, and $W$ is the amount we would win, whereas $L$ is the amount we’d lose, expected return for a single bet is calculated as follows:\n$E[R] = P_wW \u0026#43; P_lL$  For the biased coin where we place all of our bets on one table, this would be:\n$E[R] = .51(\\$1000) \u0026#43; .49(−\\$1000) = \\$20$  If we placed 1 token on each table, the expected return would be:\n$E[R] = \\sum_{i=1}^{1000} .51(\\$1) \u0026#43; \\sum_{i=1}^{1000} .49(−\\$1000) = \\$20$  So, in terms of reward, neither is better or worse. So how do we choose how to allocate the tokens? It turns out that the risk makes it easy to choose.\nFirst, what’s the chance that we lose it all? For the case where all of our tokens are on one table, the chance is 49%. For the second table, it’s around $10^{-308}%, which is quite a bit smaller\u0026hellip;\nAnother way to look at the risk is by looking at the standard deviation of the bets. An example of the outcomes for situation 2 is:\n$−1, 1, 1, 1, −1, −1, 1, −1, ..., 1$  The standard deviation of which is just 1. Now, for the case where we put all of our tokens on one table, the outcomes look like this:\n$1000, 0, 0, 0, 0, ..., 0$  or\n$−1000, 0, 0, 0, 0, ..., 0$  The standard deviation (risk) in both cases is $\\sqrt{1000} \\approx 31.62$, which is much higher than the standard deviation for putting bets on each table. Now, we can create a risk-adjusted reward (Sharpe Ratio) for the single-bet case:\n$R_s = \\dfrac{\\$20}{\\$31.62} = 0.63$  For the multi-bet scenario, it’s:\n$R_m = \\dfrac{\\$20}{\\$1} = 20.0$  Clearly, the second case wins based on this ratio. Something interesting about these results is that:\n$20 = .63\\sqrt{1000}$  It turns out that this can be generalized to\n$SR_{multi} = SR_{single}\\sqrt{bets}$  Which shows that as we increase the number of bets (diversify), the Sharpe Ratio increases. This is the relationship described in the Fundamental Law of Active Portfolio Management; to increase performance, you can either increase skill or diversify (bets), although diversification only goes as the square root.\nNow, back to equation 3.1. Let’s define what information ratio means. If we consider the CAPM equation:\n$r_p[t] = \\beta_p r_m[t] \u0026#43; \\alpha_p[t]$  we can associate the first term, $\\beta_p r_m[t]$ with the market, and the second term, $\\alpha_p[t]$ with skill. Information ratio is defined as:\n$IR = \\dfrac{\\overline{\\alpha_p[t]}} {\\sigma_{\\alpha_p[t]}}$  Information ratio can be thought of as a Sharpe Ratio of excess return (due to skill). Now, the information coefficient, IC, is the correlation of forecasts to returns. IC can range from 0 (no skill) to 1 (perfect skill). BR, or breadth, is the number of trading opportunities per year. For example, if you are Warren Buffet, and hold only 120 stocks for a whole year, BR is just 120. However, if you have 120 stocks and trade them daily, $BR = 120∗365$.\nLet’s do an example: say that James Simons and Warren Buffet both have the same information ratio, and that Simons’ algorithm is 1\u0026frasl;1000 as smart as Buffet’s. Buffet only trades 120 times per year. How many trades per year must Simons execute to have the same information ratio (performance)?\n$IR_S = IC_S\\sqrt{BR_S}$  $IR_B = IC_B\\sqrt{BR_B}$  $IR_S = 1/1000IC_B$  $\\dfrac{IR_B}{IR_S} = \\dfrac{IC_S\\sqrt{BR_S}}{IC_B\\sqrt{BR_B}} = 1$  $= \\dfrac{\\dfrac{1}{1000}\\sqrt{BR_S}}{\\sqrt{BR_B}}$  $\\Rightarrow BR_S = (1000)^2 BR_B$  $= 120,000,000$ \nSo Simons must execute 120 million trades, whereas Buffet only needs to execute 120. That’s quite a difference! Indeed, skill is an extremely important factor in performance.\nPortfolio Optimization Now we wish to optimize a portfolio, and what this means is minimizing risk for a given target return. Risk is largely defined as the volatility of a stock. A portfolio is composed of some stocks that individually have their own return-risk ratios, but it is possible to weight them such that the return-risk ratio of the portfolio is higher than that of any individual stock.\nThis is done through combining correlated and anti-correlated stocks to highly reduce volatility. In the case of a highly correlated group of stocks, their combination results in a similar volatility, but if they’re combined with highly anti-correlated stocks, then with accurate weighting, fluctuations cancel out and volatility is minimal while yielding similar returns. A useful algorithm to find the best weighting is mean variance optimization (MVO). This algorithm is not explained, but we should find it. MVO and similar algorithms find the minimal risk for a given target return, and if this is plotted over all target returns, we get a curve called the efficient frontier. On a return-risk plot, a line tangent to the efficient frontier with an intercept at the origin also points to the portfolio with the minimal Sharpe ratio.\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/6242/",
	"title": "6242",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/routing/",
	"title": "Routing",
	"tags": [],
	"description": "",
	"content": " Routing Overview With your head wrapped around routing we\u0026rsquo;ll now take a look at the nuts and bolts that make routing possible: naming, addressing and forwarding.\nAnd you\u0026rsquo;ll start your first significant Mininet project. In the project you\u0026rsquo;ll investigate switched buffer sizing which can have an important effect on network performance.\nInternet Routing The next few lessons will cover internet routing. Contrary to what you might think, the internet is not a single network, but rather a collection of tens of thousands of independently operated networks, or autonomous systems, sometimes simply called ASes. Networks such as Comcast, Georgia Tech, and Google, are different types of autonomous systems. An autonomous system might be internet service provider, a content provider, a campus network, or any other independently operated network. Now when you\u0026rsquo;re sitting at home on Comcast and trying to reach content in Google or Georgia Tech, your traffic actually traverses multiple autonomous systems. This process of internet routing actually involves two distinct types of routing. One is intradomain routing, which is the process by which traffic is routed inside any single autonomous system. The other is interdomain routing, which is the process of routing traffic between autonomous systems. So computing a path between a node in an ISP like Comcast and another node in a network like Georgia Tech\u0026rsquo;s involves computation of both intradomain paths and interdomain paths. In this part of the lesson we\u0026rsquo;ll look at intradomain routing. Then we\u0026rsquo;ll study interdomain routing, as well as the business relationships that make interdomain routing so complicated. So let\u0026rsquo;s jump into our study of intradomain routing and topology.\nAutonomous Systems (AS) AS Quiz As a quick quiz, which of the following types of routing protocols are responsible for routing within an autonomous system?\nAS Solution Intradomain routing protocols are responsible for routing within an autonomous system. Interdomain routing protocols, on the other hand, are responsible for routing traffic between autonomous systems.\nIntra AS Topology Before we jump into intradomain routing, let\u0026rsquo;s take a look at what a topology might look like inside a single autonomous system. A topology inside an AS consists of nodes and edges that connect them. The nodes are sometimes called points of presence, or PoPs. A PoP is typically located in a dense population center, so that it can be close to the PoPs of other providers for easier interconnection and also close to other customers for cheaper backhaul to customers that may be purchasing connectivity from this particular AS. The edges between pops are typically constrained by the location of fiber paths, which for the sake of convenience typically parallel major transportation routes such as railroads and highways.\nHere\u0026rsquo;s an example of a single AS topology which is the Abilene Network, which is a research network in the United States. Each of these locations would be considered a PoP, and each of these PoPs may have one or more edges between them. Georgia Tech is an autonomous system that connects at the Atlanta PoP of the Abilene Network.\nHere\u0026rsquo;s a close up of the Abilene Network in the south eastern U.S. The Abilene network connects to other universities in the southeast near Atlanta and an internet exchange point called SOX, or southern crossroads. Now, thus far we\u0026rsquo;ve just talked about the topology of an autonomous system, which essentially defines the graph. The next step is to compute paths over that topology, a process called routing. Routing is the process by which nodes discover where to forward traffic so that it reaches a certain node. There are two types of intradomain routing. One is called distance vector, and the other is called link state. In the rest of this lesson we\u0026rsquo;ll explore the two different types of intradomain routing and the advantages and disadvantages of each of them. Let\u0026rsquo;s first take a look at distance vector routing.\nDistance Vector Routing In distance vector routing, each node sends multiple distance vectors to each of its neighbors, essentially amounting to copies of its own routing table. Routers then compute costs to each destination in the topology based on shortest available path. Distance vector routing protocols are based on the Bellman-Ford algorithm. A node X\u0026rsquo;s forwarding table is based on the solution to the following equation. Suppose that node X is trying to find a shortest cost route to node Y. In this case node X is trying to find a path through some intermediate node, V, that minimizes the cost between X and V, and the already known shortest cost path between V and Y. Again, the solution to this equation for all destinations, Y, in the topology is X\u0026rsquo;s forwarding table. Let\u0026rsquo;s now take a look at distance vector routing by way of example.\nExample of Distance Vector Routing 1 Let\u0026rsquo;s suppose that we have a three node network with the costs on the edges as shown. Initially, each node has a single distance vector representing the shortest path cost to each other incident node in the graph. For example, the distance between x and x is obviously zero. And the shortest known distance between x and y from x\u0026rsquo;s perspective is one, the direct path. Similarly, the shortest known distance between x and z to x at the outset is five because all it knows is the direct path. Note that a shorter path between x and z exists via y, but x simply doesn\u0026rsquo;t know about it yet. Now in distance vector routing, every node send its vectors to every other adjacent node. And each node then updates its routing table according to the Bellman-Ford equation. Let\u0026rsquo;s look at what happens when node x learns of y\u0026rsquo;s distance vectors. Well in this case, the distance from x to z will be computed as the minimum of the sums of all distances to z through any intermediate node. So the cost between x and y is one, and the distance between y and z as discovered by y\u0026rsquo;s distance vector is two. Therefore, x can update its shortest cost distance to z as three. Similarly, x will receive a distance vector from z, five two zero, but of course, when it uses the Bellman-Ford equation to update its distances, again the distance between z and x will be updated from five to three. We can repeat this exercise at other nodes, as they receive distance vectors from other nodes in the topology, and quickly, every node in the network has a complete routing table. Now when costs decrease, the network converges quickly, but one problem is that when failures occurs, bad news can actually travel slowly.\nExample of distance Vector Routing 2 Let\u0026rsquo;s look at a different example. So for the sake of illustration, I\u0026rsquo;ve increased the cost between x and z to 50, and now everyone starts with a different set of initial distance vectors. Now eventually, after running the distance vector protocol, we would see the tables converge as such. Let\u0026rsquo;s suppose that the cost of the link between x and y suddenly increased from 1 to 60. Well now in this case, y would need to update its view of the shortest path between y and x. Now it\u0026rsquo;s no longer one, but it\u0026rsquo;s not 60 either. To see why let\u0026rsquo;s go back to our Bellman-Ford equation. We can see that y thinks it can get to z with a cost of two, and that z can get to x with a cost of three. So in fact it\u0026rsquo;s going to update this entry from one to five. Then it will tell it\u0026rsquo;s neighbor z its new distance vector. In other words, that now its distance to x is no longer one but five. At this point, z needs to re-compute it\u0026rsquo;s shortest path to x. Now, it knows that it can get to y with a cost of two but it thinks still that y can get to x with a cost of five. Therefore, this entry is no longer three but seven. And now z sends its new distance vector back to y. Y then updates it\u0026rsquo;s distance vector for z and this process continues. So, then y thinks it is now nine units away from x. So z has to do this all over again and now z thinks that its shortest path is two plus nine or 11. Now this process repeats of course until z finally realizes that it has a shorter path of 50 directly through x after this counting up process exceeds the value of 50.\nThis problem is called the count to infinity problem, and the solution is called poison reverse. The idea here is that if y must route through z to get to x in its table, as it did here, then y advertizes an infinite cost for the destination x to z. So instead of sending five, zero, two, y would send infinity, zero, two. This would thus prevent z from routing back through y, and immediately, it would choose the shortest path to x, of path cost 50.\nRouting Information Protocol An example of a distance vector routing protocol is the routing information protocol or RIP. The first version of RIP was defined in 1982 where edges had unit cost, and infinity for the count to infinity problem was 16. Table refreshes occur every 30 seconds and when an entry changes, it sends a copy of that update to all of its neighbors except for the one that induced the update. This rule is sometimes called the split horizon rule. The small value for infinity ensures that the count to infinity doesn\u0026rsquo;t take very long and every round has a time out limit of 180 seconds which is basically reached when a router hasn\u0026rsquo;t received an update from a next hop for six 30 second periods. In practice, when a router or link fails in RIP, things can often take minutes to stabilize. So because of problems such as slow convergence and count to infinity, protocol designers look to other alternatives.\nLink State Routing The prevailing alternative and the one that is used in most operational networks today is link state routing. In link state routing, each node distributes a network map to every other node in the network and then each node performs a shortest path computation between itself and all other nodes in the network. So, initially each node adds the cost of its immediate neighbors, D(v), and every other distance to a node that is infinite. Then each node floods the cost between nodes u and v to all of its neighbors. And the distance to any node v becomes the minimum of the cost between u and w plus the cost to w, or the current shortest path to v. The shortest path computation is often called the Dijkstra shortest path routing algorithm. Two common link state routing protocol are open shortest paths first or OSPF and intermediate system- intermediate system or IS-IS. In recent years, IS-IS has gained increasing use in large internet service providers and is the more commonly used link state routing protocol in large transit networks today. One problem with link state routing is scale. The complexity of a link state routing protocol grows as n cubed where n is the number of nodes in the network.\nCoping with Scale Hierarchy One way of coping with scale is to introduce hierarchy. OSPF has a notion of areas, and IS-IS has an analogous notion of levels. In a backbone network, the network\u0026rsquo;s routers may be divided into levels, or areas, and the backbone itself may have its own area. In OSPF, the backbone area is called area zero, and each area in the backbone that\u0026rsquo;s not in area zero has an area zero router. The area zero routers perform shortest path computations and the routers in each of the other areas independently perform shortest path computations. Now paths are computed by computing the shortest path within an area, or, if the path must leave an area, it\u0026rsquo;s computed by stitching together the shortest path to the area zero backbone router, and then the shortest path across area zero followed by another intra-area shortest path.\nInterdomain Routing We\u0026rsquo;re now moving on to cover interdomain routing or routing between ASes. Recall that internet routing consists of routing between tens of thousands of independently operated networks, or autonomous systems. Each of these networks operates in their own self-interest and have independent economic and performance objectives, and yet they must cooperate to provide global connectivity so that when you\u0026rsquo;re sitting at home, you can retrieve content that might be hosted at the Georgia Tech network.\nNow, each independently operated network is called an autonomous system, or AS. And each AS advertises reachability to some destination by sending what are called route advertisements or announcements.\nThe protocol that ASes use to exchange these route advertisements is called the Border Gateway Protocol, or simply, BGP. A route advertisement has many important attributes, but for now, let\u0026rsquo;s just talk about three. Now a router here, let\u0026rsquo;s say on the Comcast network, might receive a route advertisement, typically from its neighboring AS. That route advertisement might contain a destination prefix, such as the IP prefix for Georgia Tech. Then it might contain what\u0026rsquo;s called a next hop IP address, which is the IP address of the router that the Comcast router must send traffic to, to send traffic along that route. Typically that next hop IP address is the IP address for the first router in the neighboring network. And the Comcast router knows how to reach that next hop IP address because its border router and the border router in the neighboring AS are on the same subnet. Typically this might be a /30 subnet, therefore this IP address is reachable from Comcast\u0026rsquo;s border. A third important attribute is what\u0026rsquo;s called the AS path, which is a sequence of what are called AS numbers that describe the route to the destination. Now strictly speaking, the AS path is nothing more than the sequence of ASes that the route traversed to reach the recipient AS. So for example, Georgia Tech\u0026rsquo;s AS number is 2637 and Abilene\u0026rsquo;s is 10578 so the AS path that Comcast would hear if it received a route advertisement from Abilene for Georgia Tech, would be 10578 followed by 2637. So in the remainder of the lesson we\u0026rsquo;ll look at other BGP route attributes. But these are essentially the three most important because they describe how to stitch together an interdomain path to a global destination. So we have the destination IP prefix for the destination that a router might want to send traffic to; the next hop, which is the IP address for the router for the next hop along the path; and finally, the AS path, which is the sequence of ASes that the route traversed en route to the AS that\u0026rsquo;s hearing the announcement. The last AS number on the AS path is often called the origin AS, because that is the AS that originated the advertisement for this IP prefix. In this case, the origin AS is 2637, or Georgia Tech, because it is the AS that originated the announcement for this prefix.\nInterdomain Routing 2 Now thus far, we\u0026rsquo;ve talked about interdomain routing BGP, or the border gateway protocol, as consisting of route advertisements solely between border routers of adjacent autonomous systems. In fact, this is a specific type of BGP called external BGP, or eBGP. But in fact, as we know, each one of these autonomous systems has routers of its own, inside. Those routers also need to learn routes to external destinations. The protocol that is used to transmit routes inside an autonomous system for external destinations, is called internal BGP or iBGP. Okay, so to review, external BGP is responsible for transmitting routing information between border routers of adjacent ASes about external destinations. And internal BGP is responsible for disseminating BGP route advertisements about external destinations to routers inside any particular AS. Note the distinction between iBGP and an intra-domain routing protocol or an IGP.\nIGP vs iBGP The IGP or the intra-domain routing protocol, disseminates routes inside an AS to internal destinations whereas iBGP or internal- border gateway protocol, disseminates routes inside an AS to external destinations. So let\u0026rsquo;s suppose that a router inside AS A is trying to reach a destination inside AS B. AS A would learn the route via eBGP and the next hop of course, at this router, would be the border router at B. And now a router inside autonomous system A would learn the route to B via iBGP. Now the BGP next stop, would be the border router. And so, this router inside AS A, needs to use the IGP, to reach the iBGP next hop.\nProtocol Quiz So as a quick quiz, which routing protocol is responsible for disseminating routes inside an AS to external destinations? Is it the IGP? Is it iBGP. Or is it eBGP?\nProtocol Solution iBGP is responsible for disseminating routes inside an AS about destination IP prefixes that are located outside that AS. The iBGP next hop is typically a next hop IP address that is reachable via the ASes intradomain routing protocol, or IGP.\nBGP Route Selection Let\u0026rsquo;s now take a quick look at BGP route selection. It is often the case that a router on a particular autonomous system might learn multiple routes to the same destination. In this case, a router on autonomous system one, might learn a route to a destination in AS4 via both AS2 and AS3. In this situation. The router in AS one must select a single best route to the destination among the choices. The selection among multiple alternatives is known as the BGP route selection process. Let\u0026rsquo;s now take a quick look at that process.\nBGP Route Selection Process The first step in the BGP route selection process is to prefer a route with the higher local preference value. The local preference value is simply a numerical value that a network operator in the local AS can assign to a particular route. This attribute is purely local. It does not get transmitted between autonomous systems, so it is dropped in eBGP route advertisements. But it allows a local network operator the ability to explicitly state that one route should be preferred over the other. Among routes with equally high local preference values, BGP prefers routes with shorter AS path length. The idea is that a path might be better if it traverses a fewer number of autonomous systems. The third step involves comparison of multiple routes advertised from the same autonomous system. The multi-exit discriminator (MED) value allows one AS to specify that one exit point in the network is more preferred than another. So lower MED values are preferred, but this step only applies to compare routes that are advertised from the same autonomous system. Because the neighboring AS sets the MED value on routes that it advertises to a neighbor, MED values are not inherently comparable across routes advertised from different ASes. Therefore this step only applies to routes advertised from the same AS. Fourth, BGP speaking routers inside an autonomous system will prefer a BGP route with a shorter IGP path cost to the IGP next up. The idea here is that if a router inside an autonomous system learns two routes via iBGP then it wants to prefer the one that results in the shortest path to the exit of the network. This behavior results in what is called \u0026ldquo;hot potato\u0026rdquo; routing, where an autonomous system sends traffic to the neighboring autonomous system via a path that traverses as little of its own network as possible. Finally, if there are multiple routes with the highest possible local preference, the shortest AS path and the shortest IGP path, the router uses a tiebreak to pick a single breaking route. This tiebreaking step is arbitrary. It might be the most stable, or the route that\u0026rsquo;s been advertised the longest. But often, to induce determinism, operators typically prefer that this tie breaking step is performed based on the route advertisement from the router with the lowest router ID, which is typically the neighboring router\u0026rsquo;s IP address. Let\u0026rsquo;s now take a closer look into local preference, AS path length, multi-exit discriminator, and hot potato routing. Now as I mentioned, the first step in the router selection process is for routers to prefer routes with higher local preference values. Now an operator can actually set the local preference value on incoming BGP route advertisements to affect which route a router ultimately selects. Let\u0026rsquo;s see how this works.\nLocal Preference Now, a router in AS1 might learn two routes to a destination, one via the AS path 2-4 and the other via the AS path 3-4. Local preference, or simply, local pref, allows an operator to configure the router to assign different preference values to each of the routes that it learns. The default local preference value is 100. But if the operator prefers that this router select the path through AS two, it can configure the router to set a higher local preference for that route such as 110. This results in this router selecting the route through AS two and sending traffic to the destination in AS four via AS two. In this way an operator can adjust local preference values on incoming routes to control outbound traffic or to control how traffic leaves its autonomous system en route to a destination. This is extremely useful in configuring primary and back up routes. For example, here the route though AS two might be the primary route ,and the route through AS three, is the backup route. Now typically, as I mentioned, local preference is used to control outbound traffic. But sometimes autonomous systems can attach what\u0026rsquo;s called a BGP community to a route to affect how a neighboring autonomous system sets local preference. A community is nothing more but a fancy jargon word for a tag on a route. So let\u0026rsquo;s suppose that AS four wanted to control inbound traffic by affecting how AS two or AS three set local preference. In this case, let\u0026rsquo;s suppose that AS two wanted traffic to arrive via AS three, its primary, rather than by AS two, its backup. In this case, AS two might advertise its BGP routes with primary and backup communities. The backup community value might cause a router in AS two to adjust its local preference value, thus affecting how AS two\u0026rsquo;s outbound traffic choices are made. So, again local preference is used to control outbound traffic, in this case AS two\u0026rsquo;s outbound traffic decision. But the use of a BGP community on the route advertisement can sometimes be used to cause a neighboring AS to make different choices regarding it\u0026rsquo;s outbound traffic, thereby, allowing an AS to specify a primary or back up path for incoming traffic. This type of arrangement requires prior agreement.\nMultiple Exit Discriminator Let\u0026rsquo;s suppose that two autonomous systems connect in two different cities, San Francisco and New York. Let\u0026rsquo;s further suppose that AS 1 wants traffic to destination d to enter via New York City, rather than via the peering link in San Francisco. Well, remember that all things being equal, routers inside AS 2 will select the BGP route with the shortest IGP path cost to the next hop, resulting in hot potato routing. So some routers will select the San Francisco egress, and other routers might select the New York egress. To override this default hot potato routing behavior, AS1 might advertise its BGP routes to AS2 with MED values. For example, if the MED value on the route learned at the border router in New York was 10, and the MED value from the route learned from the router in San Francisco was 20, then instead of performing hot potato routing, all of these routers that would ordinarily be closer to the San Francisco egress, would instead pick the route learned via the New York egress because the preference for a lower MED value comes before the preference for a next hop with the lower IGP path process. So all of these routes would instead be carried over AS 2\u0026rsquo;s backbone network and exit via New York. Thus MED overrides hot potato routing behavior allowing an AS to explicitly specify that it wants another neighboring AS to carry the traffic on its own backbone network, rather than dumping the traffic at the closest egress and forcing traffic across the neighbor\u0026rsquo;s backbone. MEDs are typically not used in conventional business relationships, but they\u0026rsquo;re sometimes used, for example, if AS 1 does not want AS2 free riding on AS 1\u0026rsquo;s backbone network. So effectively MED allows AS 1 to say, yes, I will connect or peer with you, but it is your job to carry the traffic long distances across the country. This mechanism is sometimes used when a transit provider peers with a content provider, and the transit provider doesn\u0026rsquo;t want the content provider essentially getting free transit through the neighboring AS.\nIn the absence of MED overriding any behavior, typically what will happen is a router inside AS 2 would learn multiple routes via internal BGP to different egress points for the same destination d, and it would simply pick the next hop, or the egress router with the lowest IGP path cost, in this case, 5. It\u0026rsquo;s very common practice to set these IGP costs in accordance with distance, or propagation delay, thus resulting in routers inside the AS picking shorter paths. Now one problem with this notion of hot potato routing is that a very small change in IGP path cost can result in a lot of BGP routing changes. Remember that it\u0026rsquo;s probably not just one destination that\u0026rsquo;s being routed through the San Francisco egress, but maybe tens of thousands of routes. So a single IGP path cost change can result in rerouting of tens of thousands of IP prefixes in BGP. People have looked at various ways to improve the stability of BGP routing by decoupling the IGP and the BGP in this part of the route selection process.\nInterdomain Routing Business Models So now we\u0026rsquo;re going to look at Interdomain Routing Business Models. So the one thing to remember about interdomain routing is that it\u0026rsquo;s really all about routing money. Let\u0026rsquo;s consider this AS that wants to send traffic to a particular destination. Well, in the internet there are two different types of business relationships: a customer-provider business relationship, where money flows from customer to provider regardless of the direction that traffic flows; the other type of business relationship is a peering relationship where an AS can exchange traffic with another AS free of charge. This is sometimes also called settlement-free peering. So already you can see given three possible ways to reach the destination. This AS is first going to prefer a route through its customer, because regardless of the direction of traffic on this link, money is always flowing from the customer. The peering link is the second most preferable because it\u0026rsquo;s free. And the least preferable route is through the provider, because the AS has to pay money every time it sends traffic on this link. This leads to the basic rules of preference in interdomain routing, where customer routes are preferred over peer routes, which are in turn preferred over provider routes.\nThe other consideration that an AS has to make is filtering, or export decisions. In other words, given that an AS learns a route from its neighbor, to whom should it re-advertise that route? To understand filtering and export decisions, let\u0026rsquo;s add a couple more AS\u0026rsquo;s to the graph. Let\u0026rsquo;s add another peer, and let\u0026rsquo;s add another provider. Let\u0026rsquo;s call this AS in the middle of the picture Cox Communications. This ISP might have smaller regional customers and it might also buy transit connectivity from other providers. Now let\u0026rsquo;s suppose that this AS learns routes to a destination via its customer, its peer, and its provider. Now we already have established that it would prefer the customer route, so that it can make money by sending traffic to that destination. But what about filtering decisions? Well, routes that are learned from a customer, Cox of course would want to re-advertise to everyone else, because the more people use that route, the more money Cox makes. Therefore a route that\u0026rsquo;s learned from a customer, gets advertised to everybody else. On the other hand, a route that\u0026rsquo;s learned from a provider, if it were actually selected, would of course, only be advertised to customers. It wouldn\u0026rsquo;t make any sense to take a route like this and advertise it to another provider. The reason, of course, is that money is flowing in the direction of the providers. So any route that\u0026rsquo;s learned from a provider would never be advertised to another provider, because it would result in Cox essentially becoming a transit provider between two of its own providers and paying them both for the privilege of carrying that traffic. So routes learned from a provider would only ever be advertised to other customers. And similarly, routes from peers would only be advertised to other customers, not to other peers or other providers. So to summarize, interdomain routing has both ranking rules, where, given multiple choices, an AS might prefer a customer route over a peer route over a provider route. And then, given that it selected a particular route from either a customer, a provider, or a peer, it makes different decisions about where to re-advertise that route to other neighboring ASes. Now as it turns out, if every AS in the internet followed these rules exactly, then routing stability is guaranteed. Now you might wonder, isn\u0026rsquo;t routing stability guaranteed already? And it turns out that it isn\u0026rsquo;t.\nInterdomain Routing Can Oscillate! In fact, interdomain routing can oscillate indefinitely. To see why, consider the following 4 AS topology, where each AS specifies preferred paths, presumably via local preference. So each AS prefers the AS in the clockwise direction, rather than the shorter, direct path. Now it\u0026rsquo;s pretty easy to see that there\u0026rsquo;s no stable solution. Let\u0026rsquo;s suppose that we started off with everybody selecting the direct path. Well, in this case, any one of these ASes would notice that it has a more preferred path. So for example, AS 1 would see that because AS 3 has picked the direct path, then, in fact, it could prefer a situation where oscillations can occur indefinitely. Similarly, here now AS 3 sees that it has a more preferred path, 3 2 0, so it might switch to that.\nIn doing so, it breaks AS 1\u0026rsquo;s path. 1 3 0 no longer works. So AS1 has to switch back to its less preferred direct path, but now we\u0026rsquo;re in the same situation all over again because now AS2\u0026rsquo;s preferred path becomes available via 1, so AS 2 now reroutes, and AS 3\u0026rsquo;s most preferred path, 3 2 0, no longer works so it must switch to the direct path.\nNow, it\u0026rsquo;s very easy to see that this oscillation continues ad infinitum. This particular pathology was first discovered by Varadhan, Govindan, and Estrin, in a paper called persistent route oscillation in interdomain routing, in 1996. Later, Tim Griffin formalized this pathology and derived conditions for stability. Those stability conditions came to be known as a BGP correctness property called safety. It turns out that if ASes follow the ranking and export rules that we discussed, that safety is guaranteed. But, there are various times when those rules are violated. Business relationships, such as regional peering and paid peering, can occasionally cause those conditions to be violated. So as it turns out, to this day, BGP is not guaranteed to be stable in practice, and many common practices result in the potential for this type of oscillation to occur.\n"
},
{
	"uri": "/7646/machine-learning-algorithms/",
	"title": "Machine Learning Algorithms",
	"tags": [],
	"description": "",
	"content": " Machine Learning Algorithms In most cases, machine learning algorithms are focused on building a model. Then, the model can be used to take inputs and give outputs based on the model. The model is a tool to predict outputs based on the inputs. In our case, we will be using models to take information about stocks and predict their future prices.\nSo we use machine learning to take historical data and generate a model. When we want to use it, we give the model observations, $\\vec{x_i}$, and it gives us predictions, $y$. Examples of good inputs (predictive factors) are:\n Price momentum Bollinger value Current price  while examples of outputs would be:\n Future price Future return  We’ll first talk about Supervised Regression Learning.\nRegression and Modeling Supervised Regression Learning means that we’ll provide (supervised learning) a bunch of example data $(x, y)_i$ and allow the model to make a numerical prediction (regression). There are two main types of regression techniques:\n Linear regression (parametric) k-nearest neighbor (kNN) (instance-based), the more popular approach Decision trees Decision forests  Assessing a Model Assessing a model is much like predicting prices as it uses indicators to judge the effectiveness of the model. The first indicator is root mean square error (RMSE), which is as follows\n$\\sqrt{\\dfrac{\\sum (ytest - ypredict)^2}{N}}$  The error that is important is that of test data, which is outside of the training data. Typically 60% of the data is used for training, and 40% is used for testing. However, sometimes there isn’t enough data to adequately evaluate a learning algorithm, in which case a method called cross validation is used. This method slices the data into chunks, typically fifths. One is chosen as test and the rest are for training, then a different chunk is chosen to be the test and another trial is run. For financial data, we don’t want to accidentally look forward in time, so we would only use roll forward cross validation. This simply demands that all the training data is before the test data.\nThe second metric for how well an algorithm is working is the correlation of the test data and predicted values. Strong correlation, close to $\\pm1$, indicates a good algorithm whereas a weak correlation, close to zero, indicates a poor algorithm. Correlation and RMSE are excellent indicators on how well an algorithm is doing, but we might also want to fine tune an algorithm; we want to answer the question ”when are we trying too hard to fit data?”. This is where overfitting comes into play. Overfitting is the point at which error for training data is decreasing while error for test data is increasing.\nTypes of Learners Ensemble Learners Ensemble learners are composed of several different learners, which could include kNN, regression, and decision tree learners in one. The output of this learner is then simply a combination of the learners’ answers, which is typically an average of the outputs.\nBagging and Boosting Boot-strap aggregating, or bagging, only uses one algorithm, but many different models. If the training data is separated into learning instances where there’s a total of $n$ instances, then each model is fed a bag of these instances. Each bag is composed of $n\u0026rsquo;$ learning instances that are randomly selected with replacement, so instances may show up more than once in the same bag. These are used to train m models and the result is the average of all the outputs. Boosting builds each subsequent bag based on the results of the last. Training data is also used to test the models, and a model’s predicted data showing significant error is weighted to more likely be in the next bag for the next model. The process is continued for the desired number of models, and the results are averaged. Although this could be advantageous in predicting outliers, it’s also more susceptible to overfitting.\nReinforcement Learning As seen in figure 4.1, reinforcement learning describes the interaction of a robot with its environment. The robot performs an action, which has an effect on the environment and changes its state. The robot observes the change in state and its associated reward and makes decisions to maximize that reward.\nReinforcement learning also describes the problem that is how to go about maximizing the reward. In the stock market, the reward is return on trades, and we want to find out how to maximize returns. This problem is complicated by time constraints. The value of future gains diminishes with time, so it’s unreasonable to use an infinite horizon on which to base returns. However, optimizing returns over too short a time may limit rewards from seeing a much larger overall gain.\nMarkov Decision Problems What we’ve been talking about is called a Markov decision problem. Here’s how the problem is formalized.\nWe have:\n Set of states $S = {s1, \u0026hellip; , s_n}$ Set of actions $A = {a1, \u0026hellip; , a_n}$ Transition function $T[s, a, s_0]$ Reward function $R[s, a]$  What we’re trying to find is a policy $\\pi(s)$ that will maximize the reward. Unfortunately, we don’t know the $T$ or $R$, since that’s defined by the environment. So, the learner has to interact with the world and see what happens. Based on the reward, it can start generating policies.\nA way to encode this information is using experience tuples. Experience tuples are as follows: given a state $s_1$ and an action $a_1$ that we took, we were put into state $s'_1$ and got reward $r_1$. The tuple is shown like this:\n$\\langle s_1, a_1, s_1\u0026#39;, r_1 \\rangle$  Now, we can rename $s_1\u0026rsquo;$ to $s_2$, since that’s our new state, and then take a new action and see what happens, and we get a new tuple:\n$\\langle s_2, a_2, s_2\u0026#39;, r_2 \\rangle$  And we repeat this for many different combinations of states and actions, and then we’ll use the tuples to generate the policy. There are two ways to generate the policy:\n Model-based: For this method, we generate a model of $T[s, a, s_0]$ based on statistical analysis of the tuples. We look at a particular state and a particular action and see the probability of transitioning to another state. Same thing with $R[s, a]$. Then, we can use policy or value iteration to solve it. Model-free: This method keeps the data around and uses the original tuples to determine what the new state will be for a certain action. This is Q-learning.  Q-Learning Q-Learning is a model-free approach, which means that it doesn’t need to have any sort of model of the transition function $T$ or the reward function $R$. It builds a table of utility values as the agent interacts with the world. These are the Q values. At each state, the agent can then use the Q values to select the best action. Q-Learning is guaranteed to give an optimal policy, as it is proven to always converge.\n$Q$ represents the value of taking action $a$ in state $s$. This value includes the immediate reward for taking action $a$, and the discounted (future) reward for all optimal future actions after having taken $a$.\nHow do we use Q? What we want to find for a particular state is what policy, $\\prod(s)$ we should take. Using Q values, all we need to do is find the maximum $Q$ value for that state.\n$\\prod(s) = \\underset{a}{\\arg\\max}(Q[s,a])$  So, we go through each action a and see which action has the maximum $Q$ value for state $s$. Eventually, after learning enough, the agent will converge to the optimal policy, $\\pi^*(s)$, and optimal $Q$ table, $Q^*[s, a]$.\nHow do we get Q? To use the Q table, first we must generate it by learning. How do we go about that? Well it’s similar to previous learning algorithms, in that we provide it training data for which we know the outcomes. We then iterate over time and take actions based on the current policy (Q values), and generate experience tuples $\\langle s, a, s\u0026rsquo;, r \\rangle$ and generate the Q values based on the experience.\nIn a more detailed fashion:\n Initialize the Q table with small random values Compute $s$ Select $a$ Observe $r, s$ Update $Q$ Step forward in time, then repeat from step 2.  To update Q, we first need a formula to decide what it should be. What we can do is assign a learning rate, $\\alpha$, to weight the new observations. We can therefore update $Q$like this:\n$Q\u0026#39;[s, a] = Q[s, a] \u0026#43; \\alpha(\\textrm{improved estimate} - Q[s, a])$  As you can see, $Q$ converges as the improved estimate is the same as the current estimate (we’re at the best $Q$). Now, we need to know what the improved estimate is:\n$\\textrm{improved estimate} = \\textrm{immediate returns} \u0026#43; (\\textrm{discounted future rewards})$  Then, replacing discounted future rewards with the actual way of calculating it, and rearranging to only use the current value of $Q$ once, we find that the formula to calculate the new value of $Q$ for a state-action pair hs, ai, the formula is:\n$Q\u0026#39;[s, a] = (1 - \\alpha)Q[s, a] \u0026#43; \\alpha(r \u0026#43; \\gamma Q[s\u0026#39;, \\underset{a\u0026#39;}{\\arg\\max}(Q[s\u0026#39;, a\u0026#39;])])$  where:\n $r = R[s, a]$ is the immediate reward for taking an action $a$ in the state $s$ $\\gamma \\in [0, 1]$ is the discount factor to reduce the value of future rewards $s\u0026rsquo;$ is the resulting next state $\\underset{a\u0026rsquo;}{\\arg\\max}(Q[s\u0026rsquo;, a\u0026rsquo;])$ is the action which maximizes the Q-value among all possible actions $a\u0026rsquo;$ from $s\u0026rsquo;$, and $\\alpha \\in [0, 1]$ is the learning rate used to vary the weight given to new experiences compared to past Q-values. It’s typically around 0.2.  Exploration The success of a Q-learning algorithm depends on the exploration of the state-action space. If you only explore a small subset of it, you might not find the best policies. One way to ensure that you explore as much as possible is to introduce randomness into selecting actions during the learning phase. So basically, you see first whether you want to take the action with the maximal Q value or choose a random action, then if you take a random action, each action gets a probability which decreases over subsequent iterations.\nQ-Learning for Trading Now that we know what Q-learning is, we need to figure out how to apply it to the context of trading. That means that we need to define what state, action, and reward mean. Actions are straightforward, as there are basically three of them:\n Buy Sell Do Nothing  Our rewards can be daily returns or cumulative returns after a trade cycle (buy→sell). However, using daily returns will allow the agent to converge on a Q value more quickly, because if it waited until a sell, then it would have to look at all of the actions backwards until the buy to get that reward.\nNow, we just need to figure out how to determine state. Some good factors to determine state are:\n Adjusted Close/Simple Moving Average Bollinger Band value P/E ratio Holding stock (whether or not we’re holding the stock) Return since entry  Discretization Our state must be a single number so we can look it up in the table easily. To make it simpler, we’ll confine the state to be an integer, which means we need to discretize each factor and then combine them into an overall state. Our state space is discrete, so the combined value is the overall state. Say we have a state like this: The discretized state could be: 2950.\nTo discretize, what we do is take the data for a factor over its range, then divide it into n bins. Then we find the threshold by iterating over the data by the step size and taking the value at each position.\nstepsize = size(data) / n data.sort() for i in range(0, steps): threshold[i] = data[(i +1) * stepsize]  Problems with Q-Learning One main problem with Q-Learning is that it takes a lot of experience tuples to converge to the optimal Q value. This means the agent has to take many real interactions with the world (execute trades) to learn. The way this has been addressed is by using Dyna.\nDyna-Q Dyna is designed to improve the convergence of Q learning by building a model of $T$ and $R$ and then using Q learning to make decisions based on the model. However, the Q learning portion is still model-free, so it’s a mix of both.\nSo we do the Q-Learning steps, but after we take an action, we update the model of $T$ and $R$ with the new data, simulate a bunch of experiences based on the model, then update $Q$ based on these simulated experiences. To simulate the experiences, we basically generate random states and actions, and then find the new states/rewards based on the transition function and reward function.\nLearning T To figure out a model for $T$, what we can do is count the number of times that a transition to $s\u0026rsquo;$ by using the action $a$ in state $s$ occurred, then divide that by the total number of transitions to figure out the probability where $T_c$ is the number of times the transition occurred.\n$T[s, a, s\u0026#39;] = \\dfrac{T_c[s, a, s\u0026#39;]}{\\sum_{i} T_c[s, a, i]}$  Learning R To finalize the model, we need to find our expected reward, $R[s, a]$. Whenever we interact with the world, we get an immediate reward, $r$. We can use this to update our model for $R$ in a similar way to updating the $Q$ values where $\\alpha$ is again the learning rate:\n$R\u0026#39;[s, a] = (1 - \\alpha)R[s, a] \u0026#43; \\alpha r$  Conclusion So a summary of how Dyna-Q works is the following:\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/6601/",
	"title": "6601",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/naming-addressing-and-forwarding/",
	"title": "Naming Addressing and Forwarding",
	"tags": [],
	"description": "",
	"content": " Naming, Addressing \u0026amp; Forwarding IP Addressing In this lesson we will be covering IP addressing. In particular we will be covering IPv4 address structure and allocation. IP stands for Internet Protocol, and version four is the version of the protocol that is widely deployed on the Internet to date. Each IP address is a 32-bit number. And that 32-bit number is formatted in what is called dotted quad notation. For example, the IP address for http://www.cc.gatech.edu, is 130.207.7.36. And this is just a convenient way of writing a 32 bit number. So 130 represents 8 bits, and 207 is another eight bit number, 7 is another eight bit number, as is 36. This structure allows for two to the 32, or about 4 billion Internet addresses. Now that sounds like a lot of addresses. As it turns out it\u0026rsquo;s actually not enough, and we\u0026rsquo;re running out of IP addresses, as I\u0026rsquo;ll discuss in a later lesson. But even if we only had to deal with two to the 32 Internet addresses, that\u0026rsquo;s still a lot. Think of it if you have to store every single IP address as an entry in a table. Very quickly that becomes an extremely large table where look- ups can be slow and the memory required to store such a large table might be expensive. So what we need is a more concise way of representing groups of IP addresses. There are different ways to group IP addresses and we\u0026rsquo;ll look at the prevailing method in the next part of the lesson. But first, let\u0026rsquo;s look at how this was done before 1994.\nPre 1994 Classful Addressing Before 1994, addresses was divided into a Network ID portion and a Host ID portion. So if we take our 32-bits, suppose the first bit is a zero. Note that that\u0026rsquo;s half of all IPv4 address space. Anything that starts with a 0 is going to be known as a class A address and the next 7 bits will represent the network ID or the network that owns this portion of address space. The remaining portion of the address space is dedicated for hosts on that network. In this case, any class A network can support up to two to the twenty-fourth hosts. Addresses that started with one zero were designated as class B networks, where the first 16 bits signified the Network ID and the remaining 16 bits signified the Host ID for that network. Note here that each class B address range represents about one sixty-five thousandth of all internet address space. So, discounting the first two bits which indicate that this is a class B network, we have about 2 to the 14th, unique class B\u0026rsquo;s, each of which can have two to the sixteenth, or 65,000 hosts on each network. Class C\u0026rsquo;s use the first 24 bits for the net ID and the remaining 8 for host ID. So each class Cnetwork essentially can have only 255 hosts on it.\nThis plot shows the BGP routing table size as a function of the year, starting in 1989, and going up to the internet routing table is quite small. It started at less than 5,000 prefixes. By comparison now we have about 500,000 IP prefixes in the internet routing table. But we can see in this period, that internet routing table growth began to accelerate, in particular the growth rates were exceeding the advances in hardware and software capabilities. And, in particular, we began to run out of the class C address allocation. There were far more networks that needed just a handful of IP addresses, such as a class C address space could provide, and yet because only a certain range of the IP address space could be used for class C addresses we began to run out fairly quickly. So there began to be a need for more flexible allocation. The solution to this problem is something called classless interdomain routing, or CIDR. Something that we\u0026rsquo;ll cover in the next lesson.\nIP Addressing Quiz As a quick quiz, suppose you have a class A address space which means that the network ID is eight bits. How many hosts can a single class A network support? Is it to the 2 to the 8th, 2 to the 16th, 2 to the 24th or 2 to the 32nd?\nIP Addressing Solution Each Class A address space has a network ID of 8 bits meaning that there are 24 bits that remain for the host ID. This means that each Class A network can support up to 2 to the 24th hosts.\nIP Address Allocation Let\u0026rsquo;s take a look at how IP address space is allocated to Internet service providers. At the top of the hierarchy is an organization called the Internet Assigned Numbers Authority, or IANA. IANA has the authority to allocate address space to what are called regional routing registries. For Africa, the regional registry is called AFRINIC, for Asia and Australia the registry is called APNIC, for North America, ARIN, for Latin America it\u0026rsquo;s LACNIC, and for Europe it\u0026rsquo;s RIPE. ARIN in turn allocates address space to individual networks, like Georgia Tech. The address space across registries is far from even.\nNow, this graph is from a journal article that\u0026rsquo;s a little bit dated now, but it gives you an idea of how many /8 address allocations have been allocated to each of the regional registries. So as of 2005 North America had 23 /8s allocated to it but the entire continent of Africa had only one. And the recent news is that IANA actually finished allocating all remaining /8 Internet address blocks, essentially meaning that we\u0026rsquo;re out of IPv4 address space. So when you hear that we\u0026rsquo;re out of IPv4 addresses, it doesn\u0026rsquo;t mean that you can no longer attach a new device to the Internet. There are various ways for coping with this pressure on address space. What that means is that IANA no longer has any more /8 blocks to give to these regional registries.\nQuerying an IP address using Whois and a routing registry, such as ra.net, will tell you the owner of that particular prefix. For example, if we run a Whois query on an IP address at Georgia Tech, it will tell us that that IP address is from a /16 allocation, that Georgia Tech is the owner of the prefix, and it\u0026rsquo;s associated with this autonomous system number. The routing registry entry also gives us some additional information, such as who to contact if we need to contact the owner of this address space.\nClassless Interdomain Routing The pressure on address space usage spurred the adoption of classless interdomain routing or CIDR which was adopted in 1994. The idea is that instead of having fixed network ID and host ID portions of the 32 bits, instead, we would simply have an IP address, and what is known as a mask, where the mask is variable length and indicates the length of the network ID. So, for example, suppose we have an IP address like 65.14.248.0/22. Well in this case our 32 bits look like so, but this doesn\u0026rsquo;t tell us how long the network IDand how long the host ID should be. The /22 indicates the mask length, which says that the first 22 bits should represent the network ID.\nNow the key is that this mask can be variable length. And the mask length no longer depends on the range of IP addresses that are being used. This allows those allocating IP address ranges, to both allocate a range that\u0026rsquo;s more fitting to the size of the network and also not have to be constrained about how big the network ID should be depending on where in the IP address space the prefix is being allocated from. Of course now the complication is that it\u0026rsquo;s possible to have overlapping address prefixes. For example, 65.14.248.0/24 overlaps with 65.14.248.0/22. The red prefix is actually a subset of the black one. So supposing these two entries both show up in an Internet routing table, what are we supposed to do? The solution is actually to forward on what\u0026rsquo;s called the longest prefix match, meaning that if a routing table has two overlapping entries that it should forward according to the entry that has the longest prefix, or the longest mask length. Intuitively that makes sense because the prefix with the longer mask length is more specific than the prefix with the shorter mask, or the larger prefix.\nLongest Prefix Match Let\u0026rsquo;s take a closer look at longest prefix match. So each packet has a destination IP address, which determines where the package should be forwarded next, and a router basically looks up a table entry that matches that address. So, for example, a forwarding table might have a number of prefixes in it, and many of these prefixes might be overlapping. But when we see an IP address, it may match on one or more prefixes in this table, you simply match that IP address to the entry in the forwarding table with the longest matching prefix. So the benefits of cider and longest matching prefix are efficiency, since prefix blocks can be allocated on a much finer granularity than with classful inter-domain routing, and the opportunity for aggregation if two downstream networks with more specific or longer prefixes, should be treated in the same way by an upstream network, who might simply aggregate two contiguous shorter prefixes into one forwarding table entry with a shorter prefix. For example, a benefit for aggregation might exist if two downstream networks A and B each had slash 16 address space allocated to them. But upstream, all the traffic always came through the same upstream network, C. If the rest of the internet only reached A and B via C, then the rest of the internet need only know about C\u0026rsquo;s address space which might be 12\u0026frasl;8. This might allow the upstream network to simply aggregate, or not announce these more specific prefixes, since they\u0026rsquo;re already covered by the less specific upstream prefix.\nNow cider had a significant effect on slowing the growth of the internet routing tables from 1994 to 1998. So, from 1994 to 1998, we see roughly linear growth in the number of prefixes in the internet routing table. Around 2000, fast growth in routing tables resumed.\nYou can see that growth here once again started to pick up a significant contributor to this growth, was a practice called multi-homing. Multi-homing can actually make it difficult for upstream providers to aggregate IP prefixes together, often requiring an upstream provider to store multiple IP prefixes for a single autonomous system. Sometimes those IP prefixes are contiguous and sometimes they aren\u0026rsquo;t. Let\u0026rsquo;s take a quick look at how multi-homing can stymie aggregation.\nMultihoming Frustrates Aggregation This example, a stub AS, in this case 30308, might receive IP address space, say, 12.20.249\u0026frasl;24, from one of its providers, such as AT\u0026amp;T, which happens to own 12.0.0.0/8. Now in this case AS 30308 wants to be multihomed. In other words, it wants to be reachable via two upstream Internet service providers. In this diagram, the two Internet service providers are AT\u0026amp;T and Verizon. To be reachable by both of these ISPs, AS 30308 has to advertise its prefix, which it received from AT\u0026amp;T via both AT\u0026amp;T and Verizon. The problem occurs when AT\u0026amp;T and Verizon want to advertise that prefix to the rest of the internet. Well, unfortunately, although AT\u0026amp;T might like to aggregate this prefix as I previously described, it can\u0026rsquo;t. If it did, Verizon would still be advertising the longer /24 via it\u0026rsquo;s upstream link. And because of longest prefix match, all of the traffic would then arrive via the Verizon link regardless of what AS 30308 wanted to have happened to that incoming traffic.\nAs a result, both AT\u0026amp;T and Verizon must advertise the same /24 to the rest of the internet. This results in an explosion of /24s in the global internet routing table. You can imagine, that if a lot\nof stub AS\u0026rsquo;s wanted to be multihomed, then suddenly, we\u0026rsquo;ve got a lot more /24s in the global routing table than might otherwise exist without multihoming.\nLongest Prefix Match to Control Inbound Traffic Now in a previous lesson, we looked at how AS path prepending, can be used to control inbound traffic. As it turns out, longest prefix match can also be used to control inbound traffic. Suppose that AS A owns 10.1.0.0/16, and it might advertise that prefix out both of its upstream links and that route might similarly be advertised further upstream. Now of course as we know from a previous lesson, given the advertisement of one prefix upstream, AS D is going to pick one best BGP route along which to send traffic back to A. But let\u0026rsquo;s suppose that AS A wanted to balance that traffic across its incoming links. Well in that case, ASA could actually advertise routes for 2 more specific prefixes, effectively splitting the slash 16 in half, so in addition to advertising 10.1\u0026frasl;16, across both links, AS A might advertise 10.1\u0026frasl;17 on the top link and 10.1.128.0/17, the other half of the /16 on the bottom link. Now, if either link fails, the covering /16 will ensure that the prefix remains reachable by one of the two upstream links. But because longest prefix match wins, the traffic for 10.1.128 would now traverse the bottom link, and the traffic for 10.1/ would now traverse the top link, effectively sending traffic for half of the prefixes along the top path and traffic for the other half of the prefixes along the bottom path. Although we just explored a perfectly good reason to deaggregate a contiguous prefix, it turns out that sometimes autonomous systems may deaggregate larger prefixes unnecessarily.\nA report called the CIDR Report, which is released weekly, shows autonomous systems who are advertising IP prefixes that, at least according to observation, are continuous and could be aggregated. For example, the top offender for the week of December 12th, 2013, was AS6389. This single autonomous system is actually advertising more than 3,000 unique IP prefixes. The CIDR report analysis suggests that with appropriate aggregation, this autonomous system could instead advertise only 56 unique IP prefixes. Now this might be overly optimistic. As we just explored, there are perfectly good reasons to deaggregate a contiguous IP prefix into multiple smaller contiguous IP prefixes. But nonetheless, the report shows that there are probably a lot more IP prefixes in the Global Internet Routing table than there could be if AS\u0026rsquo;s took full advantage of aggregation.\nCIDR Quiz Let\u0026rsquo;s have a quick quiz about cider. So, how many IP addresses does a /22 prefix represent? Two to the 22, two to the 32, two to the tenth, or two to the eighth?\nCIDR Solution The /22 represents the length of the network ID, and the remaining 10 bits are for hosts in that /22 prefix. So those 10 bits reserved for the host for that /22 mean that this /22 prefix represents 2 to the tenth IP addresses.\nLookup Tables and How LPM Works (put with other slide) Okay, in this lesson, we will explore how lookup tables in routers are designed and how longest prefix match works; we\u0026rsquo;ll explore exact match versus longest prefix match and when each is used; we\u0026rsquo;ll explore IP address lookup in more depth; and finally, we\u0026rsquo;ll explore how longest prefix match is implemented in the form of tries.\nLookup Algorithm Depends on Protocol So, the look up algorithm that a router uses depends on the protocol that it\u0026rsquo;s using to forward packets, and the look up mechanism might be implemented with a variety of different algorithms or techniques. For example, MPLS, Ethernet, and ATM use an exact match look up. Exact matches can be implemented as a direct look up, an associative look up, hashing, or via a binary tree. IPv4 and IPv6 on the other hand are implemented with what\u0026rsquo;s called longest prefix match. We\u0026rsquo;ve already looked at longest prefix match a little bit in some lessons and, in this lesson we\u0026rsquo;ll look at it in a bit more detail as well as how it\u0026rsquo;s implemented. It might be implemented as a radix trie, a compressed trie, which is something that we will look at in this lesson, and it can also be implemented as a binary search on the prefix intervals. Ethernet based forwarding is based on exact match of a layer two address which is usually 48 bits long. It\u0026rsquo;s address is global, not just local to the link. And the range or size of the address is not negotiable. Now 2 to the 48th is far bigger than 2 to the 12th, therefore, it\u0026rsquo;s not possible to hold all the addresses in the table and use direct look up. The advantages of exact matches and Ethernet switches is that exact match is simple and the expected lookup time is small, or O of 1. But the disadvantages include inefficient use of memory. This potentially results in nondeterministic lookup time if the lookup might require multiple memory accesses. Lets now take a closer look at longest prefix match.\nIP Lookups Find Long Prefixes IP lookups find longest prefixes. Let\u0026rsquo;s suppose that we want to represent a particular IP address as one point in the space from zero to 2 to the 32 minus 1, or the range of all 32 bit IP addresses. Each prefix represents a smaller range inside this larger range of 32-bit numbers. Obviously, this is not to scale. Now these ranges, of course, might be overlapping, as is shown here, and the idea is that longest prefix match will match the smallest prefix for which the IP address range overlaps that of the specified IP address. So longest prefix match is harder to perform than exact match. For one, the destination IP address does not indicate the length of the longest matching prefix, so some algorithm needs to determine the length of the longest matching prefix, which in this case is 21. So we somehow need a way to search the space of all prefix lengths, as well as prefixes of a given length.\nLPM in IPv4 Exact Match Suppose, for example, that we wanted to implement longest prefix match for IPv4 using exact match. Now in this case we might take our network or our IP address, and send it to a bunch of different exact match tables. And then among the tables that had a match, we would select the longest, and then forward the packet out the appropriate output port. Of course, this is horribly inefficient, because we\u0026rsquo;d have to have tables for each of these links, and every time a packet arrived, we\u0026rsquo;d have to send it to each one of these 32 tables. This is extremely wasteful of memory.\nAddress Lookup Using Tries An alternative is to perform address lookups using a data structure called a trie. In a trie, prefixes are spelled out by following a path from the root. And to find the best prefix, we simply spell out the address in the trie. For example, let\u0026rsquo;s suppose we had the following table. Such a lookup table has entries of varying lengths. Let\u0026rsquo;s see how this might be encoded in a trie. In a trie, spelling out the bit one always takes us to the right, and spelling out the bit zero always takes us to the left. So to insert one one one star, we\u0026rsquo;d basically start here. One. One. One. And then we insert P1, and then we repeat this process. One zero star results in P2. One zero one zero results in P3. And one zero one zero one results in P4. If we want to insert one one one zero, insertion is easy. We can simply insert P5 as such. Look ups are easy, so for example let\u0026rsquo;s suppose we want to look up 10111. Well all we have to do, is spell this out in the trie. So we can follow 1-0-1 and now, we see that there\u0026rsquo;s no entry for 1011. So, we use the entry of the last node in the tree that we traverse that has an entry, in this case P2. Now this structure here is what\u0026rsquo;s called a single bit try. Single bit tries are very efficient. Note that every node in this try exists due to one of the five folding table entries that we\u0026rsquo;ve inserted in the try. So, a single bit trie is a very efficient use of memory. Updates are also very simple. We saw how easy it was, to insert the entry for P5. Unfortunately, the main problem is the number of memory accesses that are required to perform a lookup. For 32 bit address, we can see, that looking up the address in a single bit trie, might require 32 look ups, in the worst case, one for each bit. So it\u0026rsquo;s each bit in the address requires, one traversal in the trie, or one memory look up. So this could be very bad. At worst, 32 accesses in the worst case. To put this in perspective, an OC48 requires a 160 nanosecond lookup, or simply 4 memory accesses. So 32 accesses, is far too many, especially for high speed links.\nDirect Trie The other extreme, of course, is to use a direct trie where instead of 1 bit per look up we might have 1 memory access responsible for looking up a much larger number of bits. So, for example, we might have a two level try where the first memory access is dictated by the first 24 bits of the address, and the second memory access is dictated by last 8 bits of the address. Now here we can look up an entry in the forwarding table with just two memory accesses. The problem is that this structure results in a very inefficient use of memory, unlike the single bit trie. To see why, suppose that we want to represent a /16 prefix. Well unfortunately we have no way of encoding a lookup that\u0026rsquo;s just 16 bits. We have to rather encode 2 to the 8th identical entries, corresponding to the 2 to the 8th /24 prefixes that are contained in that /16, so this is extremely inefficient use of memory.\nDirect Trie Quiz As a quick quiz, suppose we have a direct trie, and the first level is 16 bits, the next level is eight bits, and the third level is the final eight bits. In the worst case, how many accesses would be required per lookup?\nDirect Trie Solution Because the Trie has a depth of three, in the worst case, a look up might require three memory accesses.\nDirect Trie Quiz 2 How many entries, would I need, to represent a /20 prefix?\nDirect Trie Solution 2 A /20 prefix is 2 to the 4th, or 16, /24\u0026rsquo;s. And I need to basically represent 16 entries, at the 24 bit level of the trie, or the second level, and therefore, I\u0026rsquo;d need 16 entries to represent, a /20 prefix.\nMemory Efficiency and Fast Lookup To achieve the memory efficiency of a single bit trie with the fast lookup properties of a direct trie, a compromise is to use what\u0026rsquo;s called a multi-bit trie, or a multi-ary trie. Let\u0026rsquo;s start with a binary trie, where one bit is resolved at each node. Here, the depth is big W, the degree of each node is two, and the stride for each lookup is one bit. Now we can generalize this to a multi-ary trie, where the depth is now W over K if the degree is 2 to the K, and each level resolves K bits. The binary trie is a simple case of the multi-ary trie, where K equals 1.\n4 ary Trie Let\u0026rsquo;s take a look at the 4-ary trie where k equals 2. Suppose we have the same forwarding table as before. But now, each node in the trie is responsible for resolving two bits. So if we take one one, and now we take one star, that\u0026rsquo;s one zero and one one. And now we basically have to put p in two places in the tree. One zero star results in just one entry. 1010 star results in two traversals, and 10101 star again represents two entries, for 101010 and 101011. Now suppose we want to look up 10111. Again, we can spell this out, 101, and we can see that we get no further than P2 and again, we match at P2.\nOne thing we can do to save space further is create what\u0026rsquo;s called a leaf-pushed trie. In such a setting, we can save our self some space. Instead of having these pointers, we can push these entries into the left and right side of this node, respectively.\nSo 10 becomes P1 on the left side and 11 becomes P1 on the right side. There are variety of other optimization algorithms, including one called Lulea and another called Patricia. Each of them use the same basic idea that we have explored here, except some of them like Lulea are a three level trie, and often they use a bitmap to compress out repeated entry such as those that exist here.\nAlternatives to LPM with Tries Now there are alternatives to implementing longest prefixes match with a trie. One could start with a content addressable memory or a CAM. Now a CAM is a hardware base route look up where the input is a tag and the output is a value. So, for example, the tag might be an address and the value might be the output port. Now the CAM really only supports exact match but it is an O of 1 lookup. There is something called a ternery CAM, where instead of exact matching In the tag, you can have 0, 1, or don\u0026rsquo;t care, or a star. The ternary CAM and in particular its support for a wild card permits an implementation of longest prefix match. One can thus have multiple matching entries, but prioritize the match according to the longest prefix in the ternary CAM.\nNAT and IPv6 Let\u0026rsquo;s now talk about various problems that resulted from IPv4 and the growth of the internet routing tables, and two different solutions to internet routing table growth: network address translation, or NAT, and IPv6. So the main problem that we are seeing is that IPv4 addresses have only 32 bits, which means that there can only be a total of 2 to the 32 unique IP addresses. Not only that, as we\u0026rsquo;ve seen, IP addresses are allocated in blocks, and fragmentation of this space can mean that IPv4 addresses can be quickly exhausted. In fact, we\u0026rsquo;ve already seen the last slash eight from IPv4 address space allocated to the registries. So we\u0026rsquo;re well on our way towards running out of IPv4 addresses. In some sense, you can say that we\u0026rsquo;ve essentially already run out. In this lesson, we\u0026rsquo;re going to look at two solutions: network address translation, or NAT, and IPv6, whose main feature is 128 bit addresses. Let\u0026rsquo;s first take a look at NAT.\nNetwork Address Translation NAT allows multiple networks to reuse the same private IP address space. Let\u0026rsquo;s suppose that we have two networks. These networks might be, for example, homes or they might be larger networks in regions of the Internet, where IPv4 address space is scarce, for example, in developing regions. What NAT allows these networks to do is reuse the same portion of internet address space. For example, a particular, special, private IP address space, is 192.168\u0026frasl;16. Other private IP address space is specified in RFC 3130. Now, obviously these two networks couldn\u0026rsquo;t coexist on the public Internet, because routers wouldn\u0026rsquo;t know if they got a packet destined for an IP address in this space, which network the packet should be sent to. What a NAT, or a Network Address Translator does, is take the private IP addresses that are behind the NAT and translate those IP addresses to a single, globally visible IP address.\nNow, to the rest of the Internet, network one appears to be reachable by a single IP address, 203.178.1.1, and network two is reachable via a single distinct global IP address 133.4.1.5. Now, a host back here, say 192.168.1.10 might send a packet towards a global internet destination. Now, the key behind NAT is that this packet has a source port and the NAT is basically going to take that source IP address and port and it\u0026rsquo;s going to translate it into a publicly reachable source IP address and port, and the destination will remain the same. That packet will make its way to a global destination and the reply will make its way to the globally reachable IP address on the corresponding port. Now, when that packet with that particular destination IP address and port reaches the NAT, the NAT has a table that knows the mapping between that public IP address and port and the private one that it rewrote to generate the corresponding public IP address and port. So we can simply now rewrite the destination IP address of this packet to the corresponding private address and port. NATs are popular on broadband access networks, small or home offices and VPNs. There\u0026rsquo;s a clear savings in IPv4 address space, since there can be many devices in one of these private networks and yet all of the devices that are behind the NAT only use up one public IP Address. The drawback, of course, is that the end-to-end model is broken. And we talked about the end-to-end model in a previous lesson and let me just remind you how NAT breaks it. If the NAT device failed in this instance, for example, the mapping between the private source IP address and port and the public source IP address and port would be lost, thereby breaking all active connections for which the NAT is on the path. It\u0026rsquo;s also asymmetric. Under ordinary circumstances it\u0026rsquo;s rather difficult for a host on the global Internet to reach a device in a private address space in network one or network two, because by default those devices in these private networks do not have public globally reachable IP addresses. So, NAT both breaks end- to-end communication, and it also breaks by directional communication.\nIPv4 to IPv6 Another possible solution to the IP address space exhaustion problem is to simply add more bits. This is the gist of the contribution of the IPv6 protocol. Here\u0026rsquo;s a picture of the IPv4 protocol header, and all of the fields shown in red have basically been removed in IPv6, resulting in both a much simpler header and addresses that are much larger.\nBy contrast, here\u0026rsquo;s the IPv6 header. The IPv6 header provides 128 bits for both the source and destination IP addresses. Now the format of these addresses are as follows. Of the 128 bits, the top 48 bits are for the public routing topology, and we have a 16-bit site identifier. And finally, a 64-bit interface ID, which effectively has the 48-bit Ethernet address of the interface plus 16 more bits. Now, the top 48 bits can be broken down further. They include top level provider, something like a tier one ISP, 8 reserve bits, and 24 additional bits. Now, note that there are 13 bits in the top 48 that directly map to the tier one ISP, meaning that addresses are purely provider-based, thus changing ISPs would require renumbering. IPv6 has many claimed benefits. There are more addresses, the header is simpler, multihoming is supposedly easier, various aspects of security are built in, such as the IPv6 crypto extensions. Now despite all of these benefits, we have yet to see a huge deployment of IPv6 yet.\nIPv6 Routing Table Entries Now despite all of these benefits, we\u0026rsquo;ve yet to see a significant deployment of IPv6. Here you can see the number of routing table entries for IPv6 routes, as well as the growth over time from 2004. To the end of 2013. What\u0026rsquo;s remarkable is that we only see 16,000 IPv6 routes in the global routing table. This is not that many considering that there are about 500,000 IPv4 routes in the global routing table. The problem is that IPv6 is very hard to deploy incrementally. Remember our discussion of the narrow waist. Everything runs over IPv4 and IPv4 was designed to run over a variety of physical layers. This common protocol has allowed tremendous growth, but because everything depends on the narrow waist of IPv4 and because IPv4 is built on top of so many other types of infrastructure, changing it becomes extremely tricky. Incremental deployment where part of the internet is running IPv4 and other parts have been upgraded to IPv6 results in significant incompatibility. There are various incremental deployment options for IPv6.\nIPv6 Incremental Deployment One is what\u0026rsquo;s called a dual stack deployment. In a dual stack deployment a host can speak both IPv4 and IPv6. It communicates with an IPv4 host using IPv4 and communicates with an IPv6 host using IPv6. What this means is that the dual stack host has to have an IPv4 compatible address. Either the host has both an IPv4 and an IPv6 address, thus allowing it to speak to an IPv4 host, or it must rely on a translator which knows how to take a v4 compatible IPv6 address, and translate it to the v4 address. One possible way of ensuring compatibility of a v6 address with IPv4, is simply to embed the IPv4 address in 32 bits of the 128 that are allocated for the IPv6 address.\nNow, a dual stack host configuration or a v4 compatible IPv6 address solves the problem of host IP address assignment, but it doesn\u0026rsquo;t solve the problem that IPv6 deployments might exist as islands. For example, multiple independent portions of the Internet might deploy IPv6, but what if the middle of the network only speaks in routes IPv4? The solution here is to use what\u0026rsquo;s called 6 to 4 tunneling. In 6 to 4 tunneling, a v6 packet is encapsulated in a v4 packet. Now, that v4 packet is routed to a particular v4 to v6 gateway corresponding to the v6 address that lies behind that gateway. And at this point the outer layer of encapsulation can be stripped, and the v6 packet can be sent to its destination. This of course, requires the gateways at the boundaries between the v4 and v6 networks to perform encapsulation of the packet as it enters the v4 only part of the network, and de-capsulation as the packet enters the v6 island, where the destination host resides.\n"
},
{
	"uri": "/7641/",
	"title": "7641",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/router-design/",
	"title": "Router Design",
	"tags": [],
	"description": "",
	"content": " Router Design In this lesson, we will cover the design of big, fast, modern routers. Here\u0026rsquo;s a picture of two modern router chassis. On the left, we have a picture of the Cisco CRS-1, and on the right, we have a picture of the Juniper M320. The M320 has for some time been used at the border of the Georgia Tech network between Georgia Tech and the rest of the Internet.\nHere\u0026rsquo;s a picture of a couple of line cards that go into these chassis. These kind of look like network interface cards, except the ports are special. Instead of terminating Ethernet, these ports terminate high capacity fiber links. As you can see, these cards are actually a whole lot bigger than your typical network interface card as well. And, as a result, these chassis are often anywhere from three to six feet tall, and can fill up an entire rack.\nThere\u0026rsquo;s a significant need for big, fast routers. Links are getting faster. Traffic demands are also increasing, particularly with the rise of demanding applications, such as streaming video. Networks are getting bigger too, in terms of the number of hosts, routers, and users. So there\u0026rsquo;s a perennial need to design big, fast routers, particularly in Internet backbone networks. The rest of this lesson will focus on how a router works, in particular, how it goes from the process of taking a packet as input and sending it on to where it needs to go. The Internet\u0026rsquo;s routing protocols, of course, are responsible for populating the forwarding tables on a router. But once those tables are populated, the router still has the hard job of taking a packet as input and ultimately getting it to the appropriate output port, so that the traffic can proceed en route to the destination.\nBasic Router Architecture Let\u0026rsquo;s take a look at a generic router architecture. As a summary of basic router function, a router receives a packet. It then looks at the packet header, to determine the packet\u0026rsquo;s destination. It looks in the forwarding table to determine the appropriate output interface for the packet. It modifies the packet header, such as decrementing the time to live field and updating the IP header check sum appropriately. And finally, it sends the packet to the appropriate output interface. The basic I/O component of a router architecture is the line card, which is the interface by which a router sends and receives data. When a packet arrives, the line card looks at the header to determine the destination, and then it looks in the forwarding table to determine the output interface. It then updates the packet header and finally sends the packet to the appropriate output interface. Now this drawing shows just a single line card. But in fact, when the packet is sent to the output interface, it must traverse the router\u0026rsquo;s interconnection fabric, to be sent to the appropriate output port.\nSo in fact, we can zoom out from that depiction of a single line card, and what we have is a bunch of line cards that are all connected via an interconnection fabric. Each of these line cards has a lookup table, the capability to modify headers, and a queue, or buffer, for packets as they enter and leave the line card. In other lessons we talk about several important questions such as how big queues should be and how lookup works. In the rest of this lesson, I\u0026rsquo;ll discuss important decisions in router design such as the placement of lookup tables on each line card and the design of the interconnection fabric.\nEach Line Card Has Own Forwarding Table Copy One important decision in the design of the modern routers was to place a copy of the forwarding table on each line card in the router. Well, this introduces some complications in making copies of the forwarding table. Doing so prevents a central table on the router from becoming a bottleneck at high speeds. Consider an alternative where the router has only one copy of the forwarding table. In that case all of the line cards would need to be performing look ups on a central table which involves communication across the back plane as well as many more look ups against a central table. So while distributing the forwarding table across line cards prevents a central table from becoming a bottleneck, early router architectures did not place the look up table on each line card. And as a result, when packets arrived at an individual line card, they would induce a look up in a shared buffer memory which could be accessed over a shared bus. But this shared bus, of course, introduces a bottleneck, as well as contention between the different line cards that may be all performing lookups to the same shared memory. The solution, of course, was thus to remove the shared memory and instead place copies of the forwarding table on each line card. In summary, an important innovation in the design of these router was to eliminate the shared bus and place the look up table on individual line bus.\nDecision Crossbar Switching The second important decision is the design of the interconnect, or how the line cards should be connected to one another. Now one possibility is to use a shared bus. But the disadvantage of a bus for the interconnect is that it can only be used by one input-output combination in any single time slot. What we\u0026rsquo;d like to do is enable input output pairs that don\u0026rsquo;t compete to send traffic from input to output during the same time slot. For example, one should be able to send one to four, two to six and three to five, all in the same time slot. The solution to this problem is to create what\u0026rsquo;s called a crossbar switch, or sometimes is also called a switched backplane.\nCrossbar Switching In crossbar switching every input port has a connection to every output port, and during each time slot, each input is connected to zero or one outputs. The crossbar is often depicted as follows. So if one wants to send to four, we could connect the input to the output in that time slot, and now this row and this column is occupied. But we could connect two to six and three to five in the same time slot without introducing contention. So the advantage of this design is that it can exploit parallelism by allowing multiple packets to be forwarded across the interconnect in parallel. But of course we also need proper scheduling algorithms to ensure fair use of the crossbar switch. Let\u0026rsquo;s take a quick look at what this algorithm needs to achieve.\nSwitching Algorithm Maximal Matching We\u0026rsquo;d like the cross bar switching algorithm to achieve what\u0026rsquo;s called a maximal matching. Conceptually we have a router with n inputs and n outputs, but of course the inputs are also outputs. It\u0026rsquo;s just easier to think about the inputs and the outputs being separate when we talk about the switching problem. Now in each time slot we would like to achieve a one-to-one mapping between inputs and outputs, which is a matching. And our goal is that the matching should be maximal. So in a particular time slot, we might have a certain set of traffic demands, or traffic at certain input ports, that is destined for certain output ports. And our goal is, given these demands to produce a matching that is maximal and fair. Now, given demands for a particular time slot and the resulting matching, notice that certain demands were not satisfied. These packets that arrived at inputs must wait until the next time slot to be forwarded to the appropriate output port, because they couldn\u0026rsquo;t be matched in the same time slot as those shown here. Remember that there must be exactly a one-to-one matching between any inputs and outputs in a particular time slot. Most router crossbars have a notion of speedup whereby multiple matchings can be performed in the same time slot. So, for example, if the line cards are running at, say, ten gigabits per second, then running the interconnect twice as fast would allow matchings to occur twice as fast, as packets would arrive on the inputs or be forwarded from the outputs. It is thus common practice to run the interconnect at a higher speed than the input and output ports. Just speeding up the interconnect does not solve all problems. Note, for example, that in this set of demands we have packets arriving at this input port destined for this output port, but if there\u0026rsquo;s only a single queue at this input, the packets that are destined for the output port circled in orange, might actually be blocked behind a set of packets that are destined for other output ports. So even if we could induce a speed up at the interconnect, certain packets may be blocked in the queue by packets ahead of them destined for other output ports.\nHead of Line Blocking For example, if we have packets arriving in this queue destined for the orange queue, at the front of the queue, then even with the speed up, there may be packets that are sufficiently far behind in the queue that they\u0026rsquo;re waiting behind the orange packets. What we\u0026rsquo;d like to be able to do is perform matchings to allow these packets to be sent to the output ports, and not have to wait for the entire queue to be drained of packets destined for the orange output port.\nA solution is to create virtual output queues, where instead of having a single queue at the input, we have one queue per output port. This prevents packets at the front of the queue that are destined for a particular output port from blocking packets that could otherwise be matched to other output queues in earlier timeslots.\nScheduling and Fairness Let\u0026rsquo;s now talk about scheduling and fairness. And when we talk about, in a crossbar switch, the process of matching input ports to output ports, the decision about which ports should be matched in any particular time slot is a process called scheduling. There are two important goals in scheduling. One is efficiency, which is to say that if there is traffic at inputs destined for output ports, the crossbar switch should schedule inputs and outputs so that traffic isn\u0026rsquo;t sitting idle at the input ports if some traffic could be sent to the available output ports. Another consideration in scheduling is fairness, which is to say that given demands at the inputs, we want to make sure that each queue at the input is scheduled fairly for some definition of fairness. Now, defining fairness is tricky. And there are multiple possible definitions of fairness. Here, we\u0026rsquo;ll look at an important fairness definition called max min fairness.\nMax Min Fairness Now, to define max-min fairness, let\u0026rsquo;s first assume that we have some allocation of rates across flows x_i. Now, we say that this allocation is max-min fair if increasing any rate x_i implies that some other x_j that is smaller than x_i must be decreased to accommodate for the increase in x_i. So in other words, the allocation is max-min fair if we can\u0026rsquo;t make any one of these flow rates better off without making some flow rate worse off, that\u0026rsquo;s already worse than the flow rate x_i. So the upshot results in small demands getting exactly what they asked for, and the larger demands splitting the remaining capacity among themselves equally. More formally, we perform this procedure as follows. We allocate resources to users in order of increasing demand. No user receives more than what they requested. And users that still have unsatisfied demands, split the remaining resources.\nMax Min Fairness Example Let\u0026rsquo;s consider an example for max-min fair allocation. Let\u0026rsquo;s suppose that we have a link with capacity ten and four demands: 2, 2.6, 4, and 5. Now, obviously the demands exceed capacity. So we need to figure out a way of allocating rates to each of these demands that is max-min fair. First, note that 10 divided by 4 is 2.5. But this is not a good solution, because the first user only needs 2. So the first user would have an excess of .5 under this allocation. So what we want to do is take this excess of .5 and divide it among the remaining 3 users, whose demands have not yet been fulfilled. This would yield an allocation of 2, 2.67, 2.67, and 2.67. But now user two has an excess of 0.07, so we take that excess, divide it among the remaining 2, and that gives us our final max-min fair allocation. Note that this is called max-min fairness, because it maximizes the minimum share to each user whose demand is not fully serviced. In this case, the 3rd and 4th users.\nMax Min Fairness Quiz As a quick quiz, let\u0026rsquo;s try doing a max min fair allocation. Suppose that we have demands of one, two, five, and ten, and link, whose rate is 20. Please give the max-min fair allocation across these four users.\nMax Min Fairness Solution To compute the max min fair allocation, we take 20 and we divide it by 4, which yields 5. But the first user only needs one, which yields an excess of four. The second user only needs two, which yields an excess of three. So in this case the max min fair allocation is easy. We simply take this excess of seven and give it to the only user whose demand is not yet satisfied, resulting in the max min fair allocation of one, two, five, and twelve.\nHow to Achieve Max Min Fairness Now, how do we achieve max-min fairness? One approach is via round robin scheduling, where given a set of cues, the router simply services them in order. The problem here is that packets may have different sizes. So if the first queue had a huge packet, and the second queue had a little packet, and the third queue had a medium sized packet, then servicing these queues in order obviously isn\u0026rsquo;t fair. Because the first queue would effectively get more of its fair share, because its packet just happened to be bigger. An alternative is to use bit by bit scheduling, where during each time slot, each queue only has one bit serviced. This, of course, is perfectly fair, but the problem is feasibility. How do we service one bit from a queue? A third alternative is called Fair Queuing, which achieves max-min fairness by servicing packets according to the soonest finishing time. A Fair Queuing algorithm computes the virtual finishing time of all candidate packets, which are the packets at the head of all non-empty flow queues. Based on these virtual finishing times, Fair Queuing compares the finishing times of each queue and services the queue with the minimum finishing time. So the queue whose packet has the minimum virtual finishing time is serviced.\n"
},
{
	"uri": "/7642/",
	"title": "7642",
	"tags": [],
	"description": "",
	"content": " About: Reinforcement Learning is a broad introduction to research and method in RL. Reading and replicating research finding as well as implementing projects are the course deliverables.\nStaff:  Charles Isbell Michael Littman Miguel Morales, Head TA  Resources:  Course site  Reading list:  Coming soon  Preparation:  Watch David Silver\u0026rsquo;s RL tutorial: here and here Start reading Sutto \u0026amp; Barto Skim the reading lists (papers) above Become familiar with the the Open AI gym, in particular the Lunar lander environment Skim/ review game theory (Nash/Correlated EQ, repeated games, etc): http://game-theory-class.org/game-theory-I.html  "
},
{
	"uri": "/6250/dns/",
	"title": "DNS",
	"tags": [],
	"description": "",
	"content": " DNS: Domain Name System Domain Name System Part 1 We\u0026rsquo;ll now have a look at the domain name system or DNS. The purpose of the domain name system is to map human readable names such http://www.gatech.edu to IP addresses such as 130.207.160.173. A name such as this is human readable and much easier to remember and type than an IP address. But in fact, the IP address is what\u0026rsquo;s needed to send traffic to the intended destination. So, we need a lookup mechanism that takes a human readable name and maps it to an IP address. The system that does this is a Domain Name System, or the DNS. The system roughly works as follows. A client may want to look up a domain name such as http://www.gatech.edu. A networked application source code might do so by invoking a function such as get_host_by_name, which takes as an argument a domain name and returns an IP address. The client typically has what\u0026rsquo;s called a stub resolver, and that stub resolver takes that name and issues a query. The stub resolver might have cached the answer or the IP address corresponding to this name, but if not, the query is sent to what\u0026rsquo;s called a local DNS resolver.\nYour local DNS resolver, is typically configured automatically when your host is assigned an IP address using a protocol called the domain host control protocol or DHCP. In your host configuration such as this one, you can see that this local host has two local DNS resolvers. Typically, a client will try the first DNS resolver and if it doesn\u0026rsquo;t receive a response within a preconfigured timeout, it will try sending the same query to the second local DNS resolver as a backup. This query is typically issued recursively, meaning the client does not want intermediate referrals sent back to it. It only wants to hear when it\u0026rsquo;s received the final answer. The local resolver on the other hand will perform iterative queries. It might have the answer to this particular query in the cache, in which case it would simply reply with the answer. But let\u0026rsquo;s suppose for the moment, that nothing is cached. Each fully qualified domain name is presumed to end with a dot, indicating the root of the DNS hierarchy. Now the IP addresses for the root servers, or those that are authoritative for the root, may already be configured in the local DNS resolver. In this case, the resolver may be able to query for the authoritative server for .edu, say a.rootservers. net. This would be an A record query. The answer might return with what\u0026rsquo;s called an NS record, which is a referral. In this case the answer might be a referral to the edu servers. Now the local resolver issues the same query to the edu servers and receives a referral to the authoritative servers for gatech.edu. Finally the local resolver might query the authoritative name server for gatech.edu and actually receive an A record indicating the actual IP address that corresponds to that name.\nDomain Name System Part 2 Now this process of referrals, as you can see, can be rather slow. A particular DNS query might thus require round trips to multiple servers that are authoritative for different parts of the hierarchy. The blue server is authoritative for the root. The purple server is authoritative for .edu and the red server is authoritative for gatech.edu. Now supposing we wanted to save the extra time in trouble of these round trip times. This local resolver would typically have a cache that stores the NS records for each level of the hierarchy as well as the A records. And each of these answers would be stored or cached for a particular amount a time. Each one these replies has what\u0026rsquo;s called a time to live or a TTL that indicates how long each of these answers can be saved before they need to be looked up again. Caching allows for quick responses from the local DNS resolver, especially for repeated mappings. For example, since everyone is probably looking up domain names such as google.com it\u0026rsquo;s much faster to keep the answer in cache. So, given multiple clients trying to resolve the same domain name, the answers can all be resolved in a local cache. Some queries can reuse parts of this look up. For example, it\u0026rsquo;s unlikely that the authoritative name server for the root is going to change very often. So that answer might be kept, or cached, for a much longer period of time. A typical time might be hours or days, or even weeks. The mapping for a local name, such as http://www.gatech.edu, on the other hand, might change more frequently and thus these local TTL\u0026rsquo;s might need to be smaller. Now the most common type of DNS record is what\u0026rsquo;s called an A record, which maps an IP address to a domain name. But there are other important record types as well.\nRecord Types A records map names to IP addresses as we have seen. We have also seen what\u0026rsquo;s called an NS or a Nameserver record which maps a domain name to the authoritative nameserver for that domain. So we saw a bunch of NS records in the form of referrals, whereby, if we ask the route for a mapping of gatech.edu to an IP address, it doesn\u0026rsquo;t specifically know the answer, but it can issue a nameserver reply or an NS record referring the resolver to a different nameserver that could be responsible for that part of the domain name space. This allows the domain name system to be implemented as a hierarchy. Another important DNS record type is an MX record, which shows the mail server for a particular domain. Occasionally, one name actually is just an alias for another name. For example, http://www.gatech.edu actually has a slightly different real name. The CNAME is basically a pointer from an alias to another domain name that needs to be looked up. The PTR is another record that we\u0026rsquo;ll look at, and this maps IP addresses to domain names. For example if you wanted to know the name for a particular IP address, you need to issue a PTR query. This is sometimes called a reverse lookup. Finally, a AAAA record maps a domain name to an IPV6 address. Let\u0026rsquo;s take a look at a couple of different examples of domain name lookups using a command line utility called dig.\nDNS Quiz As a quick quiz, which DNS record is used for referral? Is it the MX record? A record? The Quad A record? The NS record? or the PTR?\nDNS Solution The NS record indicates the authoritative name server for a particular portion of the domain name space, and an NS record reply is often referred to as a referral. MX records indicate mail servers. A records are IP addresses for the domain name. AAAA are for IPv6 addresses, and a PTR is a name corresponding to an IP address being queried.\nExample DNS Lookup Example (using dig) Part 1 Here\u0026rsquo;s an example of a lookup for an A record for gatech.edu. You can try this at your own command line by typing, for example, dig http://www.gatech.edu. Now there are some interesting things to note in this trace. Here is our query and you can see that this is an A record query.\nHere\u0026rsquo;s our answer. You can see that the answer actually has a CNAME in it, which basically says, well you asked for gatech.edu but in fact what you really want to ask for is tlweb.gtm.gatech.edu. So then we issue an A record query for that name, and we ultimately get the IP address. These numbers here indicate the time to live or the amount of time in seconds that the entry can be stored in the cache.\nHere\u0026rsquo;s another example of a DNS lookup from nytimes.com. The interesting thing to note here is that in response to the A record query, we see two IP addresses. This is typically performed when a service wants to perform load balancing So, the client could use either one of these. It might prefer the first one, but if we issued the same query again, we might actually get these IP addresses in a different order. Now, again, here you can see the TTL value which indicates how long these A records can be stored in cache. In a subsequent example, we\u0026rsquo;ll look at other query types that have much longer TTL values.\nHere\u0026rsquo;s an example of a query for the NS record for gatech.edu. You can see this output by typing dig ns gatech.edu. You can see here in the question section, now instead of an A record query we have an NS record query. And our answer is a bunch of NS records that are dns1, 2, and 3.gatech.edu, any of which could answer authoritatively for sub-domains of gatech.edu. You can see that in addition to the answer, which return the name servers, we also need the IP addresses of those name servers, which is returned in the additional section of the answer. You can see here that we received not only A records for each domain name but also quad A or IPv6 addresses corresponding to each authoritative name server.\nHere\u0026rsquo;s an example of a query for an MX record or the mail server corresponding to gatech.edu. Now, here again you can see the question is the MX record and you can see the answer which returns two mail servers as well as the additional section, which returns an A record indicating the IP address corresponding to the mail server that was returned in the MX record. In addition to the TTL, we also have some metrics that indicate priorities that would allow a system administrator to configure a primary and a backup mail server.\nIn this case, the mail servers, just happen to have the same priority level.\nExamples (using dig) Part 2 Let\u0026rsquo;s put everything together now by looking at a trace of an entire lookup. Now in the examples before, we didn\u0026rsquo;t get to see the full lookup hierarchy because we issued a recursive query. But let\u0026rsquo;s suppose we wanted to see every step of the DNS lookup process. You can do this by using the trace option in dig. Here you can see exactly what we saw before, which is the local resolver. In this case, issuing a query to a local resolver and receiving a referral to an authoritative server for dot which could be any of the following. That query, elicits an answer for the .edu servers which subsequently issues a referral to the servers that are authoritative for gatech, which ultimately reply with the appropriate a records as well as the authoritative nameservers for gatech.edu.\nA final interesting example explores how to map an IP address. back to a name. In this case, we\u0026rsquo;re ultimately looking for PTR record, which is the name corresponding to this IP address. But first, what happens is we receive a special referral. When we ask the root servers about this particular IP address, instead of being referred to a particular .com or .edu domain, we\u0026rsquo;re referred to a special top level domain called inaddr.arpa, which maintains referrals to authoritative servers that are maintained by the respective internet routing registries, such as ARIN, RIPE, APNIC and so forth. So here we see a referral to inaddr.arpa. Subsequently, we see a referral to 130.in- addr.arpa corresponding to the first octet of the IP address. Next when we ask ARIN about 130.in-addr.arpa we receive another referral, which is to is to 207.130.in-addr.arpa. And because 130.207 is allocated to gatech.edu, ARIN knows that the appropriate referral for this part of the address space is to DNS 1, 2, or 3.gatech.edu. Next we issue a query for the next part of the octet. 7.207.130.in-addr.arpa corresponding to the first 3 octets. And now we actually get the PTR because DNS3.gatech.edu knows the reverse mapping between 130.207.7.36 and the name for that IP address. So you can see that the PTR records, or those that map IP addresses to names,\nare resolved through a special hierarchy through in-addr.arpa at the root followed by a walk through the regional registries and ultimately, to the domains, such as gatech, that are responsible for particular regions of the IP address space.\nLookup IP Address Quiz As a quick quiz, suppose we wanted to look up the IP address 130.207.97.11. What is the corresponding in-addr.arpa domain name? Is it 130.207.97.11? Is it 130.207.97.11.in-addr.arpa? Is it in-addr.arpa.130.207.97.11? Or is it 11.97.207.130.in-addr.arpa?\nLookup IP Address Solution The corresponding domain name for the PTR lookup for this IP address is the record corresponding to 11.97.207.130.in-addr.arpa. Notice that the reversal of the octets in this name corresponds to a strict traversal of the hierarchy from the highest levels of the hierarchy at inaddr.arpa to the lower levels, as the IP address moves from higher to lower parts of the hierarchy.\n"
},
{
	"uri": "/6250/congestion-control-and-streaming/",
	"title": "Congestion Control and Streaming",
	"tags": [],
	"description": "",
	"content": " Lecture 6: Congestion Control, \u0026amp; Streaming In this section of the course we\u0026rsquo;ll learn about resource control and content distribution. Resource control deals with handling bandwidth constraints on links. And in this first section, we\u0026rsquo;ll focus on controlling congestion along links. To learn more about TCPA and congestion control, your Mininet project will explore what happens when congestion control goes wrong.\nCongestion Control Okay, we\u0026rsquo;re starting course two on congestion control and streaming. And we\u0026rsquo;ll first talk about congestion control. In particular, what is congestion control and why do we need it? Simply put, the goal of congestion control is to fill the Internet\u0026rsquo;s pipes without overflowing them. So to think about this in terms of an analogy, suppose you have a sink, and you\u0026rsquo;re filling that sink with water. Well, how should you control the faucet? Too fast and the sink overflows. Too slow and you are not efficiently filling up your sink. So what you would like to do is to fill the bucket as quickly as possible without overflowing. The solution here is to watch the sink. And as the sink begins to overflow, we want to slow down how fast we\u0026rsquo;re filling it. That\u0026rsquo;s effectively how congest control works.\nCongestion Let\u0026rsquo;s suppose that in a network we have three hosts shown as squares at the edge of the network that are connected by links with capacities as shown. The two senders on the left can send at rates of 10 megabits per second and 100 megabits per second, respectively. But the link to the host on the right is only 1.5 megabits per second. So these different hosts on the left are actually competing for the same resources inside the network. So the sources are unaware of each other and also of the current state of whatever resource they are trying to share, in this case, how much other traffic is on the network. This shows up as lost packets or long delays and can result in throughput that\u0026rsquo;s less than the bottleneck link, something that\u0026rsquo;s also known as congestion collapse.\nCongestion Collapse In congestion collapse, an increase in traffic load suddenly results in a decrease of useful work done. As we can see here, up to a point, as we increase network load, there is an increase in useful work done. At some point, the network reaches saturation, at which point increasing the load no longer results in useful work getting done. But at some point, actually increasing the traffic load can cause the amount of work done or the amount of traffic forwarded to actually decrease. There are many possible causes. One possible cause is the spurious re-transmissions of packets that are still in flight. So when senders don\u0026rsquo;t receive acknowledgements for packets in a timely fashion, they can spuriously re-transmit, thus resulting in many copies of the same packets being outstanding in the network at any one time. Another cause of congestion collapse is simply undelivered packets, where packets consume resources and are dropped elsewhere in the network. The solution to spurious re-transmissions is to have better timers and to use TCP congestion control, which we\u0026rsquo;ll talk about next. The solution to undelivered packets is to apply congestion control to all traffic. Congestion control is the topic of the rest of this lesson.\nCongestion Collapse Quiz Let\u0026rsquo;s take a quick quiz. What are some possible causes of congestion collapse? Faulty router software? Spurious retransmissions of packets in flight? Packets that are travelling distances that are too far between routers? Or, undelivered packets? Please check all options that apply.\nCongestion Collapse Solution Congestion collapse is caused by spurious retransmissions of packets in flight and undelivered packets that consume resources in the network but achieve no useful work.\nGoals of Congestion Control Congestion control has two main goals. The first is to use network resources efficiently. Going back to our sink analogy, we\u0026rsquo;d like to fill the sink as quickly as possible. Fairness, on the other hand, ensures that all the senders essentially get their fair share of resources. A final goal, of course, is to avoid congestion collapse. Congestion collapse isn\u0026rsquo;t just a theory, it\u0026rsquo;s actually been frequently observed in many different networks.\nTwo Approaches to Congestion Control There are two basic approaches to congestion control: end-to-end congestion control and network assisted congested control. In end-to-end congestion control the network provides no explicit feedback to the senders about when they should slow down their rates. Instead, congestion is inferred typically by packet loss, but potentially, also by increased delay. This is the approach taken by TCP congestion control. In network assisted congestion control, routers provide explicit feedback about the rates that end systems should be sending in. So they might set a single bit indicating congestion, as is the case in TCP\u0026rsquo;s ECN, or explicit congestion notification extensions, or they might even tell the sender an explicit rate that they should be sending at. We\u0026rsquo;re going to spend the rest of the lesson talking about TCP congestion control.\nTCP Congestion Control In TCP congestion control, the senders continue to increase their rate until they see packet drops in the network. Packet drops occur because the senders are sending at a rate that is faster than the rate at which a particular router in the network might be able to drain its buffer. So you might imagine, for example, that if all three of these senders are sending at a rate that is equal to the rate at which the router is able to send traffic downstream, then eventually this buffer will fill up. TCP interprets packet loss as congestion. And when senders see packet loss, they slow down as a result of seeing the packet loss. This is an assumption. Packet drops are not a sign of congestion in all networks. For example, in wireless networks, there may be packet loss due to corrupted packets as a result of interference. But in many cases, packet drops do result because some router in the network has a buffer that has filled up and can no longer hold anymore packets and hence it drops the packets as they arrive. So senders increase rates until packets are dropped, periodically probing the network to check whether more bandwidth has become available; then they see packet loss, interpret that as congestion, and slow down. So, congestion control has two parts. One is an increase algorithm, and the other is a decrease algorithm. In the increase algorithm, the sender must test the network to determine whether the network can sustain a higher sending rate. In the decrease algorithm, the senders react to congestion to achieve optimal loss rates, delays in sending rates. Let\u0026rsquo;s now talk about how senders can achieve these increase and decrease algorithms.\nTwo Approaches to Adjusting Rate One approach is a window based algorithm. In this approach, a sender can only have a certain number of packets outstanding, or quote, in flight. And the sender uses acknowledgements from the receiver to clock the retransmission of new data. So let\u0026rsquo;s suppose that the sender\u0026rsquo;s window was four packets. At this point, there are four packets outstanding in the network. And the sender cannot send additional packets until it has received an acknowledgement from the receiver. When it receives an acknowledgment, or an ACK from the receiver, the sender can then send another packet. So at this point there are still four outstanding or four unacknowledged packets in flight. In this case if a sender wants to increase the rate at which it\u0026rsquo;s sending, it simply needs to increase the window size. So, for example, if the sender wants to send at a faster rate, it can increase the window size from four, to five. A sender might increase its rate anytime it sees an acknowledgement from the receiver. In TCP, every time a sender receives an acknowledgement, it increases the window size.\nUpon a successful receipt, we want the sender to increase its window by one packet per round trip. So, for example, in this case if the sender\u0026rsquo;s window was initially four packets, then at the end of a single round trip\u0026rsquo;s worth of sending, we want the next set of transmissions to allow five packets to be outstanding. This is called Additive Increase. If a packet is not acknowledged, the window size is reduced by half. This is called Multiplicative Decrease. So TCP\u0026rsquo;s congestion control is called additive increase multiplicative decrease, or AIMD.\nThe other approach to adjusting rates is an explicit rate-based congest control algorithm. In this case the sender monitors the loss rate and uses a timer to modulate the transmission rate. Window based congestion control, or AIMD, is the common way of performing congestion control in today\u0026rsquo;s computer networks. In the next lesson we will talk about the two goals of TCP congestion control further (efficiency and fairness) and explore how TCP achieves those goals.\nWindow Based Congestion Control Quiz Let\u0026rsquo;s have a quick quiz on window based congestion control. Suppose the round trip time between the sender and receiver is 100 ms, each packet is 1kb, and the window size is 10 packets. What is the rate at which the sender is sending? Please put your answer here in terms of kilobits per second, keeping in mind that 1 byte is 8 bits.\nWindow Based Congestion Control Solution The sending rate of the sender is approximately 800 kbps. With a window size of ten packets and a round trip time of 100 milliseconds, the sender can send 100 packets per second. With each packet being one kilobyte, or 8000 bits, that gives us 800,000 bits per second or about 800 kilobits per second.\nFairness and Efficiency in Congestion Control The two goals of congestion control are fairness (meaning every sender gets their fair share of the network resources) and efficiency (meaning that the network resources are used well). In other words we shouldn\u0026rsquo;t have a case where there are spare capacity or resources in the network, and senders have data to send, but are not able to send it. So, we\u0026rsquo;d like the network to be used efficiently, but we\u0026rsquo;d also like it to be shared among the senders.\nWe can represent fairness and efficiency in terms of a phase plot, where each axis represents a particular user, or particular senders\u0026rsquo; allocation. In this case we just have two users, one and two, and we represent their allocations with X1 and X2. If the capacity of the network is C, then we can represent the optimal operating line as X1 + X2 being some constant, C. Anything to the left of this diagonal line represents under utilization of the network, and anything to the right of the line represents overload. We can also represent another line, X1 = X2 as some notion of fair allocation. So the optimal point is where the network is neither under or over utilized, and when the allocation is fair. So being on this diagonal line represents efficiency, and being on the green diagonal line represents fairness. We can use the phase plot to understand why senders who use additive increase multiplicative decrease, converge to fairness. The senders also converge to the efficient operating point. Let\u0026rsquo;s suppose that we start at the operating point shown in blue. At this point both senders will additively increase their sending rates. Additive increase results in moving along a line that is parallel to x1 and x2, since both senders increase their rate by the same amount. Additive increase will continue until the network becomes overloaded. At this point the senders will see a loss and perform multiplicative decrease. In multiplicative decrease each sender decreases its rate by some constant factor of its current sending rate. For example, suppose each one of these senders decreases its sending rate by half. The resulting operating point is shown by this second blue dot. Note that that new operating point, as a result of multiplicative decrease, is on a line between the point on the efficiency line that the centers hit, and the origin. At this point the sender\u0026rsquo;s will again increase their sending rate along a line that\u0026rsquo;s parallel to X1 equals X2 until they hit over load again, at which point they will again retreat towards the origin. You can see that eventually the senders will reach this optimal operating point through the path that\u0026rsquo;s delineated by the blue line. To think about this a bit more you can see that every time additive increase is applied, that increases efficiency. Every time multiplicative decrease is applied that improves fairness because every time we apply multiplicative decrease, we get closer to this X1 equals X2 line.\nAIMD The result is the additive increase multiplicative decrease congestion control algorithm. The algorithm is distributed, meaning that all the senders can act independently, and we\u0026rsquo;ve just shown using the phase plots that it\u0026rsquo;s both fair and efficient. To visualize this sending rate over time, the sender\u0026rsquo;s sending rate looks roughly as shown. We call this the TCP sawtooth behavior or simply the TCP sawtooth. TCP periodically probes for available bandwidth by increasing its rate using additive increase. When the sender reaches a saturation point by filling up a buffer in a router somewhere along the path, it will see a packet loss, at which point it will decrease its sending rate by half. You can thus see that a TCP sender sends at a sending rate shown by the dotted green line that is halfway between the maximum window size at which the sender sends, and half that rate which it backs off to when it sees a loss. You can see that between the lowest sending rate and the highest is w_m over 2 plus 1 round trips. Now, given that rate we can compute the number of packets between periods of packet loss and compute the loss rate from this. The number of packets sent for every packet lost is the area of this triangle. So the lost rate is on the order of the square of the maximum window divided by some constant. Now, the throughput is the average rate, 3 4ths w_max divided by the RTT. Now if we want to relate the throughput to the loss rate, where we call the loss rate p and the throughput lambda, we simply need to solve for w_m.\nAnd I\u0026rsquo;m just going to get rid of the constant. So a loss occurs once for this number of packets, so the loss rate is simply 1 over that quantity. And then when we solve for w_m and plug in for throughput, we see that the throughput is inversely proportional to both the round trip time and the square root of the loss rate.\nAdditive Increase Quiz So as a quick quiz, and returning to our phase plot, does additive increase increase or decrease fairness? And does it increase, or decrease, or reduce, efficiency? Please check all that apply\nAdditive Increase Solution Additive increase increases efficiency because it gets us closer to that efficiency line parallel to the X1 equals X2 line. It technically also decreases fairness because the centers are both increasing their rates, so we\u0026rsquo;re relatively further away from this X1 equals X2 line. In contrast, remember that multiplicative decrease reduces efficiency by moving us further away from this green dotted line, but it increases fairness by moving us closer to the blue dotted line.\nData Centers and TCP Incast We\u0026rsquo;ll now talk about TCP congestion control in the context of modern datacenters. And we\u0026rsquo;ll talk about a particular TCP throughput collapse problem called the TCP incast problem. A typical data center consists of a set of server racks, each holding a large number of servers, the switches that connect those racks of servers, and the connecting links that connect those switches to other parts of the topology. So the network architecture is typically made up of some sort of tree and switching elements that progressively are more specialized and expensive as we move up the network hierarchy. Some of the characteristics of a data center network include a high fan in. There is a very high amount of fan in between the leaves of the tree and the top of the root workloads are high bandwidth and low latency, and many clients issue requests in parallel, each with a relatively small amount of data per request. The other constraint that we face is that the buffers in these switches can be quite small. So when we combine the requirements of high bandwidth and low latency for the applications, the presence of many parallel requests coming from these servers. and the fact that the switches have relatively small buffers, we can see that potentially there will be a problem. The throughput collapse that results from this phenomenon is called the TCP Incast problem. Incast is a drastic reduction in application throughput that results when servers using TCP all simultaneously request data, leading to a gross underutilization of network capacity in many-to-one communication networks like a datacenter. The filling up of the buffers here at the switches result in bursty retransmissions that overfill the switch buffers. And these bursting retransmissions are cause by TCP timeouts. The TCP timeouts can last hundreds of milliseconds. But the roundtrip time in a data center network is typically less than a millisecond. Often just hundreds of microseconds. Because the roundtrip times are so much less than TCP timeouts, the centers will have to wait for the TCP timeout before they retransmit an application. Throughput can be reduced by as much as 90% as a result of link idle time.\nBarrier Synchronization and Idle Time A common request pattern in data centers today is something called barrier synchronization whereby a client or an application might have many parallel threads, and no forward progress can be made until all the responses for those threads are satisfied. For example, a client might send a synchronized read with four parallel requests. But, suppose that the fourth is dropped. At this point we have a request sent at time zero, then we see a response less than a millisecond later, and at this point, threads one to three complete but TCP may time out on the fourth. In this case, the link is idle for a very long time while that fourth connection is timed out. The addition of more servers in the network induces an overflow of the switch buffer, causing severe packet loss, and inducing throughput collapse. One solution to this problem is to use fine grained TCP retransmission timers, on the order of microseconds, rather than on the order of milliseconds. Reducing the retransmission timeout for TCP thus improves system throughput. Another way to reduce the network load is to have the client acknowledge every other packet rather than every packet, thus reducing the overall network load. The basic idea here, and the premise, is that the timers need to operate on a granularity that\u0026rsquo;s close to the round-trip time of the network. In the case of a data center that\u0026rsquo;s hundreds of microseconds or less.\nTCP Incast Quiz As a quick review what are some solutions to the TCP incast problem? Having senders send smaller packets? Using finer granularity TCP timeout timers. Having the clients send fewer ackowledgements or having fewer TCP senders?\nTCP Incast Solution Using finer granularity timers and having the clients acknowledge only every other packet, as oppose to every packet, are possible solutions to the TCP Incast problem.\nMultimedia and Streaming In this lesson, we\u0026rsquo;ll talk about multimedia and streaming. We\u0026rsquo;ll talk about digital audio and video data, multimedia applications (in particular, streaming audio and video for playback), multimedia transfers over a best-effort network (in particular, how to tolerate packet loss delay and jitter), and quality of service. So we use multimedia and streaming video very frequently on today\u0026rsquo;s internet. YouTube streaming videos are an example of multimedia streaming as are applications for video or voice chat such as Skype or Google Hangout. In this lecture we\u0026rsquo;ll talk about the challenges for streaming these types of applications over best effort networks as well has how to solve those challenges. We\u0026rsquo;ll also talk about the basics of digital audio and video data. First of all, let\u0026rsquo;s talk about the challenges for media streaming.\nChallenges One challenge is that, there\u0026rsquo;s a large volume of data. Each sample is a sound or an image and there are many samples per second. Sometimes because of the way data is compressed, the volume of data that\u0026rsquo;s being sent may vary over time. In particular, the data may not be set at a constant rate. But in streaming, we want smooth playout, so the variable volume of data can pose challenges. Users typically have a very low tolerance for delay variation. Once playout of a video starts for example, you want that video to keep playing. It\u0026rsquo;s very annoying if once you\u0026rsquo;ve started playing, that the video stops. The users might have a low tolerance for delay period, so in cases like games or Voice over IP, delay is typically just unacceptable, although users can tolerate some loss. Before we get into how the network solves these challenges. Let\u0026rsquo;s talk a little bit about digitizing audio and video.\nDigitizing Audio and Video Suppose we have an analog audio signal that we\u0026rsquo;d like to digitize, or send as a stream of bits. What we can do is sample the audio signal at fixed intervals, and then represent the amplitude of each sample with a fixed number of bits. For example, if our dynamic range was from 0 to 15, we could quantize the amplitude of this signal such that each sample could be represented with four bits.\nDigitizing Audio and Video Quiz 1 Let\u0026rsquo;s take a couple of examples. So with speech you might take 8000 samples per second, and you might have 8 bits per sample. So what is the sampling rate in this case? Please give your answer in kilobits per second.\nDigitizing Audio and Video Solution 1 At 8,000 samples per second, and eight bits for every sample, the rate of digitized speech would be 64 kbps, which is a common bit rate for audio.\nDigitizing Audio and Video Quiz 2 Suppose we have a MP3 with 10,000 samples per second and 16 bits per sample. What\u0026rsquo;s the resulting rate in this case in kbps?\nDigitizing Audio and Video Solution 2 The resulting rate in this case is, a 160 kbps.\nVideo Compression Video compression works in slightly different ways. Each video is a sequence of images and each image can be compressed with spatial redundancy, exploiting aspects that humans tend not to notice. Also there is temporal redundancy. Between any two video images, or frames, there might be very little difference. So if this person was walking towards a tree, you might see a version of the image that\u0026rsquo;s almost the same, except with the person shifted slightly to the right. Video compression uses a combination of static image compression on what are called reference frames, or anchor frames (sometimes called I frames), and derived frames, sometimes called P frames. The P frame can be represented as the I frame, compressed.\nIf we take the I frame and divide it into blocks, we can then see that the P frame is almost the same except for a few blocks here that can be represented in terms of the original I frame blocks, plus a few motion vectors.\nA common video compression format that\u0026rsquo;s used on the internet is called MPEG.\nStreaming Video In a streaming video system where the server streams stored audio and video, the server stores the audio or video files, the client requests the files, and plays them as they download. It\u0026rsquo;s important to play the data at the right time. The server can divide the data into segments and then label each segment with a time stamp indicating the time at which that particular segment should be played, so the client knows when to play that data. The data must arrive at the client quickly enough, otherwise the client can\u0026rsquo;t keep playing. The solution is to have a client use what\u0026rsquo;s called a playout buffer, where the client stores data as it arrives from the server, and plays the data for the user in a continuous fashion. Thus, data might arrive more slowly or more quickly from the server, but as long as the client is playing data out of the buffer at a continuous rate, the user sees a smooth playout. A client may typically wait a few seconds before it starts playing the stream to allow data to be built up in this buffer to account for cases when the server might have times where it is not sending at a rate that\u0026rsquo;s sufficient to satisfy the client\u0026rsquo;s playout rate.\nPlayout Delay Looking at this graphically, we might see packets generated at a particular rate and the packets might be received at slightly different times, depending on network delay. These types of delays are the types that we want to avoid when we playout. So if we wait to receive several packets and fill the buffer before we start playing, say to here, then we can have a playout schedule that is smooth regardless of the erratic arrival times that may result from network delays. So this playout delay or buffering allows the client to achieve a smooth playout. Some delay at the beginning of the playout is acceptable. Startup delays of a few seconds are things that users can typically tolerate, but clients cannot tolerate high variation in packet arrivals if the buffer starves or if there aren\u0026rsquo;t enough packets in the buffer. Similarly, small amount of loss or missing data does not disrupt the playback, but retransmitting a lost packet might actually take too long and result in delays or starvation of the playout buffer.\nStreaming Quiz So as a quick review, which of these pathologies can streaming audio and video tolerate? Packet loss, delay, variation in delay, or jitter?\nStreaming Solution Some delay at the beginning of a packet stream is acceptable. And similarly, some small amount of missing data is okay. We can tolerate small amounts of missing data that result in slightly reduced quality of the audio or video stream. On the other hand, a receiver is not very good at tolerating variability in packet delay in the packet stream, particularly if the client buffer is starved.\nTCP is Not a Good Fit It turns out that TCP is not a good fit for congestion control for streaming video, or streaming audio. TCP retransmits lost packets, but retransmissions may not always be useful. TCP also slows down its sending rate after packet loss, which may cause starvation of the client. There\u0026rsquo;s also a fair amount of overhead in the protocol. A TCP header of 20 bytes for every packet is large for audio samples and sending acknowledgements for every other packet may be more feedback than is needed. Instead, one might consider using UDP. UDP does not retransmit lost packets and it does not automatically adapt the sending rate. It also has a smaller header. Because UDP does not automatically retransmit packets or adjust the sending rate, many things are left to higher layers, potentially the application, such as when to transmit the data, how to encapsulate it, whether to retransmit, and whether to adapt the sending rate, or to adapt the quality of the video or audio encoding. So higher layers must solve these problems. In particular, the sending rate still needs to be friendly or fair to other TCP senders, which may be sharing the link. There are a variety of video streaming and audio streaming transport protocols that are built on top of UDP that allows senders to figure out when and how to retransmit lost packets and how to adjust sending rates.\n(More) Streaming We\u0026rsquo;re going to talk a little bit more about streaming and in particular, specific applications and how they work. We\u0026rsquo;ll talk about a streaming video application, YouTube, and a Voice over IP (or streaming audio) application, Skype. With YouTube, all uploaded videos are converted to Flash or html5 and nearly every browser has a Flash plug-in. Thus, every browser can essentially play these videos. HTTP and TCP are implemented in every browser and the streams easily get through most firewalls. So, in this case, even though we\u0026rsquo;ve talked about why TCP is suboptimal for streaming, the designers of YouTube decided to keep things simple, potentially at the expense of video quality. When a client makes an HTTP request to youtube.com, it is redirected to a content distribution network server located in a content distribution network, such as Limelight or perhaps even YouTube\u0026rsquo;s own content distribution network. We will talk about content distribution networks later on in this course. When the client sends an HTTP get message to this CDN server, the server responds with a video stream. Similarly with Skype, or Voice Over IP, your analog signal is digitized through an A to D conversion and this resulting digitized bit stream is sent over the internet. In the case of Skype, this A to D conversion happens by way of the application. And in the case of Voice over IP, this conversion might be performed with some kind of phone adapter that you actually plug your phone into. An example of that might be Vonage. In VoIP with an analogue phone, the adapter converts between the analogue and digital signal. It sends and receives data packets and then communicates with the phone in a standard way. Skype, on the other hand, is based on what\u0026rsquo;s known as peer-to-peer technology where individual users using Skype actually route voice traffic through one another. We will talk more about peer-to-peer content distribution later in this course.\nSkype So Skype has a central log-in server but then uses pier to pier to exchange the actual voice streams, and compresses the audio to achieve a fairly low bit rate. At around 67 bytes per packet and 140 packets per second, we see Skype uses about 40 kilobites per second in each direction. Data packets are also encrypted in both directions. Good audio compression and avoiding hops through far away hosts can improve the quality of the audio, as can forward error correction. Ultimately, though, the network is a major factor. Long propagation delays, high congestion, or disruptions as a result of routing changes can all degrade the quality of a voice over IP call. To ensure that some streams achieve acceptable performance levels, we sometimes use what\u0026rsquo;s called Quality of Service. One way of doing Quality of Service is through explicit reservations. But, another way is to simply mark certain packet streams as higher priorities than others. Let\u0026rsquo;s first take a look at marking and policing of traffic sending rates.\nMarking (and Policing) So we know from before that applications compete for bandwidth. Consider a Voice over IP application and a file transfer application that want to share a common network link. In this case, we\u0026rsquo;d like the audio packets to receive priority over the file transfer packets since the user\u0026rsquo;s experience can be significantly degraded by lost or delayed audio packets. In this case, we want to mark the audio packets as they arrive at the router so that they receive a higher priority than the file transfer packets. You might imagine implementing this with priority queues where the VOIP packets were put in one queue and the file transfer packets are put in a separate queue that is served less often than a high priority queue. An alternative is to allocate fixed bandwidth per application, but the problem with this alternative is that it can result in inefficiency if one of the flows doesn\u0026rsquo;t fully utilize its fixed allocation. So the idea, in addition to marking and policing, is to apply scheduling. One way of applying scheduling is to use what\u0026rsquo;s called weighted fair queuing, whereby the queue with the green packets is served more often than the queue with the red packets. Another alternative is to use admission control, whereby an application declares its needs in advance, and the network may block the application\u0026rsquo;s traffic if the application can\u0026rsquo;t satisfy the needs. A busy signal on a telephone network is an example of admission control. But can you imagine how annoying it would be if you attempted to load a web page and were blocked. This blocking, or the user experience that results from it, is a negative consequence of admission control and is one of the reasons it\u0026rsquo;s not commonly applied for internet applications.\nQoS Quiz So as a quick quiz what are some commonly used Quality of Service techniques for streaming audio and video? Marking packets and putting them into queues with different priorities, scheduling the service of these queues at different rates depending on the priority of the application, blocking flows with admission control if the network can\u0026rsquo;t satisfy their rates, or performing fixed bandwidth allocations? Please check all that apply.\nQos Solution A common way of applying QoS for streaming applications is to mark the packets at a higher priority, put those packets into a higher priority queue, and then schedule that queue so that it\u0026rsquo;s serviced more regularly, or more frequently, than traffic or applications that are lower priority.\nParking Lot Problem Recall that congestion collapse occurs when packets consume valuable network resources only to get dropped later at a downstream link. And although the network is forwarding packets, none of them actually reach the destination and no useful communication occurs, resulting in what we called congestion collapse. We talked about congestion collapse in an earlier lesson in this course.\nRecall that the goal of TCP is to prevent congestion collapse. In this assignment, you will become familiar with the Mininet environment, creating custom network topologies, running programs in virtual hosts that generate TCP traffic, and learn about TCP congestion control, an the TCP sawtooth. You\u0026rsquo;ll also see how bandwidth is shared across multiple flows.\nWe are going to explore these concepts in a particular topology that we\u0026rsquo;ll just call a parking lot. So in the assignment, you\u0026rsquo;ll create the topology below with the following parameters. N will be the number of receivers connected via this switch like topology, the data rate of each link will be 100 megabits per second, and the delay of each link will be one millisecond. Now you\u0026rsquo;re going to use iperf to generate simultaneous TCP flows from each of the senders to the lone receiver. Then you\u0026rsquo;re going to plot the time series of throughput versus time for each of these senders for each of your experiments as you vary the value of N. Your plot should run for 60 seconds. We have given an initial file called parkinglot.py. Your goal is to fill in the gaps. You\u0026rsquo;ll need to both fill in the part that sets up the topology using a parameter of N, and then you\u0026rsquo;ll need to fill in the part that actually generates the TCP flows between the senders and the receiver using iperf, and monitor the throughput of each of these flows.\n"
},
{
	"uri": "/6476/",
	"title": "6476",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/rate-limiting-and-traffic-shaping/",
	"title": "Rate Limiting and Traffic Shaping",
	"tags": [],
	"description": "",
	"content": " Rate Limiting \u0026amp; Traffic Shaping TCP congestion control responds to interpreted network events like packet loss, but does not provide a high-level control mechanism for congestion. In this section, we\u0026rsquo;ll look at traffic shaping and network measurement, which are important tools for operating the network.\nTraffic Classification and Shaping In this lesson we will talk about traffic classification and shaping. We\u0026rsquo;ll first talk about different ways to classify traffic, then we\u0026rsquo;ll talk about different traffic shaping approaches, then we\u0026rsquo;ll talk about a traffic shaper called a leaky bucket traffic shaper, then we\u0026rsquo;ll talk about an (r, T) traffic shaper. Then we\u0026rsquo;ll talk about a token bucket traffic shaper, and finally, we\u0026rsquo;ll talk about how to combine a token bucket shaper with a leaky bucket shaper to build what\u0026rsquo;s called a composite shaper. The motivation here is to control network resources and ensure that no traffic flow exceeds a particular pre-specified rate.\nSource Classification Traffic sources can be classified in a number of ways. Data traffic might be bursty, it might be weakly periodic, or it might also be regular. Audio traffic is typically continuous and strongly periodic. Video traffic is continuous, but it\u0026rsquo;s often bursty due to the nature of how video is often compressed, as we saw in a previous lecture, and it may also be periodic. Typically, we think of taking these sources and classifying them into two kinds of traffic. One is a constant bit rate source or a CBR source. In a constant bit rate source of traffic, traffic arrives at regular intervals, and packets are typically the same size as they arrive, resulting in a constant bit rate of arrival. Audio is an example of a constant bit rate source. Many other sources of traffic are variable bit rate or VBR. Video and data are often variable bit rate. Typically when we shape CBR traffic, we shape it according to a peak rate. Variable bit rate traffic is shaped according to both an average rate, and a peak rate, where the average rate might actually be a small fraction of the peak rate. You can see that at certain times the peak rate might well exceed the average rate. Let\u0026rsquo;s now talk about how to perform traffic shaping in a number of different ways.\nVBR Quiz So what are some examples of variable bit rate traffic? Audio streams, Video streams, or Data Transfers? Please check all that apply.\nVBR Solution Video streams and data transfers can be variable bit rate or bursty. Audio tends to be constant bit rate with each packet being of a small, fixed packet size.\nTraffic Shaping Leaky Bucket Traffic Shaping One way of shaping traffic is with what\u0026rsquo;s called a Leaky Bucket Traffic Shaper, where each flow has its own bucket. In a Leaky Bucket Traffic Shaper, data arrives in a bucket of size beta and drains from the bucket at rate rho. The parameter rho controls the average rate. Data can arrive faster or slower into the bucket, but it cannot drain at a rate faster than rho. Therefore, the maximum average rate that traffic can be sent is this smooth rate, rho. The size of the bucket controls the maximum burst size that a sender can send for a particular flow. So even though the average rate cannot exceed rho, at times, the sender might be able to send at a faster rate, as long as the total size of the burst does not exceed the size of the bucket. Or does not overflow the bucket. The leaky bucket allows flows to periodically burst, and the regulator at the bottom of the leaky bucket ensures that the average rate does not exceed the drain rate of the bucket. For example, for an audio application one might consider setting the size of the bucket to be 16 kilobytes. So packets of one kilobyte would then be able to accumulate a burst of up to 16 packets in the bucket. The regulator\u0026rsquo;s rate of eight packets per second, however, would ensure that the audio rate would be smooth to an average rate not to exceed 8 kilobytes per second or 64KBps. Setting a larger bucket size can accommodate a larger burst rate. Setting a larger value of rho can accommodate or enable a faster packet rate. The leaky bucket traffic shaper was developed in 1986 and soon to follow was a technique called (r, T) traffic shaping.\n(r, T) Traffic Shaping In (r, T) traffic shaping, traffic is divided into T-bit frames, and a flow can inject less than or equal to r bits in any T-bit frame. If the sender wants to send more than one packet of r bits, it simply has to wait until the next T-bit frame. A flow that obeys this rule has what is known as an (r, T) smooth traffic shape. In the case of (r, T) smooth traffic shaping, one cannot send a packet that\u0026rsquo;s larger than r bits long. Unless T is very long, the maximum packet size may be very small. So the range of behaviors is typically limited to fixed rate flows. Variable flows have to request data rates that are equal to the peak rate, which is incredibly wasteful if you have to configure the shaper such that the average must support whatever peak rate the variable rate flow may send. The (r, T) traffic shaper is slightly relaxed from a simple leaky bucket because rather than sending one packet every time unit, the flow can send a certain number of bits every time unit. Now there\u0026rsquo;s a question of what to do when a flow exceeds a particular rate. And typically what\u0026rsquo;s done is that if a flow exceeds its rate, the excess packets in that flow are given a lower priority, and if the network is heavily loaded or congested, the packets from a flow that exceeds a rate may be preferentially dropped. Priorities might be assigned at the sender, or at the network. At the sender, the application may mark its own packet, since the application knows best which packets may be less important. In the network, the routers may mark packets with a lower priority, which is sometimes called policing.\nShaping Bursty Traffic Patterns Sometimes we may want to shape bursty traffic patterns allowing for bursts to be sent on the network, but still ensuring that the flow does not exceed some average rate. For this we might use what\u0026rsquo;s called a token bucket. In a token bucket, Tokens arrive in a bucket at a rate rho, and beta is again the capacity of the bucket. Now, traffic may arrive at an average rate lambda_average, and a peak rate lambda_peak. Traffic can be sent by the regulator as long as there are tokens in the bucket. To consider the difference between a token bucket and a leaky bucket, consider sending a packet of size b That\u0026rsquo;s less than beta. If the token bucket is full, the packet is sent, and b tokens are removed. If the bucket is empty though, the packet must wait until b tokens drip into the bucket. If the bucket is partially full, well, then it depends. If the number of tokens in the bucket exceed little b, then the packet is sent immediately. Otherwise we have to wait until there are little b tokens in the bucket before we can send the packet.\nToken Bucket vs Leaky Bucket Let\u0026rsquo;s compare the difference between a token bucket and a leaky bucket. The token bucket permits traffic to be bursty, but it bounds it by the rate rho. On the other hand, a leaky bucket simply forces the bursty traffic to be smoothed. The bound in a token bucket is as follows. If our bucket size is beta, then we know that in any interval T, then the rate is always less than beta, that is, the maximum number of tokens that can be accumulated in the bucket, plus the rate at which tokens accumulate, times that time interval. We also know that the long term rate will always be less than rho. Token buckets have no discard or priority policies, whereas leaky buckets typically implement priority policies for flows that exceed the smoothing rate. Both are relatively easy to implement, but the token bucket is a little bit more flexible since it has some additional parameters that we can use to configure burst size. One of the limitations of token buckets is the fact that in any traffic interval of length T, the flow can send beta plus T times rho tokens of data. If a network tries to police the flows by simply measuring their traffic over intervals of length T, the flow can cheat by sending this amount of data in each interval. Consider, for example, an interval of twice this length. Well, if the flow can send beta plus T times rho in each interval, then over 2T the flow can consume 2 times beta plus tau times rho tokens. But actually this is greater than how much the flow is actually supposed to be able to send which is beta plus 2T times rho. So policing traffic being sent by token buckets is actually rather difficult. So, token buckets allow for long bursts, and if the bursts are of high priority traffic, they are difficult to police and may interfere with other high priority traffic. So there\u0026rsquo;s some need to limit how long a token bucket sender can monopolize the network.\nPolicing With Token Buckets So, to apply policing to Token Buckets, what\u0026rsquo;s often done is to use what\u0026rsquo;s called a Composite Shaper, which is to combine a Token Bucket Shaper with a Leaky Bucket. The combination of the Token Bucket Shaper with the Leaky Bucket Shaper allows for good policing. Confirming that the flow\u0026rsquo;s data rate does not exceed the average data rate allowed by the smooth Leaky Bucket is easy, but, the implementation is more complex since each flow now requires two counters and two timers. One timer and one counter for each bucket.\nToken Bucket Shaper Quiz So as a quick quiz, suppose that we have a token bucket shaper, and suppose that the size of the bucket is 100 kilobytes, that rho is ten packets per second, and that packets are one kilobyte. Assume also that we are talking about an interval of one second. Remember than in any given interval, a flow can never send more than beta plus tau times rho bits of data. Please give your answer in kilobits per second. Keeping in mind that one byte is eight bits.\nToken Bucket Shaper Solution So the maximum rate would be 100 kilobytes times 1 second plus 10 packets per second times 10 kilobytes, or 110 kilobytes, which is 880 kilobits in one second.\nPower Boost In this lesson we\u0026rsquo;ll talk about Power Boost which is a traffic shaping mechanism that was first deployed in commercial broadband networks in June 2006 by Comcast. The Power Boost allows a subscriber to send at a higher rate for a brief period of time. So if you subscribed at a rate of ten megabits per second, then Power Boost might allow you to send at a higher rate for some period of time before being shaped back to the rate at which you were subscribed at. So, Power Boost targets the Spare Capacity in the network for use by subscribers who don\u0026rsquo;t put a sustained load on the network. There are two types of Power Boosts. If the rate at which the user can achieve during this burst window is set to not exceed a particular rate. Then we say that the policy is capped Power Boost, otherwise the policy, or the shaping, is called uncapped Power Boost. Now in the uncapped setting, the configuration is simple and as we described in the last lesson. The area here is the Power Boost bucket size. That\u0026rsquo;s the maximum amount of traffic that can be sent that exceeds the sustained rate. The maximum sustained traffic rate is simply Rho, as we\u0026rsquo;ve defined it before.\nNow suppose that we wanted to cap the rate that the sender could send during the power boost window. Well all we need to do in that case is to simply apply a second token bucket with another value of Rho. That token bucket limits the peak sending rate for Power Boost eligible packets to the rate Rho C, where Rho C is larger than Rho. Remember that this value of Rho also affects how quickly tokens can refill in the bucket, so it also plays the role in the maximum rate that can be sustained during a power boost window.\nCalculating Power Boost Rates Suppose that a sender is sending at some rate r, which is bigger than the sustained rate R that they are allowed to be sending it, and suppose that our bucket size is beta. Then how long can a sender send at the rate r that exceeds the sustained rate? In other words, what is the value of d? We know that the bucket size, beta, as shown in the shaded green area, is simply d times r minus the sustained rate R. So a sender can send at the rate, little r, that exceeds the sustained rate, R, for beta divided by r minus R sustained. Now based on what we\u0026rsquo;ve learned here, let\u0026rsquo;s just take a quick quiz.\nPower Boost Quiz Suppose that the sustained rate that a subscriber subscribes to is ten megabits per second, but they like to burst at a rate of fifteen megabits per second. Suppose that the bucket size is one megabyte or eight megabits. How long can the sender send at the higher rate? Please give your answer in decimal form in seconds.\nPowerboost Solution One megabyte is eight megabits, and from our previous calculation, we know that the duration should be eight megabits, over five megabits per second, or 1.6 seconds.\nExamples of Powerboost In the Bismark Project at Georgia Tech, which you can go check out at http://projectbismark.net, we\u0026rsquo;ve done some measurements of Comcast Power Boosts in different home networks. Here are some real world examples of Power Boost\u0026rsquo;s traffic shaping profile in four different home networks, each with a different cable modem as shown in the caption. You can see that different homes exhibit different shaping profiles. Some have a very steady pattern whereas others have a\nmore erratic pattern. Interestingly you can see in some cases that there appear to be two different tiers of higher throughput rates.\nEffects on Latency Power Boost also has effect on the latency that uses perceive, as well as the loss rate. Here we\u0026rsquo;ve shown Power Boost latency effect for two different users. The latency is shown in terms of round trip time in milliseconds and the loss rates are shown on the right side of the Y axis. Latencies are also shown in a log scale. In this particular experiment, we start sending traffic here and we stop sending traffic here, in both cases. We can see that, even though power boost allows you to just send at a higher traffic rate, actually users may experience high latency and loss over the duration that they\u0026rsquo;re sending at a higher rate. The reason for this is that the access link may not be able to support the higher rate. So if a sender can only send at R sustained for an extended period of time but is allowed to burst at a rate r for some shorter period of time, then buffers may fill up and the resulting buffers may introduce additional delays in the network since packets are being buffered up rather than dropped. TCP senders can continue to send at higher rates, such as little r, without seeing any packet loss even though the access link may not be able to send at that higher rate. As a result, packets buffer up and users see higher latency over the course of the power boost interval. To solve this problem, you might imagine instead that a sender might shape it\u0026rsquo;s rate never to exceed the sustained rate, big R. If it did this, then it could avoid seeing these latency effects. So, certain senders who are more interested in keeping latency under control than they are in sending at bursty volumes, may wish to run a traffic shaper in front of a power boost enabled link. So shaping traffic to a rate of less than this sustained rate R can prevent this buffering. More details about power boost and the experiments that we\u0026rsquo;ve run to keep latency under control in its presence are available in a paper we wrote called Broadband Internet Performance, A View from the Gateway. Which appeared in Sigcomm 2011.\nBuffer Bloat In this lesson, we will talk just briefly about buffer bloat. We saw an example of buffer bloat in the last lesson where we explored the latency effects of power boost. In the example we explored, the sender could send at a rate r that was bigger than the sustained rate R without seeing packet loss. Now if there\u0026rsquo;s a buffer in the network that can support this higher rate, what we\u0026rsquo;ll see is that buffer will start filling up with packets. But this buffer can still only drain at the sustained rate R. So even though the sender might be able to send at a faster rate for a brief period of time in terms of throughput, all of those packets that the sender sent at that faster rate are queued up in line waiting to be sent. As these packets are waiting in this buffer, they\u0026rsquo;ll see higher delays than they would see if they simply arrived at the front of the queue and could be sent immediately. The delay that the packet will see in the buffer is the amount of data in the buffer divided by the rate that the buffer can drain. These large buffers can introduce delays that ruin the performance for time-critical applications such as voice and video. These large buffers actually show up all over the place: in home routers, in home WiFi devices or access points, in hosts on device drivers, and also in switches and routers. Let\u0026rsquo;s take an example of buffer bloat that we observed in home routers as part of the Bismarck study that I described in the last lesson.\nBuffer Bloat Example In the example we\u0026rsquo;ve shown here, we have three different DSL routers. The y axis shows the round trip time, or the latency to a nearby server in milliseconds, and is again shown in a log scale. We started an upload at the time 30 seconds shown on the plot. Now you can see that different modems experience a huge increase in latency when we start this upload. Some of them experience a latency of as much as one second, up from a typical latency of about ten milliseconds. One particular modem saw a round trip latency of as high as ten seconds during uploads. Now to remind you what\u0026rsquo;s going on here, is that the modem itself has a buffer. Your ISP may be upstream of that buffer, and your access link may be draining that buffer at a certain rate. TCP senders in your home will send until they see lost packets, but if the buffer\u0026rsquo;s large, the senders won\u0026rsquo;t actually see those lost packets until this buffer has already filled up. The senders continue to send at increasingly faster rates until they see a loss. As a result, packets that are arriving at this buffer see increasing delays, and senders continue to send at faster rates, because without packet loss they don\u0026rsquo;t have a signal to slow down. There\u0026rsquo;s several solutions to the buffer bloat problem. One is obviously to use smaller buffers, but given that we have a lot of deployed infrastructure, simply reducing the buffer size in deployed routers, modems, switches, home Wi- Fi devices, and so forth, is a tall order. The other thing that we can do is to use the traffic shaping methods that we have learned about. Consider that the buffer drains at a particular rate, which in this case is the rate of the uplink to the ISP. If we shape traffic such that traffic coming into the access link never exceeds the uplink that the ISP has provided us, then the buffer will never fill. Thus, by shaping traffic at the home router such that the rate that traffic is sent to the ISP never exceeds the rate of the uplink, the modem buffer will never actually fill up. This type of shaping can be done on many open WRT capable routers, including the Bismark routers that we\u0026rsquo;ve developed here at Georgia Tech.\nNetwork Measurement In this lesson we\u0026rsquo;ll be talking about network measurement, or how to see what traffic is being sent on the network. There are two types of network measurement. One is passive measurement. In passive measurement we collect packets, flow statistics, and so forth, of traffic that is already being sent on the network. So, this might include packet traces, flow statistics, or application level logs. In active measurement, we inject additional traffic into the network to measure various characteristics of the network. So we\u0026rsquo;ve seen some examples of active measurement already, such as in the previous lessons where we actively sent traffic on the network to measure speeds of downloads. Other common active measurement tools include those such as ping, and traceroute. Ping is often used to measure the delay to a particular server, and traceroute is often used to measure the network level, or the IP level path between two hosts on the network.\nWhy Measure So why do we want to measure the traffic on the network? One reason might be billing. So for example, we might want to charge a customer based on how much traffic they\u0026rsquo;ve sent on a network. In order to do so, we need to passively measure how much traffic that customer is sending. Here\u0026rsquo;s an example of measurements of inbound and outbound traffic volumes on a link on the Georgia Tech campus network. The Y axis is shown in bits per second, and the X axis is the time of day. Now, a user might be billed based on how much traffic they send on the network. A common mode of billing is called 95th Percentile billing, where a customer pays for what\u0026rsquo;s called a committed information rate, or CIR, and throughput is measured every five minutes. The customer, then, may be billed on the 95th percentile of these five minute samples. So if we were to bill on the 95th percentile of inbound traffic, we might approximate that 95th percentile by the orange line I\u0026rsquo;ve drawn here. And the customer might be billed at this rate, even though they\u0026rsquo;re allowed to sometimes burst at higher rates. Another common reason to measure is security. For example, network operators may want to know the type of traffic that\u0026rsquo;s being sent on the network so they can detect rogue behavior. A network operator may want to measure traffic on the network to detect compromised hosts or the presence of Botnets or Denial of Service attacks, two phenomena that we\u0026rsquo;ll talk about later on in the course. For the rest of this lesson, since we focused a lot on performance measurement already, I will mainly focus on passive traffic data measurement.\nHow to Measure (Passively) Let\u0026rsquo;s talk about how to perform passive Network Traffic Management. One way to do this is using the Packet and Byte Counters provided by the Simple Network Management Protocol. Many network devices provide what\u0026rsquo;s called a Management Information Base, or a MIB that can be polled or queried for particular information. One common use for SNMP is to poll a particular Interface on a Network Device for the number of Bytes or Packets that it sent. By periodically polling, we can then determine the rate at which Traffic is being sent on a link by simply taking the difference in these Packet and Byte Counters over particular intervals. The advantage of SNMP is that it\u0026rsquo;s fairly ubiquitous. It\u0026rsquo;s supported on essentially all Networking Equipment and there are many products for polling and analyzing SNMP data. On the other hand, it\u0026rsquo;s fairly coarse and you cannot express complex queries on the data. It\u0026rsquo;s coarse in the sense that because we are just polling Byte or Packet Counts on the Interface, we can\u0026rsquo;t ask specific questions such as how much traffic has been sent by a particular host or by a particular flow. Two other ways to measure passively are by monitoring at a packet granularity, whereby monitors can see full packet contents or at least headers, or at a flow level where a monitor may see specific statistics about individual flows in the network. Let\u0026rsquo;s now talk a little bit about packet and Flow Monitoring.\nPacket Monitoring So in packet monitoring, a monitor might see the full packet contents, or at least the packet headers that traverse a particular link. Common ways of performing packet monitoring that you may have tried yourself include tcpdump, ethereal, or wireshark. And in some of the exercises, you\u0026rsquo;ll get a chance to explore packet monitoring with one of these tools. Sometimes packet monitoring is performed using expensive hardware that can be mounted in servers alongside the router that forward traffic through the network. In these cases, an optical link in the network is sometimes split so that traffic can be both sent along the network and sent to the monitor. Even though packet monitoring sometimes requires this expensive hardware on very high speed links, what you do when you run tcpdump or wireshark or ethereal is essentially the same thing. Your machine acts as a monitor on the local area network. And if any packets happen to be sent towards your network interface, the network interface records those packets. Now on a switch network, you wouldn\u0026rsquo;t see many packets that weren\u0026rsquo;t destined for your own mac address. But on a network where there\u0026rsquo;s a lot of traffic being flooded, you might see quite a bit more traffic destined for an interface that you\u0026rsquo;re using to monitor. So the advantages of packet monitoring is that it provides lots of detail. You can see timing information and information in the packet headers. Unfortunately, a disadvantage is that it\u0026rsquo;s fairly high overhead. It is very hard to keep up with high speed links and often requires a separate monitoring device such as the monitoring card that we\u0026rsquo;ve shown here. What if we are happy with a little less detail than packet monitoring can provide, but we can\u0026rsquo;t afford its overhead? In that case, there is actually another approach that we can use called flow monitoring.\nFlow Monitoring In flow monitoring, a monitor which might actually be running on the router itself, records statistics per-flow. A flow consists of packets that share a common source and destination IP address, source and destination port, protocol type, TOS byte, and interface on which the packets arrive. A flow monitor can then record statistics for a flow that\u0026rsquo;s defined by the group of packets that share these features. The flow records may also contain additional information, such as the next hop IP address and other information related to routing, such as the source and destination AS on which those packets appear to be coming from and going to, based on the routing tables, as well as the prefix that those packets matched in the routing table. Flow monitoring is much less overhead than packet monitoring, but it\u0026rsquo;s also much more coarse than packet monitoring because the monitor does not see individual packets or payloads. Therefore, it\u0026rsquo;s impossible to get certain information from flow monitoring such as packet timing information. In addition to grouping packets into flows based on the fact that they share common elements in their headers, typically packets are grouped into flows if they occur close together in time. So, for example, if packets that share common sets of header fields do not appear for a particular time interval, such as 15 or 30 seconds, the router simply declares the flow to be over, and sends a flow record to the monitor based on the group of packets that it\u0026rsquo;s seen up to that point. Sometimes, to reduce monitoring overhead, flow level monitoring may also be accompanied with sampling. Sampling builds flow statistics based only on samples of the packets. So, for example, flows may be created based on one out of every ten or 100 packets, or a packet might be sampled with a particular probability and flow statistics might only be tabulated based on the packets that end up being sampled randomly from the total set of packets.\nPassive Traffic Monitoring Quiz So as a quick review, which of the following passive traffic monitoring methods, packet or flow sampling, can provide the following information? Timing information about packets, packet headers, and the number of bytes that each flow sends? Please check all boxes that apply.\nPassive Traffic Monitoring Solution Only packet monitoring can provide timing information on a packet level, or packet headers. But both methods can actually provide the number of bytes in each flow. By definition, flow records record the number of bytes in each flow as an aggregate statistic, but if you had packet-level information, you could, of course, compute the statistic yourself.\n"
},
{
	"uri": "/6750/",
	"title": "6750",
	"tags": [],
	"description": "",
	"content": " About: This course is an introductory course on human-computer interaction. It does not presuppose any earlier knowledge of human-computer interaction, computer science, or psychology. The class covers three broad categories of topics within human-computer interaction: 1) the principles and characteristics of the interaction between humans and computers; 2) the techniques for designing and evaluating user-centered systems; and 3) current areas of cutting-edge research and development in human-computer interaction.\nCourse deliverables are a series of projects focusing on design principles and method as well as two exams.\nThis class is an elective and does not count towards the foundational degree requirement.\nInstructors:  David Joyner  Resources:  Course website Course calendar Required Reading  "
},
{
	"uri": "/6250/content-distribution/",
	"title": "Content Distribution",
	"tags": [],
	"description": "",
	"content": " Content Distribution We\u0026rsquo;ve covered congestion control, which works within network protocols; traffic shaping, which is a high level network tool; and now we\u0026rsquo;ll look at content distribution, an internet-wide tool that enables websites and network operators to deliver data quickly and efficiently.\nAnd to wrap up this section of the course, your project will export TCP in its slow start state.\nThe Web and Caching In this lesson, we\u0026rsquo;ll talk about the web and how web caches can improve web performance. We will study, in particular, the hyper-text transfer protocol, or HTTP, which is an application layer protocol to transfer web content. It\u0026rsquo;s the protocol that your web browser uses to request web pages, and it\u0026rsquo;s also the protocol that the responses (or the web pages, or the objects that are returned as part of a webpage) are returned to your browser. Your web browser makes requests for web pages, and the pages and the objects in the page come back as responses. HTTP is typically layered on top of a byte stream protocol, which is almost always TCP. The client sends a request to a server asking for web content and the server responds with the content often encoded in text. The server maintains no information about past client requests. Thus we say the server is stateless. Let\u0026rsquo;s take a quick look into the format of HTTP requests, and responses.\nHTTP Requests Let\u0026rsquo;s first take a look at the contents of an HTTP Request. First there\u0026rsquo;s the Request Line which typically indicates first, a method of request, where typical methods get to return the content associated with the URL; a Post, which sends data to the server; and a Head Request which returns, typically, only the headers of the Get Response, but not the content. It\u0026rsquo;s worth noting that a Get Request can also be used to send data from the content to the server. The request line also includes the URL, which is relative, and may be something like index.HTML, and it also includes the version number of the HTTP protocol. The request also contains additional headers, many of which are optional. These include the referrer, which indicates the URL that caused the page to be requested. For example, if an object is being requested as part of embedded content in another page, the referrer might be the page that\u0026rsquo;s embedding the content. Another example header is the user agent, which is the client software that\u0026rsquo;s being used to fetch the page. For example, you might fetch a page using a particular version of Chrome or Firefox, and the user agent informs the server which client software is being used.\nExample HTTP Request Let\u0026rsquo;s take a look at an example HTTP request now. You can see here the request line, and here are some headers. Accept indicates that the client\u0026rsquo;s willing to accept any content type, that it would like the content to be returned in English, that it can accept pages that are encoded in particular compression formats. We talked about the user agents. So in this case, it\u0026rsquo;s a Mozilla 5.0 browser. Here\u0026rsquo;s the host that the request is being made to. This is particularly useful in cases where a particular web server IP address might be hosting multiple websites on the same server.\nHTTP Header Quiz As a quick quiz, which HTTP header field indicates the client software that\u0026rsquo;s being used to make the request? Accept encoding, get, user-agent, or host? Please pick the best answer.\nHTTP Header Solution The user agent field in the HTTP request header indicates the client software that\u0026rsquo;s being used to make the request.\nHTTP Response Let\u0026rsquo;s now take a look at the anatomy of an HTTP response. A response includes a status line, which includes the HTTP version, and a response code, a where the response code may indicate a number of possible outcomes. 100 response codes are typically informational, 200s indicate success. So an example 200 response code is a common server response that indicates okay. 300 response codes indicate redirection. For example, a 301 response code indicates that the page has moved permanently. 400s are errors, a well known one being 404, which is not found, and 500s indicate server errors. Other headers include the location, which may be used for redirection; a server, which indicates server software; allow, which indicates the HTTP methods that are allowed, such as get, head and so forth; content-encoding, which describes how the content is encoded (for example, if it\u0026rsquo;s compressed); content length, which indicates how long the content is in terms of bytes; expires, which indicates how long the content can be cached; and last- modified, which indicates the last time the page was modified.\nExample Response Let\u0026rsquo;s take a quick look at an example response. Here is the status line indicating the HTTP version number and a response code, OK; the date the response was sent; the server that served the request; some cookies that are used to set some state on the client (for example, whether or not the client is logged in or not); when the page expires; when it was last modified; and some more instructions about how the page can or cannot be cached. There\u0026rsquo;s also a content type header to indicate that the response is coming back in HTML format.\nEarly HTTP Now, early versions of HTTP actually only had one request or response for every TCP connection. On the plus side, this is simple to implement. But the main drawback is that it requires a TCP connection for every request, thereby introducing a lot of overhead and slowing transfer. First of all, we need a TCP three-way handshake for every request, and TCP must start in slow start every time the connection opens. This is exacerbated by the fact that short transfers are very bad for TCP because TCP is always stuck in slow start and never gets a chance to actually ramp up to steady state transfer. Also, as TCP connections are terminated after every request is completed, the servers have many connections that are forced to keep TCP connections in time-wait states until the timers expire, thus resulting in additional resources that the server needs to keep reserved even after the connections have completed. So a solution to increase efficiency and account for many of these drawbacks is to use something called persistent connections.\nPersistent Connections In persistent connections, multiple HTTP requests and responses are multiplex onto a single TCP connection. Delimiters at the end of an HTTP request indicates the end of a request and the content length allows the receiver to identify how long a response is. So, the server actually has to know the size of the transfer in advance. Persistent connections can also be combined with something call pipelining. In pipelining, a client sends the next request as soon as it encounters a referenced object. So there\u0026rsquo;s as little as one round trip time for all referenced objects before they began to be fetched. Persistent connections with pipelining is the default behavior in HTTP 1.1.\nCaching To improve performance, clients often cache parts of a webpage. Caching can occur in multiple places. Your browser can cache some objects locally on your very machine. Caches can also be deployed in the network. Sometimes your local ISP may have a web cache, and later we\u0026rsquo;ll also look at how content distribution networks are a special type of web cache that can be used to improve performance. To see how caching can improve performance, consider the case where the origin web server may host the content for a particular website, but it\u0026rsquo;s particularly far away. Now, we already know that TCP throughput is inversely proportional to round-trip times. So, the further away that this web content is, the slower the web page will load, both because latency is bigge, and because throughput is lower. If, instead, the client could fetch content from the local cache, performance could be drastically improved by fetching content from a more nearby location. Caching can also improve the performance when multiple clients are requesting the same content. In this case, not only do all of the local clients benefit from the content being cached locally, but the ISP also saves costs on transit, because it doesn\u0026rsquo;t have to pay to keep transferring the same content over these expensive links. Instead, it can simply serve the content to the clients locally. To ensure that clients are seeing the most recent version of a page, caches periodically expire content, based on the expire setter that we already saw. Caches can also check with the origin server to see whether the original content has been modified. If the content has not been modified, the origin server would respond to a cache check request with a 304, or a not modified, response. Clients can be directed to a cache in multiple ways. One is with browser configuration. So you can open your browser and explicitly configure the browser to point to a local cache so that all HTTP requests first are directed to the local cache before the request is forwarded to the origin. In the second approach, the origin server, or the service hosting the content, might actually direct your browser to a cache. This can be done with a special reply to a DNS request.\nWe can see these effects, for example, when we do a DNS look up for Google.com. The response returns a number of IP addresses, and when I ping the IP address, we see that the resulting IP address is only one millisecond away, which indicates that that server is not far away, but is in fact very likely on a local network, probably even the Georgia Tech campus network in this case.\nCaching Quiz As a quick quiz, what are some of the benefits of HTTP caching? Reduced transit costs for the local ISP, more up to date content, or improved performance for local clients? Please check all that apply.\nCaching Solution Web caching can reduce transit costs for the local ISP by preventing every HTTP request from needing to go to the origin server. Because the content\u0026rsquo;s also closer to the client, clients should also see improved performance.\nCDNs Let\u0026rsquo;s now talk a little bit about web content distribution networks, or CDNs. We\u0026rsquo;ll first talk about what a CDN is and why a content provider might want to use one. We\u0026rsquo;ll then talk about how service selection works in CDNs and how clients get redirected to the right server.\nSo, first of all, what is a content distribution network? It\u0026rsquo;s an overlay network of web caches that\u0026rsquo;s designed to deliver content to a client from the optimal location. Now, in many cases optimal means geographically closest, but sometimes optimal is not the geographically closest cache, and we\u0026rsquo;ll see some examples of when that\u0026rsquo;s the case. CDNs are made of distinct geographically disparate groups of servers, where each group can serve all the content on the CDN. These CDNs can often be quite extensive. Here is a global map depicting the deployment of the Google cache servers around the world, as mapped in a recent project by researchers at the University of Southern California. As you can see, these Web caches can be quite extensive and in many cases there\u0026rsquo;s a concerted effort to place caches as close as possible to users. Some CDNs are owned by content providers such as Google and others are owned and operated by networks such as Level 3, Limelight, and AT\u0026amp;T. Still others such as Alcamai operate independently. Non network CDNs, such as Alcamai and Google can typically place servers in other autonomous systems or ISPs. The number of cache nodes in a large content distribution network can vary. For example, in the Google Network, the USC researchers found that there were about 30,000 unique front-end cache nodes. As of about two years ago, the Alcamai Edge platform reported about 85,000 unique caching servers in nearly 1,000 unique networks around the world in 72 countries.\nChallenges in Running a CDN Operating a CDN presents many different challenges, and the underlying goal is to replicate content on many servers so that the content is replicated close to the clients. Yet this leaves many open questions including, how to replicate the content, where it should be replicated, how clients should find the replicated content, how to choose the appropriate server replica (or cache) for a particular client, and how to direct clients towards the appropriate replica once it\u0026rsquo;s selected. This problem is commonly known as server selection, and this problem is sometimes called content routing. Let\u0026rsquo;s take a look at each of these problems in a little bit more detail.\nServer Selection The fundamental problem with server selection is determining which server to direct the client to. One could do this based on a number of criteria, such as the least loaded sever, the one with the lowest network latency, or simply to any alive server to help provide fault times. Content distribution networks typically aim to direct clients towards servers that provide the lowest latency for the reasons that we talked about before, since latency plays a hugely significant role in the web performance that clients see.\nContent Routing Content routing concerns how to direct clients to a particular server. One might do this in a number of ways. One could use the routing system. For example, Anycast. So one could number all of the replicas with the same IP address and then rely on routing to take the client to the closest replica based on the routes that the internet routers choose. Routing-based redirection is simple but it provides the service providers with very little control over which servers the clients ultimately get redirected to because the redirection is at the whims of internet routing. Another way to do redirection is application based. For example, by using an HTTP redirect. This is effective but it requires the client to first go to the origin server to get the redirect in the first place, increasing latency. The third and most common way that service selection is performed is as part of the naming system using DNS. In this approach, a client looks up a particular domain name, such as Google.com, and the response contains an IP address of a nearby cache. Naming base redirection provides significant flexibility in directing different clients to different server replicas. So, in summary, routing based redirection is simple but it\u0026rsquo;s very coarse. Application based routing is also fairly simple but it incurs significant delays which operators really care about, as well as users. Naming based redirection provides fine-grained control and it\u0026rsquo;s also fast.\nNaming Based Redirection Let\u0026rsquo;s take a closer look at how naming base redirection works. In the example shown here, I\u0026rsquo;ve looked up symantec.com from two different locations. You can see that when we look up the domain name, we don\u0026rsquo;t get an A record immediately, but rather we get a CNAME, or a canonical name, which tells us to look up the following domain name in Alcamai. When we look up that domain name, we see two corresponding IP addresses. Notice that when we perform the same look up from Boston, we also get redirected to Alcamai through the CNAME, but we get two different IP addresses that are presumably more local to the Boston area. So, depending on where the client looks up the domain name, it receives different IP addresses at different locations in the network. This is how operators use DNS to redirect clients to nearby replicas.\nAs another example you can see when I ping youtube.com, I get very low latencies. And when I do a reverse lookup on this IP address, I in fact see that the content was posted on Google CDN.\nCDNs and ISPs It turns out that content distribution networks and ISPs have a fairly symbiotic relationship when it comes to peering relationships. CDNs like to peer with ISPs because peering directly with ISPs where a customer\u0026rsquo;s located provides better throughput since there are no intermediate AS hops and network latency is lower. Having more vectors to deliver a content increases reliability, and during large request events, having direct connectivity to multiple networks where the content is hosted allows an ISP to spread its traffic across multiple transit links, thereby potentially reducing the 95th percentile and lowering its transit costs. On the other hand, there are other good reasons for ISPs to peer with CDNs. First of all, providing content closer to the ISP\u0026rsquo;s customers allows the ISP to provide the customers with good performance for a particular service. For example, you could already see that Georgia Tech has placed a Google cache node in its own network, resulting in very low latencies to Google, and thereby happy customers. You can imagine that providing good performance to popular services is a major selling point for ISPs. Another reason that ISPs like to peer with CDNs or host cache nodes locally is to lower their transit costs. You can imagine that if there are a huge demand for a particular video on YouTube and all the requests and responses were going over expensive transit links, then the ISPs cost would be potentially prohibitively high. On the other hand, peering with the CDN, or hosting a local cache node, prevents all of that traffic from traversing expensive links, thus reducing costs.\nCDNs and ISPs Quiz As a quick quiz, why do ISPs want to peer with CDNS? Lower transit costs, better security, better performance for its customers, or more predictability? Please check all that apply.\nCDNs and ISPs Solution ISPs typically want to peer with CDNs to lower the transit costs and to provide better performance for their customers. CDNs don\u0026rsquo;t inherently provide better security or more predictability. And, in fact, some ways of redirecting clients to servers may actually reduce predictability.\nBit Torrent Okay, we\u0026rsquo;re now going to talk about Bit Torrent, which is a peer to peer content distribution network that is commonly used for file sharing and distribution of large files. Okay, suppose we have a network with a bunch of clients, all of whom want a particular file and the file might be particularly big. Now, those clients could all fetch the same file from the source, or the origin. But the problems with that, of course, are that the origin may be overloaded and the act of making this request for a large file from the same location on the network may also create congestion or overload at the network where the content is being hosted.\nSo, a solution is to fetch content from other peers. Rather than having everyone fetch the content from the origin, we can take the original file and chop it into many different pieces and replicate different pieces on different peers in the network, as soon as possible. So the idea is that each peer is assembling the file, but it\u0026rsquo;s assembling it by picking up different pieces of the file. And then it can retrieve the pieces that it doesn\u0026rsquo;t have from the remaining peers in the network. By trading different pieces of the same file, everyone eventually gets the full file. The idea is that hopefully we\u0026rsquo;ll be able to assemble the entire file at the end by the time all of the clients have swapped.\nBit Torrent Publishing Bit Torrent has several steps for publishing. First, a peer creates what\u0026rsquo;s called a torrent which contains metadata about tracker and all of the pieces of the file in question as well as a checksum for each piece of the file at the time the torrent was created. Now some peers in the network need to maintain a complete initial copy of the file. Those peers are called seeders. Now to download a file, a client first contacts the tracker which provides this metadata about the file, including a list of seeders that contain an initial copy of the file. Next, the client starts to download parts of the file from the seeder. Once the client starts to accumulate some initial chunks, hopefully those chunks were different than those that other clients in the network that are also trading the file have. At this point clients can begin to swap chunks. As clients begin swapping distinct chunks with one another, the idea is that eventually, after enough swapping, everyone gets a copy of the complete file. Clients that contain incomplete copies of the file are called leechers. The tracker allows peers to find each other and it also returns a random list of peers that any particular leecher can use to swap chunks of the file. Previous, peer to peer file-sharing systems used similar swapping techniques, but a problem that many of them faced, and which Bit Torrent solved, is called free-loading, whereby a client might leave the network as soon as it finished downloading a copy of the file, not providing any benefit to other clients who also want the file.\nSolution to Free-riding Bit Torrent\u0026rsquo;s solution to free-riding is called choking, which is a type of game theoretic strategy, called tit for tat. Choking is a temporary refusal to upload chunks to another peer that is requesting them. Downloading, of course, occurs as normal. But if a node is unable to download from any particular peer, it simply doesn\u0026rsquo;t upload to that peer. This ensures that nodes cooperate and eliminates the free-rider problem. If you\u0026rsquo;re interested in the game theory behind why this strategy ensures cooperation, I encourage you to go read about the repeated prisoner\u0026rsquo;s dilemma problem where a tit-for-tat strategy, such as that which is shown here, ensures cooperation among mutually distrustful parties.\nGetting Chunks to Swap One of the problems that Bit Torrent needs to solve is ensuring that each client gets chunks to swap with other clients. If all the clients received the same chunks, then no-one would want to trade with one another and everyone would have an incomplete copy of the file. To solve this problem, Bit Torrent clients use a policy called rarest piece first. Rarest piece first allows a client to determine which pieces are the most rare among clients, and download the rarest pieces of the file first. This ensures that the most common pieces are left till the end to download and that a large variety of pieces are downloaded from the seeder. Additionally, a client has nothing to trade and it\u0026rsquo;s important to get a complete piece as soon as possible. Rare pieces are typically available at fewer peers initially. Downloading a rare piece is initially maybe not a good idea. So one policy that clients use is to select a random piece of the file and download it from a seeder. In the end game the client actively requests any missing pieces from all peers, and redundant requests are cancelled when the missing piece arrives. This ensures that a single peer with the slow transfer rate doesn\u0026rsquo;t prevent the download from completing.\nDistributed Hash Tables In this lesson we will talk about distributed hash tables, which enable a form of content overlay called a structured overlay. We\u0026rsquo;ll talk about a particular distributed hash table called Chord and an underlying mechanism that enables it, called consistent hashing. Chord is a scalable, distributed lookup service. A lookup service is simply any service that maps keys to values. Examples of lookup services on the internet include DNS and directory services. Chord has some desirable properties, including scalability, provable correctness, and reasonably good performance that\u0026rsquo;s also fairly easy to reason about.\nChord Motivation The main motivation of Chord is scalable location of data in a large distributed system. So a publisher might want to publish the location of a particular piece of data, such as an MP4 with a particular name, such as Annie Hall. It needs to figure out where to publish this data in a place that the client can find it so that when the client performs a look up for Annie Hall, it\u0026rsquo;s directed to the right location that is hosting the data. The key problem that we need to solve here is look up and you can see that the function that needs to be provided is just a simple hash table, but the thing that makes this problem interesting is that the hash table isn\u0026rsquo;t located in one place but that it\u0026rsquo;s distributed across the network. So what we\u0026rsquo;re trying to build is what\u0026rsquo;s called a distributed hash table or a DHT. The way that we\u0026rsquo;re going to build this is using a mechanism called consistent hashing.\nConsistent Hashing In consistent hashing, the main idea is that the keys and the nodes map to the same ID space. So what we\u0026rsquo;re going to do is create a metric space, such as a ring, and we\u0026rsquo;ll put nodes on this ring, and the idea is that these nodes each have some ID. Now the keys should also map to the ID space. So in this case, just for the sake of example, let\u0026rsquo;s suppose that we have a six bit ID space, so ID\u0026rsquo;s might range from zero to 63. Now you can see that the nodes have ID\u0026rsquo;s, and the keys also have ID\u0026rsquo;s in the same space. A consistent hash function will assign the nodes and the keys and identifier in this space. A hash function such as SHA-1 might be used to assign these identifiers. In the case of nodes, the ID might be a hash of the IP address. In the case of keys, the ID might simply just be the hash of a key. Both of these hash operations create ID\u0026rsquo;s that are uniformly distributed in the ID space. The question now is how to map the key ID\u0026rsquo;s to the node ID\u0026rsquo;s, so that we know which nodes are responsible for resolving the look ups for a particular key.\nThe idea in chord is that a key is stored at its successor, which is the node with the next highest ID. So, for example, the key corresponding to the key ID of 60 would be stored at the node with the node ID of one, similarly for the key with the key ID of 54. Forty-two would be stored at the node with the node ID of 43, 17 at the node with 32, seven and five at the node with ID of 10, and so on. Consistent hashing offers the properties of load balance, because all nodes receive roughly the same number of keys and flexibility because when a node joins or leaves the network, only a fraction of the keys need to be moved to a different location. You can actually prove that the solution is optimal, meaning that the minimal number of keys need to be remapped to maintain load balance when a node joins or leaves the network.\nImplementing Consistent Hashing Let\u0026rsquo;s talk a little bit about how to implement consistent hashing. One option is for every node to know the location of every other node. In this case, lookups are fast. In fact, they are order one, but the routing tables are large. In particular, because every node needs to know the location of every other node in the network,, the routing table must be order N, where N is the number of nodes in the network.\nSo, for example, if node 32 wanted to look up the location of Annie Hall, that value might hash to 60, and if every node maintains a routing table entry for every other node, 32 would know that the key corresponding to ID 60 was located at node one. So the look up, would be order one, but the table ,would be order N.\nAnother option is that each node only knows the location of its immediate successor in the ring. So, for example, node 32 would know the location of node 43, but of no other node. This results in a small table, of size order one. But locating the content, as before, would require order N lookups. So in summary, if every node knows the location of every other node, then lookups have good performance at the expense of larger tables. If every node only knows its successor, then routing tables can be small, but every lookup operation is order N.\nFinger Tables A solution that provides the best of both worlds is called finger tables, where every node knows m other nodes in the ring and the distance of the nodes that it knows increases exponentially. So, for example, node 10 would maintain mappings for 10 plus 2 to the 0, 10 plus 2 to the 1, and so\nforth, where finger i Points to the successor of n plus 2i. So finger 0 would point to the successor of 11, which is 32; Finger 1 would also point to 32 and so forth. Finger 5 would point to 43. Now every node knows its immediate successor. So what you want to do is find the predecessor for a particular ID and then ask for the successor of that ID. So let\u0026rsquo;s suppose that node 10 wanted to look up a key corresponding to the id of 42. It can use the finger tables to find the predecessor of that node, which in this case, is 32. Its finger tables have the mapping of that nodes location as well. It then can ask node 32 for its successor. At this point, we can move forward around the ring looking for the node whose successor\u0026rsquo;s ID is bigger than the ID of the data, which in this case is node 43. Due to the structure of the finger table, these lookups require order of log n hops. This results in efficient lookups, order log n messages per look up, and the size the finger table is order of log n state per node. Another consideration that we have to take into account is what happens when nodes join and leave the network. When a new node joins, we first have to initialize the fingers of this new node. Then we must update the fingers of existing nodes so that they know that they can point to the node with the new ID. And finally, the third step is to transfer the keys from the successor to the new node. In this case, the key that we must transfer from the successor, one, is the data with ID of 54. In this case, each node\u0026rsquo;s successor is maintained and the successor of any particular ID k is always responsible for k. A fallback for handling leaves is to ensure that any particular node not only keeps track of its own finger table, but also of the fingers of any successor, so that if a node should fail at any time, then the predecessor node in the ring also knows how to reach the nodes corresponding to the entries in the failed nodes finger table.\n"
},
{
	"uri": "/6460/",
	"title": "6460",
	"tags": [],
	"description": "",
	"content": " About: This class is simultaneously an introductory course about educational technology and an advanced, project-oriented class on designing or researching technology\u0026rsquo;s intersection with education. As such, the course provides information about a large number of topics within educational technology, including pedagogical strategies, research methodologies, current tools, open problems, and broader issues. The scope of the material provided goes beyond what any one person could reasonably learn in a semester. Instead, you will select those areas that appeal to you or that support your ultimate project ideas. For example, if you’re interested in research, you may focus on the applicable research methodologies to your chosen area of investigation, relevant pedagogical strategies or theories, and the current state-of-the-art within that community. If you’re interested in design, you may focus on the relevant pedagogical strategies or theories for your chosen domain, the current popular tools within that domain, and open problems that need to be addressed.\nCourse deliverables are a series of essays/papers followed by a final project (group or individual). Students work with a mentor throughout the course to select a topic, conduct research, and complete the project.\nThis class counts towards the foundational degree requirements, and is an interaction elective in the Interactive Intelligence specialization.\nInstructors:  David Joyner  Resources:  Course website Course calendar Course wiki Edtech Project Archive  "
},
{
	"uri": "/6250/software-defined-networking/",
	"title": "Software Defined Networking",
	"tags": [],
	"description": "",
	"content": " Software Defined Networking Introduction Operations and Management Overview Welcome to the final third of the course. In the first section, you reviewed the basic building blocks of the internet, and in the second section, you learned how networks deal with large amounts of network traffic. In the final section, you\u0026rsquo;ll learn how network operators manage their networks. More importantly, these topics will introduce you to the forefront of networking research.\nThat\u0026rsquo;s right. We\u0026rsquo;re going to cover software defined networking, traffic engineering, and network security. Let\u0026rsquo;s get started.\nNetwork Management Overview Welcome to the third course in CS 6250 where we will be discussing network operations and management. This segment in the course has three lessons. The first lesson is focused on software-defined networking and its role in making network operations and management easier. The second module covers traffic engineering, which is the process by which network operators reconfigure the network to balance traffic demands across the network. The third lesson covers network security. We will start with a lesson on Software Defined Networking. But, before we jump into the details, I\u0026rsquo;d like to motivate, a little bit—why? In particular, I plan to tell you about the role of network operators in running the network.\nSo what is network management? Network management is the process of configuring the network to achieve a variety of tasks. Network configuration achieves a variety of tasks including balancing traffic load across the network, achieving various security goals, and satisfying business relationships that may exist between the network that\u0026rsquo;s being configured and neighboring networks, such as the network\u0026rsquo;s upstream Internet service provider. A key aspect to network management is configuring the network. Unfortunately, if the network is not configured correctly, many things can go wrong. Configuration mistakes can lead to problems such as persistent oscillation, whereby routers can\u0026rsquo;t agree on a route to a destination; loops, where packets get stuck in between two or more routers and never actually make it to the destination; partitions, whereby the network is split into two or more segments that are not connected; and black holes, where packets reach a router that does not know what to do with the packet and drops it as opposed to sending it on to its ultimate destination.\nWhy is Configuration Hard So why is configuration hard to get right? First, it\u0026rsquo;s difficult to define what we mean by correct behavior in the first place. Second, the interactions between multiple routing protocols can lead to unpredictability. Furthermore, each autonomous system on the internet is independently configured, and the interaction between the policies of these autonomous systems can lead to unintended, or unwanted behavior. The third reason that configuration is hard is that operators simply make mistakes. Configuration is difficult, and network policies are very complex. Furthermore, Network configuration has historically been distributed across hundreds or more network devices across the network, where each device is configured with vendor-specific low- level configuration. We\u0026rsquo;ll see in the first part of this course how Software Defined Networking or SDN changes this by centralizing the network\u0026rsquo;s configuration in a logically centralized controller.\nAt a very high level, Software Defined Networking provides exactly the primitives that operators need to run the network better. In particular, SDN provides operators three things. The first is network-wide views of both topology and traffic. The second is the ability to satisfy network level objectives such as those that we talked about before including load balance, security, and other high level goals. The third thing that software defined networking provides (that network operators need) is direct control. In particular, rather than requiring network operators to configure each device individually with indirect configuration, SDN allows an operator to write a control program that directly affects the data plane. So rather than having to configure each device individually and guess or infer what might happen, software-defined networking allows a network operator to express network level objectives and direct control from a logically centralized controller.\nSo to make network operations easier, routers should forward packets since router hardware is specialized to forward traffic at very high rates. They should collect measurements such as traffic statistics and topology information. But, on the other hand, there\u0026rsquo;s no reason that a router should have to compute routes. Although conventionally routing has operated as a distributed computation of forwarding tables, the computation doesn\u0026rsquo;t inherently need to run on the routers. Rather, the computation could be logically centralized and controlled from a centralized control program. This logical centralization is the fundamental tenant of SDN. So a simple way of summing up Software Defined Networking is simply to remove routing from the routers, and perform that routing computation at a logically centralized controller. Now of course, SDN has evolved to incorporate a much broader range of controls than simply routing decisions, and we\u0026rsquo;ll talk about the range of control that SDN controllers enable in today\u0026rsquo;s networks throughout this lesson.\nSoftware Defined Networking Let\u0026rsquo;s start with a brief overview of Software Defined Networking, or SDN. We\u0026rsquo;ll first start by defining SDN, and in particular we\u0026rsquo;ll talk about what is a Software Defined Network. Then we\u0026rsquo;ll talk about what are the advantages of SDN over a conventional network architecture. We\u0026rsquo;ll overview the history of SDN, the infrastructure that supports it (in particular how SDNs are designed and built), and the applications of SDN. Specifically, what they can be used for and how they can be used to simplify various network management tasks.\nPerhaps the best way to understand what an SDN is, is to compare it to the behavior of today\u0026rsquo;s networks. Today\u0026rsquo;s networks have two functions. The first is the Data Plane, whose task it is to forward packets to their ultimate destination. But in order for the Data Plane to work, we also need a way of computing the state that each of these routers has that allows the routers to make the right decision in forwarding traffic to the destination. The state that lives in each of these routers that allows the routers to make these decisions about how to forward packets are called routing tables. It\u0026rsquo;s the job of the network\u0026rsquo;s Control Plane to compute these routing tables. In conventional networks, the Control and Data Plane both run on the routers that are distributed across the network. In an SDN, the Control Plane runs in a logically centralized controller. Additionally, the controller typically controls multiple routers across the network and often, the control program exerts control over all the routers in the network, thus facilitating network-wide control. These two characteristics are the defining features of a Software Defined Network. The separation of data and control allows a network operator to build a network with commodity devices, where the control resides in a separate control program. This re-factoring allows us to move from a network where devices are vertically integrated (making it very tough to innovate) to a network where the devices have open interfaces that can be controlled by software, thus allowing for much more rapid innovation.\nLet\u0026rsquo;s survey a brief history of SDN. Previous to 2004, configuration was distributed, leading to buggy and unpredictable behavior. Around 2004, we had the idea to control the network from a logically centralized high level program. That logically centralized controller focused on the border gateway protocol and was called the routing control platform, or RCP. In 2005, researchers generalized the notion of the RCP for different planes. The decision plane, which computed the forwarding state for devices in the network; the data plane, which forwarded traffic based on decisions made by the decision plane; and the dissemination and discovery planes, which provide the decision plane the information that it needs to compute the forwarding state which ultimately gets pushed to the data plane. Around 2008, these concepts effectively hit the mainstream through a protocol called OpenFlow. OpenFlow\u0026rsquo;s intellectual roots are with the RCP and 4D, but OpenFlow was made practical when merchant silicon vendors opened their APIs, so that switch chipsets could be controlled from software. So suddenly there was an emergence of cheap switches that were build based on open chip sets that could be controlled from software. This development effectively allowed us to decouple the control plane and the data plane in commodity switching hardware.\nAdvantages of SDN SDN has many advantages over conventional networks. It\u0026rsquo;s easier to coordinate behavior among a network of devices, the behavior of the network is easier to evolve, and it\u0026rsquo;s also easier to reason about. These characteristics are all rooted in the fact that the control plan is separate from the data plane. Having a separate control plane or control program allows us to provide conventional cs techniques to old networking problems. So, whereas before it was incredibly difficult to reason about or debug a network\u0026rsquo;s behavior, if the network behavior is now controlled by a logically centralized control program, we can use techniques from programming languages or software engineering to help us reason about the behavior of the network.\nAs far as SDN\u0026rsquo;s infrastructure is concerned, the Control Plane is typically a software program written in a high level language, such as Python or C. On the other hand, the Data Plane is typically programmable hardware that\u0026rsquo;s controlled by the control plane. The controller effects the forwarding state that\u0026rsquo;s in the switch using control commands. Open flow is one standard that defines a set of control commands by which the controller can control the behavior of one or more switches.\nSDN has many applications including data centers, wide area backbone networks, enterprise networks, internet exchange points (or IXPs), and home networks. Later modules in this course will explore how software defined networks can solve network management problems in some of these areas. In this course we will focus in particular on the first three applications.\nControl Plane Operations Quiz So as a quick quiz, which of the following are examples of control plane operations? Computing a forwarding path that satisfies some high-level policy such as an access control policy? Computing a shortest path routing tree? Rate-limiting traffic so that the overall sending rate doesn\u0026rsquo;t exceed a certain throughput? Load balancing traffic based on a hash of the packet source IP address? Or authenticating a user\u0026rsquo;s device based on its MAC address? Please check all options that apply.\nControl Plane Operations Solution The job of the Control Plane is to compute the state that ultimately ends up in the data plane. So computing a forwarding path that satisfies a high-level policy is something that the Control Plane would do. The Control Plane can also compute shortest path routing trees. And it might make decisions about whether or not a user\u0026rsquo;s device should be allowed to send traffic or not based on that device\u0026rsquo;s MAC address. Rate-limiting is something that is typically done in the data plane, and the load-balancing example that we have listed here is such that a router or a switch would make decisions in the data plane based on a hash of the source IP address. So all of the decisions are being made at forwarding time, not by a centralized high-level program.\nSeparating Data and Control Let\u0026rsquo;s quickly review the difference between the Control plane and the Data plane. The control plane is the logic that controls forwarding behavior. Examples of control plane functions include routing protocols as well as logic for configuring network middle boxes. Now, a routing protocol might compute shortest paths or a topology, but ultimately, the results of such computations must be installed in switches that actually do the forwarding. The forwarding table themselves and specifically the actions associated with forwarding traffic according to the Control plane logic is what constitutes the data plane. So examples of data plane functions include forwarding packets at the IP layer and doing things like switching at layer two. So, to reiterate, routing protocol functions that compute the paths are control plane functions, whereas the act of actually taking a packet on an input port and forwarding it to an output port, is a data plane function.\nSo why is separating the data and control planes a good idea? The first reason is independent evolution and development. Thus, software control of the network can evolve independently of the network hardware. The second reason that separating data and control plane is a good idea is the opportunity to control the network behavior from a high-level software program. Controlling the network from a high-level program in theory allows network operators to debug and check network behavior more easily than in the status quo where network behavior is determined by the distributed low level configuration across hundreds of switches and routers.\nThe separation of data and control provides opportunities for better network management in data centers by facilitating such network tasks as virtual machine migration to adapt to fluctuating network demands. In Routing, the separation of data and control provides more control over decision logic. In Enterprise networks, SDN provides the ability to write security applications such as applications that manage network access control. In Research networks, the separation of data and control effectively allows us to virtualize the network, so that research networks and experimental protocols can co-exist with production networks on the same underlying network hardware.\nReasons for Separating Data and Control Quiz So as a quiz, what are some of the reasons for separating the control and data planes? Eliminating a single point of failure? Ability to scale to much larger networks? Independent evolution of the data and control plane? Separating vendor hardware from control logic? Or ease of reasoning about network behavior? Please check all options that apply.\nReasons for Separating Data and Control Solution Separating the data and control plane can allow for independent evolution of the data and control plane, separating vendor hardware from the logic that controls the behavior of the network, and the potential to more easily reason about network behavior since the behavior is now controlled from a single, logically-centralized control program. While it\u0026rsquo;s possible that separating the control plane from the data plane could result in architectures that are more fault tolerant or more scalable, the separation of data and control planes does not inherently make the network more fault tolerant or more scalable. Therefore, neither of the first two options apply.\nExample Data Centers One example where SDN can provide huge wins, is in the data center. A data center typically consists of many racks of servers, and any particular cluster might have as many as 20, servers. Assuming that each one of these servers can run about 200 virtual machines, that\u0026rsquo;s 400,000 virtual machines in a cluster. A significant problem is provisioning or migrating these virtual machines in response to varying traffic loads. SDN solves this problem by programming the switch state from a central database. So, supposing I have two virtual machines within the data center that need to communicate with one another, the forwarding state in the switches in the data center ensures that traffic is forwarded correctly. If we need to provision additional virtual machines or migrate a virtual machine from one server to another in the data center, the state in these switches must be updated. Updating the state in this fashion is much easier to do from a central controller or a central database. Facilitating this type of Virtual Machine Migration in the data center is one of the early killer apps of software-defined networking. This type of migration is also made easier by the fact that the servers are addressed with Layer two Addressing, and the entire data center looks like a flat, layer two topology. What this means is that a server can be migrated from one portion of the data center to another without requiring the virtual machine to obtain new addresses. All that needs to happen for forwarding to work is the state of these switches needs to be updated. The task of updating switch date in this fashion is very easy to do when the control and data planes are separate.\nManaging Data Centers Quiz Let\u0026rsquo;s have another quiz on data centers. So how does the control/data plane separation make managing data centers easier? The ability to monitor and control routes from a central point of control? The ability to migrate virtual machines without renumbering host addresses? A requirement for fewer switches? Or making load balance automatic? Please again check all that apply.\nManaging Data Centers Solution So as we discussed, control/data plane separation can make it easier to manage the data center by monitoring and controlling routes from a central point and allowing virtual machines to be migrated without renumbering host addresses. The control/data plane separation does not inherently make it possible to build a data center with few switches nor does it automatically balance load.\nChallenges As another example where control and data plane separation comes in handy, let\u0026rsquo;s look at the security of internet backbones, where filtering attack traffic is a regular network management task. Suppose that an attacker is sending lots of traffic towards a victim. In this case a measurement system might detect the attack, identify the entry point, and a controller such as the RCP might install what is called a null route to ensure that no more traffic reaches the victim from the attacker.\nTwo fundamental challenges with SDN are scalability and consistency. In an SDN, a single control element might be responsible for many forwarding elements. So control elements might be responsible for hundreds to thousands of switches. Of course, for redundancy and reliability, typically we want to replicate the controller. So while the controller is logically centralized, physically there may be many replicas. And in such a deployment scenario, we need to ensure that different controller replicas see the same view of the network so that they make consistent decisions when they\u0026rsquo;re installing state in the data plane. A final challenge that\u0026rsquo;s also worth mentioning is security, or robustness. In particular, we want to make sure that the network continues to function correctly in the event that a controller replica fails or is compromised.\nCoping With Scalability Quiz So as a brief quiz or thought question, let\u0026rsquo;s think about some approaches for coping with the scalability associated with control and data plane separation. One could, for example, eliminate redundant data structures in the controller. Or only perform control operations for a limited number of network operations, such as only performing control operations for routing decisions. One might send all traffic through the controller to minimize forwarding decisions that routers and switches need to make. One could cache forwarding decisions in the switches, or run multiple controllers.\nCoping With Scalabilty Solution Eliminating redundant data structures can help save memory in the control program running at the controller. Only performing a fixed number of network management operations, such as routing, can insure that the controller doesn\u0026rsquo;t have to do too much, thereby improving scalability. Caching forwarding decisions that the control plane has already made in the switches can ensure that not too much traffic is redirected to the controller. And running multiple controllers can distribute the load of the control plane across multiple replicas. Sending all traffic to the controller only increases the controller load, and would not help with scale ability.\nDifferent SDN Controllers Now that we have a better understanding of the benefits of separating the data and control plane, let\u0026rsquo;s now have a look at the many different options for SDN controllers. There are a number of different SDN controllers that exist, including NOX, Ryu, Floodlight, Pyretic, Frenetic, Procera, RouteFlow, Trema, and the list goes on. In this lesson, we will explore the merits of these three controllers. And when we get to the lesson on Programming SDN, we will take a close look at Frenetic and Pyretic. Let\u0026rsquo;s now jump in and take a look at these three controllers.\nNOX Overview Nox was a first generation open flow controller. It is open source, stable, and widely used. There are two flavors of Nox, Classic Nox and the New Nox. Classic Nox was written in C++ and Python and is no longer supported. The new Nox is C++ only. The Code base is fast, clean, and well supported. More information about Nox is available at noxrepo.org.\nIn a Nox network, there may be a set of switches and various network-attached servers. The controller maintains a network view and the controller may also run several applications that operate on that network view. The basic abstraction that NOX supports is a switch control abstraction where OpenFlow is the prevailing protocol. Control is defined at the granularity of flows which are defined by a ten-tuple in the original OpenFlow specification. So depending on whether a particular packet matches a subset of values specified as a flow rule, the controller may make different decisions for packets that belong to different parts of flow space, orr packets that match different subsets of the fields defined by a flow.\nA flow is defined by the header or the 10-tuple which I just alluded to, a counter which maintains statistics, and actions that should be performed on packets that match this particular flow definition. Actions might include forwarding the packet, dropping it, or sending it to the controller. When a switch receives a packet, it updates its counters for counting packets that belong to that flow and applies the correspondence actions for that flow, which might include forwarding, dropping or sending to a controller.\nThe basic programmatic interface for the Nox controller is based on events. A controller knows how to process different types of events, such as a switch, join, or leave; a packet in or packet received event should the switch redirect packet to controller; as well as various statistics. The controller also keeps tracks of a network view, which includes a view of the underlying network topology, and it also speaks a control protocol to the switches in the network. That control protocol effectively allows the controller to update the state in the network switches. The Nox controller implements the OpenFlow protocol.\nNox is implemented in C++, and it supports OpenFlow 1.0. A fork of Nox called CPQD supports versions 1.1, 1.2, and 1.3. The programming model is event based and a programmer can write an application by writing even handlers for the Nox controller. NOX provides good performance but requires you to understand and be comfortable with the facilities and semantics of low level OpenFlow commands. Later in this module, we will explore controllers based on pyretic and frenetic that do not have this characteristic. NOX also requires the programmer to write the control application in C++, which can be slow for development and debugging. To address the shortcomings that are associated with development in C++, Pox was developed. Pox is widely used, maintained, and supported. It\u0026rsquo;s also easy to use, and easy to read and write the control programs. Of course, as might come with implementing a controller in python, the performance of Pox is not as good as the performance of Nox.\nWhen to Use Pox Quiz So as a quick quiz, when might you use Pox? In a class project? In a large internet data center? Or in a university research project? Please check all that apply.\nWhen to Use Pox Solution You might use Pox in a class project or in a university research project where there\u0026rsquo;s a need to quickly prototype and evaluate a brand new control application. Pox is less applicable in a large internet data center because it does not perform as well as other controllers.\nRyu, Floodlight, Nox, and Pox Other controllers include Ryu, which is an open source Python controller. Ryu supports OpenFlow 1.0, 1.2, and 1.3, as well as the Nicira extensions. It also works with Open Stack. The support for the later versions of OpenFlow and the integration with the Open Stack, are advantages over other SDN controllers. Because Ryu is implemented in Python, it still does not perform as well other SDN controllers, such as Nox. Another popular SDN controller is Floodlight. Floodlight is written in Java, it supports Overflow 1.0, and is a fork from the early Beacon controller. Floodlight is maintained by big switch networks. Advantages include good documentation, integration with the REST API, and good performance. Unfortunately, it also has a fairly steep learning curve. So you should use Floodlight if you already know Java, if you need production level performance and support and you will use the REST API to interact with the controller. So we can compare these two controllers with the two controllers that we already discussed, Nox and Pox. We have controllers in three different languages: Python, Java, and C++. We have controllers that support later versions of OpenFlow, and support Open Stack. And we have controllers that provide better performance, as well as controllers that are easier to use for rapid prototyping. All of these controllers are still relatively hard to use because they entail interacting directly with OpenFlow flow table modifications, which operate at a very low level of matching, on flows and performing specific actions. As we\u0026rsquo;ll see, it\u0026rsquo;s possible to develop programming languages on top of these controllers that make it much easier for a network operator to reason about network behavior. Before we jump into higher level programming languages, however, let\u0026rsquo;s first see how we can use these existing control frameworks to customize network control.\nCustomizing Control In this lesson, we will learn how to write control programs to customize network control, we will review the operation of a hub and a learning switch, then we will explore how to use the POX controller to create a simple MiniNet topology, and then we will explore how to customize the Pox controller to perform two types of network control.\nAs a review, when a host sends a packet to a hub, the hub maintains no state about which output port a package should be forwarded to reach a particular destination. Therefore, the hub simply forwards the input packet on every output port. In Pox, this code is fairly simple. When the controller starts, it adds a listener that listens for a connection up, which is a connection from a switch. When the switch connects, it simply sends an OpenFlow flow modification back to the switch which says flood all packets out every output port. The first function here involved creates the open-flow massage and the second sends that message back to the switch.\nIn contrast, a learning switch maintains a switch table that\u0026rsquo;s initially empty. But when a packet arrives on input port one, the switch creates a table that associates host A with output port one such that whenever a subsequent packet is destined for destination A, the switch knows to forward the packet via output port one. I won\u0026rsquo;t show you the full Python code here, but it\u0026rsquo;s fairly simple, and you can go look at the Pox distribution to see the learning switch example. As before, when the first packet arrives at the switch, it is diverted to the controller, at this point, the controller maintains a hash table that maps the address to the output port. When it sees that first packet from Host A, it updates the address and port table. If the packet\u0026rsquo;s a multicast packet, the controller makes a decision to flood that packet on all output ports. Likewise, if there\u0026rsquo;s no table entry for the destination for that packet, the controller also instructs the switch to forward the packet on all output ports. If the source and destination address are the same, the controller instructs the switch to drop the packet. Otherwise, the controller installs the flow table entry corresponding to that destination address and output port. Installing that flow table entry in the switch prevents future packets for that flow from being redirected to the controller. Rather, all subsequent packets on that flow can be handled directly by the switch, since it now knows which output port to send a packet for that particular destination.\nSummary OpenFlow makes modifying forwarding behavior easy because forwarding decisions are based on matches on the OpenFlow 10-tuple. Layer two switching is simply a match on the destination Mac address which has a corresponding action of forwarding out a particular output port. If all of the fields are specified for forwarding out a particular output port, then we have flow switching behavior. If all of the flow specifications are wild carded except for, say, the source MAC address to make a forwarding or drop decision, then we have a firewall. Constructing a firewall is as simple as building a hash table that stores key value pairs, where the table maps a switch and source MAC address to a true or false value depending on whether traffic should be forwarded or dropped. The controller might then only decide to forward traffic if the firewall entry maps to true.\nIt is important to emphasize the performance implications of caching the decisions at the switch. So, packets only reach the controller if there\u0026rsquo;s no flow table entry at the switch. If on the other hand, there is a float table entry at the switch, then the switch can simply forward the packets rather than sending them to the controller. So when a controller decides to take an action on a packet, it installs that action as a flow table entry in the switch, and that decision or flow table entry is cached until that flow table entry expires.\nIn summary, customizing control is easy. We\u0026rsquo;ve explored how to use the POX controller to develop alternate control programs. And it\u0026rsquo;s possible to turn a switch into a firewall in less than 40 lines of python code. We also explored the performance benefits of caching rules and decisions to avoid sending too much traffic to the controller. As we know, forwarding performance in a switch is as fast, but whenever we have to send traffic to the controller, it slows things down. So whatever decisions we can cache in the switch will only serve to improve the performance of the network.\n"
},
{
	"uri": "/6250/programming-software-defined-networks/",
	"title": "Programming Software Defined Networks",
	"tags": [],
	"description": "",
	"content": " Programming Software Defined Networks Now that you know that network management is a tough, complicated process, we\u0026rsquo;re going to take a look at one of the most recent advancements in networking, software defined networking.\nAlong the way, you\u0026rsquo;re going to complete two exciting projects in Mininet. In the first, you\u0026rsquo;ll write your own virtual switch. In the second, you\u0026rsquo;ll use a programming language designed for software-defined networking to create a firewall.\nUpdates in Software Defined Networks In this lesson we\u0026rsquo;ll be exploring consistent updates in SDN\u0026rsquo;s. As a reminder from the last lesson, we looked at how to update switch flow table entries using OpenFlow control commands from the control. The OpenFlow API, however, does not provide specific guarantees about the level of consistency that packets along an end-to-end path can experience. So for example, updates to multiple switches along a path in a network that occur at different times may result in problems such as forwarding loops. Additionally, if updates to the switches along an end-to-end path occur in the middle of a flow, packets from the same flow may be subjected to different network states. These two problems are known as consistency problems. The first problem is known as a packet level consistency problem, and the second problem is known as a flow level consistency problem. In this lesson, we will explore these problems in more detail and look at various approaches to guaranteeing consistent updates in SDNs. To think about consistency properly, we first need a notion of a high level programming model that sits on top of what we would call the southbound interface. We\u0026rsquo;ll talk about how to write applications that use the controller interface that we learned about in the last lesson that can rely on a better notion of consistency than existing controller platforms currently provide. Let\u0026rsquo;s first think about how we want to program these applications and what type of abstraction the applications would require from the underlying control interface.\nSDN Programming Introduction Let\u0026rsquo;s consider a network of SDN switches, such as OpenFlow switches, and a controller that is controlling those switches, and let\u0026rsquo;s assume that we would like to write a program using this interface. We can think about this programming as proceeding in three steps. The first is that the controller needs to read or monitor network state, as well as various events that may be occurring in the network. These events may include failures, topology changes, security events, and so forth. The second step is to compute the policy based on the state that the controller sees from the network. This is effectively what we talked about last time, is the role of the decision plane, in deciding what the forwarding behavior of the network should be, in response to various states that it reads from the network switches. The third step is to write policy back to the switches by installing the appropriate flow table state into the switches. Consistency problems can arise in two steps. First, the controller may read state from the network switches at different times, resulting in an inconsistent view of the network-wide state, and second, the controller may be writing policy as traffic is actively flowing through the network, which can disrupt packets along an end-to-end path or packets that should be treated consistently because they\u0026rsquo;re part of the same flow. Both reading and writing networks state can be challenging because OpenFlow rules are simple match action predicates, so it can be very difficult to express complex logic with these rules. If we want to read state that requires multiple rules, expressing a policy that allows us to read such a state can be complicated without more sophisticated predicates.\nFor example, let\u0026rsquo;s suppose that when we are reading state, we\u0026rsquo;d like to see all web serving traffic except for source 1.2.3.4. Simple match action rules do not allow us to express such exceptions. As a solution to this problem, we need a language primitive that allows us to express predicates. Here is a simple statement that has several predicates, such as AND and NOT. A runtime system can then translate these predicates into low-level OpenFlow rules, ensuring that they are installed atomically and in the right order. Another problem that arises is that switches only have limited space for rules. It\u0026rsquo;s simply not possible to install all possible rule patterns for every set of flows that we\u0026rsquo;d like to monitor.\nFor example, if we\u0026rsquo;d like to count the number of bytes for every source IP address and generate a histogram with the resulting traffic, we would potentially need a flow table entry for every possible source IP address. It\u0026rsquo;s simply not possible to install all of these possible rules. The solution is to have the run time system dynamically unfold rules as traffic arrives. A programmer would specify something like a group by source IP address, and the run time system would dynamically add open flow rules to the switch as traffic arrives, thereby guaranteeing that there are only rules in the switch that correspond to active traffic.\nReading Network State Another problem that arises when reading state is that extra, unexpected events may introduce inconsistencies. A common programming idiom is that the first packet goes to the controller and once the controller figures out what policy to apply for that flow, the controller then installs rules in the switches, in the network, corresponding to that flow. What if more packets should arrive at the switch before the controller has a chance to install rules for that flow? At this point, multiple packets may reach the controller, but the application it is running on top of the controller may not need or want to see these additional packets. So, the solution is to have the programmer specify by a high level language a limit of one, indicating that the application should only see the first packet of the flow and that the subsequent packet should be suppressed. The runtime system then hides the extra events.\nSo to remind you where we are, we talked about problems with consistency when reading state from the network, and we talked about three approaches to helping guarantee consistency when reading state: predicates, rule unfolding, and suppression. And let\u0026rsquo;s now talk about primitives that can help maintain consistency when writing state.\nWriting Network Policy There are many reasons that a controller might want to write policy to change the state and the network switches, including maintenance, unexpected failure, and traffic engineering. Any of these network tasks involve or require updating state in the network switches, and when that state transition happens, we want to make sure that forwarding remains correct and consistent. In particular, we would like to maintain the following invariance. There shouldn\u0026rsquo;t be any forwarding loops and there shouldn\u0026rsquo;t be any black holes whereby a router or switch receives a packet and doesn\u0026rsquo;t know what to do with it. There also shouldn\u0026rsquo;t be cases where traffic is going where it shouldn\u0026rsquo;t be allowed to go because of the network being in an inconsistent state.\nLet\u0026rsquo;s now consider an example of what might happen when policies are written to the network if they\u0026rsquo;re written in an inconsistent fashion. Let\u0026rsquo;s consider a case where we have a network that is performing shortest routing to some destination, and the link weights are as shown here in the figure. Traffic in the network would flow along the path shown in green. Let\u0026rsquo;s suppose now that an operator wants to change the network state to shift traffic off of this link. He could do so by updating the link weight. In doing so, the new shortest path from this top router would be as follows. But, what if the state in the top switch occurred before the state in the bottom switch could be updated? In this case, we would have a potential forwarding loop. Traffic would proceed to the bottom switch. But the bottom switch would still have the old network state and would continue to forward traffic to the top switch, resulting in a forwarding loop. If rules are installed along a path out of order, packets may reach a switch before the new rules do. So, in this type of model we would have to think about all possible packet and event orderings to ensure that consistent behavior resulted. So we need atomic updates of the entire configuration.\nThe solution to this problem is to use a two phase commit so that packets are either subjected to the old configuration on all switches, or to the new configuration on all switches. But packets aren\u0026rsquo;t subjected to the new policy on some switches and the old policy on others. The idea is to tag the packet on ingress so that the switches maintain copies of both P1 and P2 for some time. When all switches have received rules corresponding to the new policy, then incoming packets can be tagged with P2. After some time, when we\u0026rsquo;re sure that no more packets with P1 are being forwarded through the network, we can only then remove the rules corresponding to policy P1. Now, the naive version of two-phase commit, requires doing this on all switches at once, which essentially doubles the rule space requirements since we have to store the rules for both P1 and P2. We can limit the scope of the two phase commit by only applying this mechanism on switches that involve the affected portions of the traffic or the affected portions of the topology.\nInconsistent Policy Write Quiz So here\u0026rsquo;s a quick quiz. What types of problems can arise from inconsistent applications of writing policy? Inability to respond to failures, forwarding loops, a flood of traffic at the controller, or security policy violations?\nInconsistent Policy Write Solution Inconsistent writes can result in forwarding loops or security policy violations where traffic ends up going to parts of the network where it shouldn\u0026rsquo;t go as a result of inconsistent switch state. The ability to respond to failures is orthogonal to consistency. A flood of traffic at the controller technically involves problems with reading state in a consistent fashion. But since there also involves a step where the controller writes state to the switches while packets are still arriving at the controller, I would consider that answer to be correct as well.\nCoping With Inconsistency Quiz What are some approaches to coping with inconsistency? Running different controllers for different switches? Keeping a \u0026ldquo;hot spare\u0026rdquo; replica that has a complete view of the network state? Keeping the old and new state on the routers and switches and switching over only when all of the switches have received the new state? Or relying on the routers themselves to resolve the conflict?\nCoping With Inconsistency Solution In this case, there is only one correct answer, which is keeping the old and new state on the routers and switches. This is the two-phase commit approach that we talked about. Running different controllers for different switches could obviously result in an inconsistent state, since each of those controllers may be making independent decisions. Keeping a \u0026ldquo;hot spare\u0026rdquo; replica does no good if the replica also writes state inconsistently to the network. And resolving conflicts on the routers also doesn\u0026rsquo;t work because no router has a complete view of the network state.\nNetwork Virtualization Let\u0026rsquo;s now talk about an application of software defined networking, which is network virtualization. So we\u0026rsquo;ll talk first about what network virtualization is, then we\u0026rsquo;ll talk about how it\u0026rsquo;s implemented, and then we\u0026rsquo;ll talk about some examples and applications, such as Mininet.\nSo network virtualization is simply an abstraction of the physical network, where multiple logical networks can be run on the same underlying shared physical substrate. For example, a logical network might map a particular network topology onto the underlying physical topology. And there might be multiple logical networks that map onto the same physical topology. And these logical networks might actually share nodes and links in the underlying physical typology, but each logical network has its own view as if it were running its own private version of the network. Now you can see from this picture that the nodes in the physical network need to be shared or sliced. So the nodes in the physical topology might be virtual machines. Similarly, a single link in the logical topology might map to multiple links in the physical topology. The mechanism to achieve these virtual links is typically through tunneling. So a packet that\u0026rsquo;s destined from A to B in the logical topology, might be encapsulated in a packet that\u0026rsquo;s destined for node X first, before the packet is decapsulated and ultimately sent to B.\nIt may also be easy to understand virtual networking as an analogy to virtual machines, which you may be familiar with already. So in a virtual machine environment, we have virtual machines where a hypervisor arbitrates access to the underlying physical resources, providing to each virtual machine the illusion that it\u0026rsquo;s operating on its own dedicated version of the hardware. Similarly, with virtual networking, a network hypervisor of sorts arbitrates access to the underlying physical network to multiple virtual networks, providing the illusion that each virtual network actually has its own dedicated physical network.\nWhy Use Network Virtualization One of the main motivations for the rise of virtual networking was the \u0026ldquo;ossification\u0026rdquo; of the internet architecture. In particular because the internet protocol was so pervasive, it made it very difficult to make fundamental changes to the way the underlying internet architecture operated. There was a lot of work on overlay networks in the 2000\u0026rsquo;s, but one size fits all network architectures were very difficult to deploy. So rather than try to replace existing network architectures, network virtualization was intended to allow for easier evolution. In other words, network virtualization enables evolution because we didn\u0026rsquo;t have to pick a winner for a replacement for IP. We could instead let multiple architectures exist in parallel. Now, this was sort of a green field view of why virtual networking was potentially a good idea. In practice, network virtualization has really taken off in multi-tenant data centers where there may be multiple tenants or applications running on a shared cluster of servers. Well known examples of this include Amazon\u0026rsquo;s EC2, Rack Space, and things like Google App Engine. Large service providers such as Google, Yahoo, and so forth, also use network virtualization to adjust the resources that are devoted to any particular service at a given time.\nNetwork Virtualization Quiz So what are the motivations for network virtualization or virtual networks that we\u0026rsquo;ve discussed? Easier troubleshooting? Facilitating research and evolution by allowing coexistence of production networks with experimental ones? Better forwarding performance? And adjusting the resources of the network as demands change? Please check all that apply.\nNetwork Virtualization Solution As we discussed, virtual networks can facilitate research in evolution by allowing experimental networks to coexist with production networks. Because the networks are virtual, they can be scaled up and down, adjusting the resources that are devoted to any one particular service as demands change. We discuss this in the context of production networks, such as Google and Yahoo. Virtual networks are not inherently easier to troubleshoot, nor do they necessarily provide better forwarding performance. In fact, forwarding performance may be worse, due to the additional level of indirection that has been added.\nNetwork Virtualization Uses SDN Some of the promised benefits of Network Virtualization are more rapid innovation since innovation can proceed at the rate at which software evolves rather than on hardware development cycles, allowing for new forms of network control and potentially simplifying programming. It is important to make a distinction between Network Virtualization and Software-Defined Networking. Network Virtualization is arguably one of the first killer applications for SDN. And in some sense, SDN is a tool for implementing Network virtualization. But the two are not one and the same. Remember the defining tenant of SDN is the separation of the data and control plane, whereas the defining tenant of Network virtualization is to separate the underlying physical network from the logical networks that lie on top of it. So SDN can be used to simplify many aspects of Network virtualization. But it does not inherently obstruct the details of the underlying physical network.\nCharacteristics of Network Virtualization Quiz So which of the following are characteristics of network virtualization, but not necessarily characteristics of SDN? Be careful, the distinction between SDN and virtual networks was as we discussed in the previous part of this lesson. Allowing multiple tenants to share underlying physical infrastructure? Controlling behavior from a logically centralized controller? Separating logical and physical networks? Or separating data and control planes? Again, please feel free to check all that apply.\nCharacteristics of Network Virtualization Solution Network virtualization can allow multiple tenants to share the underlying physical infrastructure. And it also separates logical and physical networks. The other two options are defining characteristics of software defined networking, but not of network virtualization.\nDesign Goals for Network Virtualization So virtual networks have various design goals. It should be flexible, able to support different topologies, routing, and forwarding architectures, and independent configurations. They should be manageable. In other words, they should separate the policy that a network operator is trying to specify from the mechanisms of how those policies are implemented. They should be scalable, maximizing the number or coexisting virtual networks. They should be secure by isolating the different logical networks from one another. They should be programmable, and they should be heterogeneous in the sense that they should support different technologies.\nSo virtual networks have two components, nodes and edges. The physical nodes themselves must be virtualized. One possible way of virtualizing a node is a virtual machine. A more lightweight way of virtualizing a node is using a virtual environment such as a VServer or a Jail. The hypervisor, or whatever technology is enabling the virtual environment, can effectively slice the underlying physical hardware to provide the illusion of multiple guest nodes or multiple virtual nodes. Examples of node virtualization include virtual machine environments such as Xen or VMware, or what\u0026rsquo;s called OS level virtualization or virtual environments, such as Linux Vservers. Now, in a virtual network, we need to connect these virtual machines. Each virtual machine or virtual environment has its own view of the network stack. And we may want to provide the appearance that these nodes are connected to one another over a Layer two topology, even if they are in fact separated by multiple IP hops. One possible way of doing that is to encapsulate the Ethernet packet as it leaves the VM on the left in an IP packet. The IP packet can then be destined for the IP address of the machine on the right, and when the packet arrives at this machine, the host can decapsulate the packet and pass the original Ethernet packet to the VM or the virtual environment that\u0026rsquo;s residing on that physical node. Each one of these physical hosts may, in fact, host multiple virtual machines or virtual environments, which creates the need for a virtual switch that resides on a physical host. This virtual switch provides the function of networking virtual machines together over a virtual layer two topology. The Linux bridge is an example of a software switch that can perform this type of function. Open Vswitch is another example of software that performs this type of glue function. You can see more information about Open Vswitch on the URL provided here.\nVirtualization in Mininet The Mininet tool we have been using in the course is actually an example of network virtualization. We are in fact running an entire virtual network on your laptop. When you start Mininet using the MN script, each host in the virtual network is a bash process with its own network name space. A network name space is kind of like a virtual machine except it\u0026rsquo;s a lot more lightweight. It\u0026rsquo;s in fact called OS Level Virtualization. So, each one of these virtual nodes has its own view of the network stack as shown here with these interfaces. But it has a shared file system and it\u0026rsquo;s not, in fact, running its own independent virtual machine. The root namespace manages the communication between these distinct virtual nodes, as well as the switch that connects these nodes in the topology that you set up. Virtual Ethernet pairs are assigned two name spaces. For example, S1 eth1 is assigned to an interface in H2\u0026rsquo;s network name space. And S1 eth2 is assigned to a network name space in H3\u0026rsquo;s virtual network name space. The OpenFlow switch effectively performs forwarding between the interfaces in the root name space. But because the interfaces are paired, we get the illusion of sending traffic between h2 and h3. When we make modifications to the OpenFlow switch via the controller, we\u0026rsquo;re in fact doing that in the root name space.\nIn summary, virtual networks facilitate flexible, agile deployment, by enabling rapid innovation at the pace of software, vender independence, and scale. We talked about the distinction between SDN\u0026rsquo;s and virtual networks, as well as various technologies that enable virtual networks, such as virtual machines for creating virtual nodes and tunneling for creating virtual links.\nSDN Programming Difficulty In this lesson we\u0026rsquo;ll talk about the why and how of programming SDNs. Unfortunately, programming OpenFlow is not easy. It offers only a very low level of abstraction in the form of match action rules. The controller only sees events that switches don\u0026rsquo;t know how to handle, and there can be race conditions if switch level rules are not installed properly, as we\u0026rsquo;ve already seen in the lesson on consistent updates\nSDN Programming Interface The solution to this is to provide some kind of northbound API, which is a programming interface that allows applications and other kinds of orchestration systems to program the network. So where we have at the low-level the controller updating state in the switch using OpenFlow flow modification rules, we may have applications or orchestration systems that need to perform more sophisticated tasks, such as path computation, loop avoidance, and so forth. But we need a higher-level programming interface that allows these applications to talk to the controller so the application isn\u0026rsquo;t writing low-level OpenFlow rules, but rather is expressing what it wants to have happen in terms of higher-level behaviors without regard to such things as whether or not the rules are being installed in a consistent, and correct fashion. Various people may write these applications including network operators, service providers, researchers, and really anyone who wants to develop capabilities on top of OpenFlow. The benefits of such a northbound API are vendor independence, as well as the ability to quickly modify or customize control through various popular programming languages. The idea is that these applications might be written in high-level programming languages, such as Python, and wouldn\u0026rsquo;t actually have to perform low-level switch modifications, but rather could express policies in terms of much higher-level abstractions. Examples of such applications include the implementation of a large virtual switch abstraction, security applications, and services that may need to integrate traffic processing with middle boxes. This programmatic interface is called the northbound API and currently there\u0026rsquo;s no standard for the northbound API, as there is for the southbound API in OpenFlow. But we\u0026rsquo;ll look at various APIs in programming languages that each compile to OpenFlow rules that are installed on switches across the network.\nFrenetic Language One example of a programming language that sits on top of such a north-bound API is Frenetic, which is an SQL-like query language. For example, Frenetic would allow a programmer to count the number of bytes, grouped by destination Mac address, and report the updates to these counters every 60 seconds. The \u0026ldquo;group by\u0026rdquo; statement allows a grouping of counts by the destination mac address. \u0026ldquo;Where\u0026rdquo; allows restrictions to only count traffic coming from a web server coming in on a particular port, and\u0026rdquo; every\u0026rdquo; specifies that the results of this query should only be returned every 60 seconds. More information about Frenetic is available at frenetic- lang.org. And in the course, we\u0026rsquo;ll actually going to use a language called Pyretic that is based on the same underlying theory as Frenetic, except that it\u0026rsquo;s embedded in Python.\nOverlapping Network Policies One issue with programming at this higher level of abstraction is that an operator might write multiple modules, each of which effects the same traffic. For example, an operator might write an application that monitors traffic. Another one that specifies how routing should take place, another that involves the specification of firewall rules And yet another that balances traffic load across the links in the network. Ultimately, all of these applications, or modules, must be combined into a single set of OpenFlow rules that together achieve the network operator\u0026rsquo;s overall goal. For this, we need composition operators, or ways that specify how these individual modules should be combined or composed into a single coherent application. Let\u0026rsquo;s now talk about two different ways to compose policies.\nComposing Network Policies with Pyretic One way of composing policies is to perform both operations simultaneously. For example, one might want to forward traffic but also count how much traffic is being forwarded. Both of those operations can be performed in parallel. Another way of composing policies is in sequence. Sequential composition performs one operation then the next. For example, we might want to implement a firewall, and whatever traffic makes it though the firewall might then be subjected to the switching policy.\nOne example of sequential composition, might be a load balancer. In this example, a policy might take some traffic coming from half of the source IP addresses and rewrite that to one server replica, and take the other half of the traffic and rewrite it to the other replica. After the load balancer rewrites the destination IP address, we need a routing module to forward the traffic out the appropriate port on the switch. In this case, we\u0026rsquo;ve used sequential composition to first apply a load balance policy that rewrites the destination IP address based on the source IP address where the traffic is coming from and sequentially apply a routing policy that forwards the traffic out the appropriate port depending on the resulting destination IP address after that rewrite has taken place. Notice that we can use predicates to specify which traffic traverses which modules. Those predicates can apply specific actions based on things like the input port and the packet header fields. The ability to compose policies in this fashion allows each module to partially specify functionality without having to write the policy for the entire network. This leaves some flexibility so that one module can implement a small bit of the network function, leaving some functions for other modules. This also allows for module re-use, since a module need not be tied to a particular network setting. For example, in this particular example where we\u0026rsquo;ve applied that load balancer followed by routing, the load balancer spreads traffic across the replicas without regard to the underlying network paths that traffic takes once those destination IP addresses are rewritten.\nIn summary, we\u0026rsquo;ve covered two concepts. One is the notion of a Northbound API, which sits on top of an SDN controller and provides and exposes higher level abstractions that allows the operator or programmer to write policies without regard to how OpenFlow rules eventually get installed. We\u0026rsquo;ve also talked about two different composition operators. Parallel composition and sequential composition, which specify how individual simpler policies can be composed to implement more complex network applications, thus allowing different SDN control programs to independently perform tasks on the same traffic.\nPyretic Language In this lesson we will look at Pyretic which is an SDN language and run time that implements some of the composition operators that we discussed in the last lesson. The language is a way of expressing these high level policies, and the run time provides the function of compiling these policies to the OpenFlow rules that eventually are installed on the switches. One key abstraction in Pyretic is the notion of located packets, the idea that we can apply a policy based on a packet and its location in the network, such as the switch at which that packet is located or the port on which that packet arrives.\nPyretic offers several features. The first is the ability to take as input a packet and then return packets at different locations in the network. This effectively allows the implementation of network policy as a function that simply takes packets and returns other packets at different locations. The second feature of Pyretic is the notion of Boolean predicates. Unlike OpenFlow rules which do not permit the expression of simple conjunctions such as AND and OR, or negations like NOT, Pyretic allows the expressions of policies in terms of these predicates. Pyretic also provides the notion of virtual package header fields. Which allows the programmer to refer to packet locations and also to tag packets so that specific functions can be applied at different portions of the program. Pyretic also provides composition operators, such as parallel and sequential composition, which we discussed in the last lesson. The notion of network policy as a function contrasts with the OpenFlow style of programming. In OpenFlow, policies are simply bit patterns, in other words, match statements for which matching packets are subject to a particular action. These types of policies can be particularly difficult to reason about. In contrast, in Pyretic, policies are functions that map packets to other packets. Some example functions in Pyretic include the identify function, which returns the original packet; none or drop, which returns the empty set; match, which returns the identity if the field f matches the value v and returns none or drop otherwise; mod, which returns the same packet with the field f set to v; forward, which is simply syntactic sugar on mod to say that the output port field in the packet should be modified to the parameter specified; and flood, which returns one packet for each port on the network spanning tree. In OpenFlow, packets either match on a rule or they simply fall through to the next rule. So, OR, NOT, etc, can be tough to reason about. In contrast, Pyretic\u0026rsquo;s match function outputs either the packet or nothing, depending on whether the predicate is satisfied. For example, we could apply a match statement that says match destination IP equals 10.0.0.3. and this function would take packets as input and only return packets that satisfy this particular predicate. In addition to the standard packet header fields, Pyretic offers the notion of virtual packet header fields, which is a unified way of representing packet metadata. In Pyretic, the packet is nothing more than a dictionary that maps a field name such as the destination IP address to a value. Now, these field names could correspond to fields in an actual packet header. But they can also be virtual. For example, we could provide a match statement based on a switch, indicating that we only want to return packets that are located at a particular switch or on the input port, indicating that we only want to see packets whose attributes match a particular input port. The match function matches on this packet meta-data and the mod function can modify this meta-data.\nComposing Network Policies with Pyretic Pyretic enables the notion of both sequential and parallel composition as we\u0026rsquo;ve discussed in previous lessons. For example, we could match all packets for a particular destination IP address and send them or forward them out a particular output port. The double greater than sign shown here is the way of expressing sequential composition in Pyretic. Parallel composition allows two policies to be applied in parallel. In this example, we match on a particular destination IP address and subsequently forward the traffic out Output Port one. In Parallel, we apply a different set of policies that match on a different source IP address and output the packets on a different output port. In Pyretic, the plus operator performs parallel composition of policies.\nPyretic allows an operator to construct queries which allow the program to see packet streams. For example, the packets query allows the operator to see packets arriving at a particular switch with a particular source MAC address. The one parameter here indicates that we only want to see the first packet that arrives with a unique source MAC address and switch. We can then register callbacks for these packet streams that are invoked to handle each new packet that arrives for that query.\nDynamic Policies in Pyretic Dynamic policies are policies whose forwarding behavior can change. They are represented as a time series of static policies. The current value of the policy at any time is self dot policy. A common programming idiom in Pyretic is to set a default policy and then register a call back that updates that policy. In the assignment, you will create a similar topology that you created in the pox assignment, but we will now use pyretic to implement a simple switch and firewall. In pyretic every first packet with a new source MAC address at the switch is read by a query. The policy is updated with a new predicate every time a new mapping between a MAC address and an output port is learnt. In the assignment, you also create a dynamic firewall policy, register a callback to check the rules, and sequentially compose your firewall policy with a learning switch, thus provided as part of the pyritic distribution.\nIn summary, pyretic allows operators to write complex network policies as functions. It allows an operator to express predicates on packets including things such as AND, OR, and NOT. It provides the capability to specify and modify virtual packet headers as packet metadata, and it provides ways to compose complex network policies from simpler independent ones.\nPyretic Policy Quiz As a quiz, which of the following is the appropriate pyretic rule for sending traffic from source IP address 10.0.0.1 to destination at IP address 10.1.2.3, and traffic from source IP address 10.0.0.2 to destination IP address 10.2.3.4\nPyretic Policy Solution The following policy matches the appropriate source IP addresses and forwards to the corresponding output destination IP address. Each of these matching and forwarding operations can happen in parallel.\n"
},
{
	"uri": "/6250/traffic-engineering/",
	"title": "Traffic Engineering",
	"tags": [],
	"description": "",
	"content": " Traffic Engineering We just completed our exploration of software defined networking, and now we\u0026rsquo;re jumping into traffic engineering. Traffic engineering is a fancy way of describing how network operators deal with large amounts of data flowing through their networks. Companies like Google are doing exciting work in this area, so be sure to pay attention to the instructor note section, where we\u0026rsquo;ll highlight exciting research papers.\nTraffic Engineering Overview This lesson covers traffic engineering. Traffic engineering is the process of reconfiguring the network in response to changing traffic loads to achieve some operational goal. A network operator might want to reconfigure the network in response to changing loads to, for example, maintain traffic ratios in a peering relationship, or to relieve congestion on certain links in the network, or to balance load more evenly across the available links in the network. In this lesson we will explore different ways of performing traffic engineering. We will start by looking at conventional approaches to traffic engineering and we\u0026rsquo;ll explore how software-defined networking is used to make the process of traffic engineering easier in both data center networks and transit networks.\nIP networks must be managed. Now, in some sense, IP networks manage themselves. There are several examples of protocols on the internet that in some sense manage themselves. TCP senders send less traffic during congestion and routing protocols will adapt to topology changes. The problem is that even though these protocols are designed to adapt to various changes, the network may not run efficiently. There may be congested links when idle paths exist or there might be a high-delay path that some traffic is taking when a low-delay path, in fact, exists. So a key question that traffic engineering tries to address is how routing should adapt to traffic. In particular, traffic engineering attempts to avoid congested links and satisfy certain application requirements such as delay. These are, in some sense, the essential questions that traffic engineering attempts to answer.\nIn the rest of this lesson we will look at how network operators can tune a routing protocol configuration to affect how network traffic traverses the links in the network. And in particular we will look at both intradomain traffic engineering, that is how to reconfigure the protocols within a single autonomous system to adjust traffic flows, as well as interdomain traffic engineering, or how to adjust how traffic flows between autonomous systems. We will also look at multi-path routing and how it is used to achieve various traffic engineering goals. Let\u0026rsquo;s start by looking at how network operators can tune static link weights in a routing protocol like OSPF or ISIS, to affect how traffic flows through the network.\nIntradomain Traffic Engineering Let\u0026rsquo;s assume that we have a single autonomous system with static link weights as shown. In such a setup, routers will flood information to one another to learn the network topology, including the link weights on links connecting individual routers. An operator can affect how traffic flows through the network by configuring the link weights. By changing the link weight configuration, an operator can affect the shortest path between two points in this graph, thus affecting the way that traffic flows. In the link weight settings shown here, traffic would flow along the green path.\nSuppose that the operator would like to shift traffic off of a congested link in the middle of the network such as this one by changing the link weight from one to three. The shortest path between this node and this node, now takes an alternate route. So we can see from this simple example, that by adjusting the link weights in an intra-domain topology, the operator can affect how traffic flows between different points in the network, thus affecting the load on the network links. In practice, network operators set these link weights in a variety of ways. One could set the link weights inversely proportional to capacity, proportional to propagation delay, or the operator might perform some network wide optimization based on traffic.\nMeasuring, Modeling and Controlling Traffic Traffic engineering has three steps: measuring the network to figure out the current traffic loads, forming a model of how configuration affects the underlying paths in the network, and then ultimately reconfiguring the network to exert control over how the traffic flows through the network. An operator might measure the topology and traffic, feed the topology and traffic to a what-if model that predicts what will happen under various types of configuration changes, decide which changes to affect on the network, and then, ultimately, control the network behavior by readjusting link weights. So, in summary, we have measurement, modeling and control. Each of these three components requires a significant amount of heavy lifting to make both practical and accurate in practice.\nIntradomain traffic engineering attempts to solve an optimization problem where the input is the graph G, where R is the set of routers, and L is the set of unidirectional links. Each link L also has a fixed capacity. Another input is the traffic matrix ,or offered traffic load, where M_ij represents the traffic load from router i to router j. The output of the optimization problem is a set of link weights, where w_l is the weight on any unidirectional link l in the network topology. Ultimately, the setting of these link weights should result in a fraction of the traffic from i to j, traversing each link l, such that those fractions satisfy the network-wide objective function. Defining an objective function is tricky. We could talk about, for example, minimizing the maximum congested link in the network, evenly splitting traffic loads across links, and so forth.\nLink Utilization Function What we\u0026rsquo;d like to represent is that the cost of congestion increases in a quadratic manner as the loads on the links continue to increase, ultimately becoming increasingly expensive as link utilization approaches one. Solving the optimization problem, however, is much easier if we use a piecewise linear cost function. We can define utilization as the amount of traffic on the link divided by the capacity and our objective might be to minimize the sum of this piecewise linear cost function over all the links in the network. Unfortunately, solving this optimization is still NP-complete, which means that there is no efficient algorithm to find the optimal setting of link weights, even for simple objective functions.\nThe implications of this are that we have to resort to searching through a large set of combinations of link weight settings to ultimately find a good setting. So clearly searching through all the link weights is suboptimal, but the graphs turn out to be small enough in practice such that this approach is still reasonably effective. In practice, we also have other operational realities to worry about. For example, minimizing the number of changes to the network. Often just changing one or two link weights is enough to achieve a good traffic load balance solution. Whatever solution we come up with must be resistant to failure and it should be robust to measurement noise. We also want to limit the frequency of changes that we make to the network. This completes our overview of intradomain routing.\nAnd now we will take a look at interdomain routing. Intradomain routing and traffic engineering concerns traffic flow within a single domain, such as an ISP, a campus network, or a data center. In contrast, interdomain routing and interdomain traffic engineering concerns routing that occurs between domains, something that we looked at before in the context of the Border Gateway Protocol.\nInterdomain Routing Quiz Before we proceed with our discussion of interdomain traffic engineering, let\u0026rsquo;s take a brief quiz reminding ourselves about the differences between intradomain routing and interdomain routing.\nWhich of the following are examples of interdomain routing? Peering between two internet service providers? Peering between a university network, and its ISP? Peering at an internet exchange point? Routing inside a data center? Or routing across multiple data centers? Please, again, check all that apply.\nInterdomain Routing Quiz Answer Peering between two ISPs involves interdomain routing, as does peering between a university network and its ISP, or any peering at an Internet exchange point, for that matter. Routing between multiple data centers typically involves routing in a wide area, and hence may involve interdomain routing. Routing within a single data center concerns routing in a single domain and hence does not concern interdomain routing.\nBGP in Interdomain Traffic Engineering Inter-domain routing concerns routing between domains or autonomous systems. It involves the reconfiguration of the border gateway protocol, policies or configurations that are running on individual routers in the network. Changing BGP policies at these edge routers can cause routers inside an autonomous system to direct traffic to or away from certain edge links. We can also change the set of egress links for a particular destination. For example, an operator of autonomous system one might observe traffic to destination D traversing the green path.\nBut by adjusting BGP policies, the operator might balance load across these two edge links, or shift all of the traffic for that destination to the lower path.\nAn operator might wish to use inter domain traffic engineering if an edge link is congested, if a link is upgraded, or if there\u0026rsquo;s some violation of a peering agreement. For example, AS1 and AS have an agreement that they only send a certain amount of traffic load over that link in a particular time window. If the load exceeds that amount, an operator would need to use BGP to shift traffic from one egress link to another.\nInterdomain Traffic Engineering Goals Effective Inter-domain Traffic Engineering has three goals. One is predictability. In other words, it should be possible to predict how traffic flows will change in response to changes in the network configuration. Another goal is to, limit the influence of neighboring domains. In particular, we\u0026rsquo;d like to use BGP policies and changes to those policies that limit how neighboring ASes might change their behavior in response to changes to the PGP configuration that we make in our own network. And finally, we\u0026rsquo;d like to reduce the overhead of routing changes by achieving our traffic engineering goals with changes to as few IP prefixes as possible. To understand the factors that confound predictability Let\u0026rsquo;s look at how the inter-domain routing choices of a particular autonomous system can wreak havoc on predictability.\nLet\u0026rsquo;s suppose that a downstream neighbor is trying to reach the autonomous system at the top of this figure. The AS here might wish to relieve congestion on a particular peering link. To do so, this AS might now send traffic to that destination out a different set of autonomous systems. But once this AS makes that change, note that it\u0026rsquo;s choosing a longer AS path. Now taking a path of three hops rather than two. In response, the downstream neighbor might decide not to send its traffic for that destination through this autonomous system at all, thus affecting the traffic matrix that this AS sees. So, all the work that went into optimizing the traffic load balance for this AS is for naught, because the change that it made effectively changed the offered traffic loads and hence the traffic matrix. One way to avoid this type of problem and achieve predictable traffic flow changes is to avoid making changes like this that are globally visible. In particular, note that this change caused a change in the AS path link of the advertisement to this particular destination from two to three. Thus, other neighbors, such as the downstream neighbor here, might decide to use an alternate path as a result of that globally visible routing change. By avoiding these types of globally visible changes, we can achieve predictability. Another way to achieve effective interdomain traffic engineering is to limit the influence of neighbors. For example, an autonomous system might try to make a path look longer with AS path prepending. If we consider treating paths that have almost the same AS path length as a common group, we can achieve additional flexibility. Additionally, if we enforce a constraint that our neighbors should advertise consistent BGP route advertisements over multiple peering links (should multiple appearing links exist), ghat gives us additional flexibility to send traffic over different egress points to the same autonomous system, enforcing egress points to the same autonomous system. Enforcing consistent advertisements turns out to be difficult in practice, but it is doable. To reduce the overhead of routing changes, we can group related prefixes. Rather than exploring all combinations of prefixes to move a particular volume of traffic, we can identify routing choices that group routes that have the same AS paths, and we can move groups of prefixes according to these groups of prefixes that share an AS path. This allows us to move groups of prefixes by making tweaks to local preference on regular expressions on AS path. We can also focus on the small fraction of prefixes that carry the majority of traffic. Ten percent of origin AS is responsible for about 82 percent of outbound traffic. Therefore we can achieve significant gains in rebalancing traffic in the network by focusing on the heavy hitters.\nIn summary, to achieve predictability, we effect changes that are not globally visible. To limit the influence of neighbors, we enforce consistent advertisements and limit the influence of AS pass length. And to reduce the overhead of routing changes, we group pre fixed according to those that have common AS paths and move traffic in terms of groups of prefixes.\nMultipath Routing Another way to perform traffic engineering is with Multipath Routing, where an operator can establish multiple paths in advance. This approach applies both to intradomain routing and interdomain routing. The way this is done in intradomain routing is to set link weights such that multiple paths of equal cost exist between two nodes in the graph. This approach is called equal cost multipath.\nThus traffic will be split across paths that have equal costs through the network. A source router might also be able to change the fraction of traffic that\u0026rsquo;s sent along each one of these paths. Sending, for example, 35% along the top path and 65% along the bottom path.\nIt might even be able to do this based on the level of congestion that\u0026rsquo;s observed along these paths. The way that the router would do this is by having multiple forwarding table entries with different next hops for outgoing packets to the same destination.\nSource Router Path Quiz So quickly, how can a source router adjust paths to a destination when there are multiple paths to the destination? Dropping packets to cause TCB backoff, alternating between multiple forwarding table entries to the same destination, or sending alerts to incoming senders whenever a route changes?\nSource Router Path Quiz Answer A source router can adjust traffic over multiple paths by having multiple forwarding table entries for the same destination and splitting traffic flows across the multiple next hops depending on, for example, the hash of the IP packet header.\nData Center Networking Let\u0026rsquo;s now talk about data center networking and how network operators perform traffic engineering inside a data center. First of all, what characterizes a data center? Data center network has 3 important characteristics. One is multi-tenancy. Multi-tenancy allows a data center provider to advertise the cost of shared infrastructure, but because there are multiple independent users, the infrastructure must also provide some level of security and resource isolation. Data Center network resources are also elastic, meaning that as demand for service fluctuates, the operator can expand and contract these resources. It also can be pay per use, meaning that as the need to use more resources arises or disappears, a service provider can adjust how much resources are devoted to the particular service running in the data center. Another characteristic of data center networking is flexible service management, or the ability to move work, or workloads, to other locations inside the data center. For example, as load changes for a particular service, an operator may need to provision additional virtual machines, or servers to handle the load for that service or potentially move it to a completely different set of servers inside the infrastructure. This workload movement and virtual machine migration essentially creates the need for traffic engineering solutions inside a data center. A key enabling technology in data center networking is the ability to virtualize servers. This makes it possible to quickly provision, move, and migrate servers and services in response to fluctuations in workload. But while provisioning servers and moving them is relatively easy, we must also develop traffic engineering solutions that allow the network to reconfigure in response to changing workloads and migrating services.\nData Center Networking Challenges Some of the challenges for data center networking include traffic load balance, support for migrating virtual machines in response to changing demands, adjusting server and traffic placement to save power, provisioning the network when demands fluctuate, and providing various security guarantees, particularly in scenarios that involve multiple tenants. To understand these challenges in a bit more detail, let\u0026rsquo;s take a look at a typical data center topology.\nA topology typically has three layers: an access layer, which connects the servers themselves. An aggregation layer which connects the access layer, and then the core. Historically, the core of the network has been connected with layer three, but increasingly, modern data centers are connected with an entire layer-two topology. A layer-two topology makes it easier to perform migration of services from one part of the topology to another, since these services can stay on the same layer-two network and hence would not need new IP addresses when they moved. It also becomes easier to load balance traffic. On the other hand, a monolithic layer-two topology makes scaling difficult, since now we have tens of thousands of servers on a single flat topology. In other words, layer-two addresses are not topological. So the forwarding tables in these switches can\u0026rsquo;t scale as easily, because they can\u0026rsquo;t take advantage of the natural hierarchy that exists in the topology. Other problems that exist in this type of topology is that the hierarchy can potentially create single points of failure, and links at the top of the topology, in the core, can become oversubscribed. Modern data center operators have observed that as you move from the bottom of the hierarchy up towards the core, that the links at the top can carry as much as 200 times as much traffic as the links at the bottom of the hierarchy. So there\u0026rsquo;s a serious capacity mismatch in that the top part of the topology has to carry a whole lot more traffic than the bottom. We\u0026rsquo;ll explore how modern data center network architectures address these various challenges, but let\u0026rsquo;s first take a quick look at one way of solving the scale problem.\nData Center Topologies Recall that the Scale problem arises because we have tens of thousands of servers on a flat layer two topology, where all of the servers have a topology independent MAC or hardware address and thus, in the default case every switch in the topology has to store a forwarding table entry for every single MAC address. One solution is to introduce what are called pods and assign pseudo MAC addresses to each server corresponding to the pod in which they\u0026rsquo;re location is in the Topology. So in addition to having a real MAC address, each server has what\u0026rsquo;s known as a pseudo-MAC address, as shown in pink. Thus, switches in the Data Centre Topology no longer need to maintain forwarding table entries for every host. They only need to maintain entries for reaching other pods in the Topology. Once a frame answers a pod, the switch then of course has entries for all of the servers inside that pod but they don\u0026rsquo;t need to maintain entries for the MAC addresses for servers outside of each pod. For example, the switch in pod one only needs to maintain entries for these two servers with MAC addresses A and B, but it doesn\u0026rsquo;t need to maintain independent entries with servers with MAC addresses C and D. It only needs to maintain an entry for how to reach pod 2. Likewise for pods 2 and pods 3. Now, in such a Data Centre Topology, of course, these hosts are unmodified so they\u0026rsquo;re still going to respond to things like ARP queries with their real MAC addresses. So we need a way of dealing with that, as well as a way of Mapping pseudo MAC addresses to real MAC addresses.\nThe solution is as follows. When a host such as server A issues an ARP query, that query is intercepted by the switch,. But instead of flooding that query, the switch intercepts the query and forwards it to an entity called the Fabric Manager. The Fabric Manager then responds with the pseudo-MAC corresponding to that IP address. Host A then sends the frame with the destination pseudo-MAC address, and switches in the Topology can forward that frame to the appropriate pod sorresponding to the pseudo MAC address of the destination server. Once the frame reaches the destination pod, let\u0026rsquo;s say in this case pod 3, the switch at the top of that pod can then map the pseudo MAC address back to the real MAC address. And the server that receives the frame receives an Ethernet frame with its real destination MAC address, so it knows that the Ethernet frame was intended for it. By intercepting our queries in this way and providing a Mapping between topological pseudo MAC addresses and real, physical MAC addresses, we can achieve hierarchical forwarding in a large Layer 2 Topology without having to modify any host software.\nData Center (Intradomain) Traffic Engineering In this lesson, we will look at how data center traffic engineering, through customized topologies and special mechanisms for load balance, can help reduce link utilization, reduce the number of hops to reach the edge of the data center, and make the data center network easier to maintain. We saw in the last lesson how existing data center topologies provide extremely limited server to server capacity because of the over subscription of the links at the top of the hierarchy. Additionally, as services continue to be migrated to different parts of the data center, resources can be fragmented, significantly lowering utilization. For example, if the service denoted by green is running mostly in one part of the data center, but there\u0026rsquo;s a little bit running on a virtual machine in another part of the data center, this might cause traffic to traverse links of the data center topology hierarchy, thus significantly lowering utilization and cost efficiency. Reducing this type of fragmentation can result in complicated layer two or layer three routing reconfiguration. What we\u0026rsquo;d like to have is just the abstraction of one large layer to switch. This is the abstraction that VL2 provides. So, VL2 has two main objectives. One is to achieve layer- two semantics across the entire data center topology. This is done with a name-location separation and a resolution service that resembles the fabric manager which we talked about in the last lesson and which is described in more detail in the paper. To achieve uniform high capacity between the servers and balance load across links in topology, VL2 relies on flow-based random traffic interaction using valiant load balancing. Let\u0026rsquo;s take a closer look at how that load balancing works.\nValiant Load Balancing The goals of Valiant load balancing in the VL2 network are to spread traffic evenly across the servers, and to ensure that traffic load is balanced independently of the destinations of the traffic flows. Field two achieves this by inserting an indirection level into the switching hierarchy. When a switch at the access layer wants to send traffic to a destination, it first selects a switch at the indirection level to send the traffic at random. This intermediate switch then forwards the traffic to the ultimate destination depending on the destination MAC address of the traffic. Subsequent flows might pick different indirection points for the traffic, at random. This notion of picking a random indirection point to balance traffic more evenly across a topology, actually comes from multi-processor architectures and has been rediscovered in the context of data centers. So, in this lesson we have explored how valiant load balancing can be used on a slightly modified topology to achieve better load balance than in traditional fat tree networks, without an indirection layer and valiant load balancing. In the next lesson we\u0026rsquo;ll look at how a custom random topology can make some of these traffic engineering problems even easier.\nJellyfish Data Center Topology In this lesson we\u0026rsquo;ll look at Jellyfish, a technique to network data centers randomly. The goals of Jellyfish are to achieve high throughput to support, for example, big data analytics or agile placement of virtual machines and incremental expandability, so that network operators can easily add or replace servers and switches. For example, large companies like Facebook are adding capacity on a daily basis. Commercial products make it easy to expand or provision servers in response to changing traffic load but not the network. Unfortunately, the structure of the data center networks constrains expansion. Structures such as a hypercube require two to the K switches, where K is the number of servers. Even more efficient topologies, like a FAT tree, are still quadratic in the number of servers.\nData Center Topology Quiz By looking at this figure showing a data center topology where the servers are at the edge of the graph, can you try to figure out where data center topology primarily constrains expansion? Is it at individual servers, aggregation switches or top-level switches?\nData Center Topology Quiz Answer As we can see from the figure, most of the congestion occurs at the top level. Jellyfish\u0026rsquo;s answer to how data structure constrains expansion is to simply have no structure at all.\nJellyfish Random Regular Graph Jellyfish\u0026rsquo;s topology is what is called a random regular graph. It\u0026rsquo;s random because each graph is uniformly selected at random from the set of all regular graphs. A regular graph is simply one where each node has the same degree. And a graph in Jellyfish is one where the switches in the topology are the nodes. In contrast to the earlier data center topology diagram we saw, here is a picture of a Jellyfish random graph with 432 servers and 180 switches. Every node in this graph has a fixed degree of 12.\nJellyfish\u0026rsquo;s approach is to construct a random graph at the Top-of-Rack switch layer. Every Top- of-Rack switch i, has some total number of K_i ports, of which it uses R_i to connect to other Top-of-Rack switches. The remaining K_i minus R_i ports are used to connect servers. With N racks, the network then supports N times K_i minus R_i servers. And the network is a random regular graph denoted as follows. Formally, random regular graphs are sampled uniformly from the space of all R regular graphs. Achieving such a property is a complex graph theory problem, but there\u0026rsquo;s a simple procedure that produces a sufficiently uniform random graph that empirically have the desired properties.\nConstructing a Jellyfish Topology To construct a jellyfish topology, one can simply take the following steps. First, pick a random switch pair with free ports for which the switch pair are not already neighbors. Next, join them with a link, and repeat this process until no further links can be added. If a switch remains with greater than or equal to two free ports, which might happen during the incremental expansion by adding a new switch, these switches can be incorporated in the topology by removing a uniform random existing link and adding links to that switch.\nFor a particular equipment cost, using identical equipment, the jelly fish topology can achieve increased capacity by supporting twenty five percent more servers. This higher capacity is achieved because the paths through the topology are shorter than they would be in a Fat tree topology.\nConsider a topology with sixteen servers, twenty switches, and a fixed degree of four for both the fat tree topology and the jellyfish random graph. In the fat tree topology, only four of 16 servers are reachable in less than five hops. In contrast, in the jellyfish random graph, there are 12 servers reachable. By making more servers reachable along shorter paths, jellyfish can increase capacity over a conventional Fat tree topology.\nSo while Jellyfish shows some promise, there are certainly some open questions. First, how close are these random graphs to optimal in terms of the optimal throughput that could be achieved for a particular set of equipment? Second, what about typologies where switches are heterogeneous with different numbers of ports or link speeds? From a system design perspective, the random topology model could create problems with physically cabling the datacenter network, and there are also questions about how to perform routing or congestion control without the structure of a conventional datacenter network like a fat tree.\n"
},
{
	"uri": "/6250/network-security/",
	"title": "Network Security",
	"tags": [],
	"description": "",
	"content": " Networking Security We\u0026rsquo;re on our last stretch of the course with network security. Network security is an extremely important topic, especially considering high-profile data loss from large companies and government organizations.\nTo accompany this section, you\u0026rsquo;ll be building a program in Pyretic that mitigates one of the specific attacks that we\u0026rsquo;re going to look at in this course.\nNeed for Network Security We are beginning a lesson on network security. Lets first talk about why we need network security in the first place. The Internet is actually subject to a wide variety of attacks on various parts of the infrastructure. One part of the infrastructure that can be attacked is routing. So the internet\u0026rsquo;s routing protocol, the border gateway protocol, is notorious for being susceptible to different kinds of attacks. For example, on April 8, 2010, China advertised about 50,000 blocks of IP address from 170 different countries. The event lasted for about 20 minutes. In this particular case, the hijack appears to have been accidental because the prefixes were long enough such that they didn\u0026rsquo;t disrupt existing routes. But the fact that the route advertisements were allowed to leak in the first place highlights the vulnerability of the border gateway protocol. Effectively, the border gateway protocol essentially allows any AS to advertise an IP prefix to a neighboring AS, and that AS will typically just believe that route advertisement and advertise it to the rest of the internet. These events that occur where an AS advertises a prefix that it does not own are called route highjacks. And they tend to occur more often than one might expect. In addition to the event on April 8, 2010, another event in 2008 occurred when Pakistan higjacked YouTube prefixes, potentially as a botched attempt to block Youtube in the country following a government order. Unfortunately, the event resulted in disruption of connectivity to YouTube for people all around the world. In January of 2006 ConEdison accidentally hijacked a lot of transit networks, including level three Unet and several other large ISPs disrupting connectivity to many customers. And on April 25th in 1995, one of the more famous route hijack incidents was the AS7007 incident, where AS7007 advertised all of the IP prefixes on the entire internet as originating in its own AS, resulting in disruption of connectivity to huge fractions of the Internet. So we\u0026rsquo;ve surveyed some famous or, shall we say, notorious attacks on Internet routing, but another part of the infrastructure that\u0026rsquo;s vulnerable is naming or the DNS.\nOne very popular and effective means of mounting an attack on the naming system is through something called reflection. DNS reflection is a way of generating very large amounts of traffic targeted at a victim in an attack called Distributed Denial of Service, or DDoS attack. Another type of attack on the naming system is Phishing, whereby an attacker exploits the domain name system in an attempt to trick a user into revealing personal information, such as passwords on a rogue website. In general, denial of service attacks are extremely common and can be mounted in a variety of different ways. DNS reflection is just one way that distributed denial of service attacks are mounted. We\u0026rsquo;ll explore some others later on in this lesson. It\u0026rsquo;s worth asking why the internet is so vulnerable to different kinds of attacks.\nInternet is Insecure As it turns out, the internet\u0026rsquo;s design is actually fundamentally insecure. Many explicit design choices have caused the internet to be vulnerable to different types of attacks. The internet was designed for simplicity, and as a result security was not a primary consideration when the internet was originally designed. Another aspect of the internet\u0026rsquo;s design is that it\u0026rsquo;s on by default. In other words, when a host is connected to the internet, it is by default reachable by any other host that has a public IP address. This means that if one has an insecure host, that host is effectively wide open to attack by other hosts on the internet. Now, this wasn\u0026rsquo;t a primary design consideration when the internet consisted of a small number of trusted networks, but as the internet has continued to grow, this on-by-default design, or the notion that any host should always be reachable by any other host, has come under fire. Part of the reason that they\u0026rsquo;re on-by- default model does not work that well is that hosts are insecure. This makes it possible for a remote attacker to compromise a machine that\u0026rsquo;s connected to the internet and commandeer it for the purposes of attack. In many cases, an attack might actually just look like normal traffic. For example, in the case of an attack on a victim web server, every individual request to that web server might look normal, but the collection of requests together, mounted as part of a distributed denial of service attack, might add up to a volume of traffic that the server is unable to handle. Finally, the internet\u0026rsquo;s federated design obstructs cooperation for diagnosis or mitigation. In other words, because the internet is run by tens of thousands of independently run networks, it can be very difficult to coordinate a defense against an attack because each of these networks is run by different network operators, sometimes in completely different countries\nInternet Insecurity Quiz As a quick quiz, which of the following make the internet\u0026rsquo;s design fundamentally insecure? The On by default nature of the design? The fact that IP Addresses might be easy for an attacker to guess? That attacks can look like normal traffic? Or that the internet is actually a federation of tens of thousands of independently operated networks?\nInternet Insecurity Quiz Answer The fact that the Internet is on by default, that attacks can look like normal traffic, and that the Internet is in fact federated, collectively make it very difficult to design a secure Internet.\nResource Exhaustion Attacks Recall from an earlier lesson that one of the internet\u0026rsquo;s fundamental design tenants is packet switching. In a packet switch network, resources are not reserved and packets are self contained. Every packet has a destination IP address, and each packet travels independently to the destination host. In a packet switch network, a link may be shared by multiple senders at any given time, using statistical multiplexing as we learned in previous lessons. While packet switch networks have their advantages, in particular it makes it easy to achieve high utilization on a shared link, packet switch networks also have the drawback that a large number of senders can overload a network resource, such as a node or a link. Note that circuit switch networks like the phone network do not have this problem because every connection effectively has allocated, dedicated resources for that particular connection until it is terminated. So this problem that an attacker who sends a lot of traffic might exhaust resources is unique to a packet switched network environment. So packet switched networks are extremely vulnerable to resource exhaustion attacks. Resource exhaustion attacks a basic component of security known as availability.\nLet\u0026rsquo;s take a look at other components of security as well. In addition to availability, we would like the network to provide confidentiality. For example, if you\u0026rsquo;re performing a sensitive banking transaction or having a private conversation with a friend, you\u0026rsquo;d like the Internet to provide some level of confidentiality. Another component of security is authenticity. Authenticity ensures the identity of the origin of a piece of information. So, for example, if you\u0026rsquo;re reading a particular news article, you really may want to know that the article came from the New York Times website as oppose to from some other place on the internet. Similarly, you might want to know that that information wasn\u0026rsquo;t modified in flight. That property is called integrity, which prevents unauthorized changes to information as it traverses the network. Now a security threat is anything that might potentially cause a violation of one of these properties. An attack, on the other hand, is an action that results in the violation of one of these security properties. So the difference between a threat and an attack is simply the difference between a violation that could potentially occur versus an action that actually results in a violation. Let\u0026rsquo;s look at a couple example attacks on different components of security. Let\u0026rsquo;s start by looking at an attack on confidentiality.\nConfidentiality and Authenticity Attacks One attack on confidentiality is called eavesdropping, where an attacker, Eve, might gain unauthorized access to information being sent between Alice and Bob. So, for example, if Alice and Bob were chatting on instant message, or if Alice sends an email to Bob, the potential exists (in other words, there\u0026rsquo;s a threat) that Eve might be able to hear that communication. There are various packet sniffing tools, such as wireshark and tcpdump, that set a machine\u0026rsquo;s networking interface card into what\u0026rsquo;s called promiscuous mode. If Alice, Bob, and Eve are on the same local area network, where packets are being flooded (for example, if they were being connected by a hub that flooded all packets everywhere, or if the learning switch did not have an entry for Alice or Bob) then Eve might be able to hear some of those packets. If the network interface card is in promiscuous mode, then Eve\u0026rsquo;s machine will be able to capture some of the packets that are being exchanged between Alice and Bob. It\u0026rsquo;s worth thinking about how different types of traffic might reveal important information about communication. For example, the ability to see DNS look- ups would provide the attacker information about, say, what websites you\u0026rsquo;re visiting. The ability to capture packet headers might give the attacker information not only about where you\u0026rsquo;re exchanging traffic, but what types of applications you\u0026rsquo;re using. And the ability to see a full packet payload would allow an attacker to effectively see every single thing that you are sending on the network including content you\u0026rsquo;re exchanging with other people, uch as private message, email communication, and so forth.\nGiven the ability to see a packet, Eve might not only listen to that packet, but might also modify it and re-inject it into the network, potentially after altering the state of the packet. If additionally Eve could suppress the original message, let\u0026rsquo;s consider an attack on authenticity. If, in addition to being able to observe packets that traverse the network, Eve could re-inject packets after having modified them and suppress Alice\u0026rsquo;s original message, then Eve could effectively impersonate Alice. This is sometimes called a \u0026lsquo;Man in the Middle\u0026rsquo; attack. Eve could also make it appear as though this message came from Alice, in which case the attack would be an attack on message integrity.\nNetwork Attack Quiz So we\u0026rsquo;ve considered attacks on availability, confidentiality, authenticity, and integrity. Let\u0026rsquo;s have a quick quiz, on these concepts. A denial of service is an attack on what property of internet security?\nNetwork Attack Quiz Answer A denial of service attack is an attack on availability. Denial of service attacks typically are an attempt to overwhelm the network or a network host in some way by consuming its resources. A common way of launching a denial of service attack is to send a lot of traffic at a victim, often from many distributed locations. If the attacker is in fact distributed, this is called not just a denial of service attack, but a distributed denial of service attack.\nNegative Impacts of Attacks These attacks can have serious negative effects, including theft of confidential information, unauthorized use of network bandwidth or computing resources, the spread of false information, and the disruption of legitimate services. All these types of attack are related. They are all very dangerous and sometimes they come hand in hand. For example, all these attacks are, in some sense, related to one another, and they can come hand in hand with one another as well.\nRouting Security Let\u0026rsquo;s now talk about internet routing security or problems involving securing the internet\u0026rsquo;s routing protocol. We will primarily focus on inter-domain routing or the security of BGP. We will further focus on control plane security which typically involves authentication of the messages being advertised by the routing protocol. In particular, the goal of control plane security, or control plane authentication, is to determine the veracity of routing advertisements. There are various aspects of the routing protocol that we seek to verify. One is session authentication, which protects the point-to-point communication between routers. A second type of control plane authentication is path authentication, which protects the AS path and sometimes other attributes. Another type of authentication is origin authentication, which protects the origin AS in the AS path, effectively guaranteeing that the origin AS that advertises a prefix is, in fact, the owner of that prefix.\nBGP Routing Security Quiz So as a quick quiz. From last lesson, we talked about route hijacks. A route hijack is an attack on which of the following three forms of authentication?\nBGP Routing Security Quiz Answer A route hijack is an attack on origin authentication because in a route hijack, the AS that is advertising the prefix is actually not the rightful owner of that prefix. In addition to control plan security, we also have to worry about data plan security or determining whether data is traveling to the intended locations. In general, it can be extremely hard to verify that packets or traffic is traveling along the intended route to the destination, or that it, in fact, even reaches the intended destination in the first place. Guaranteeing that traffic actually traverses the advertised route remains an important open problem in internet security. So how do these attacks on routing happen in the first place?\nRoute Attacks One possible explanation is simply that the router is misconfigured. In other words, no one actually intended for the router to advertise a false route, but because of a misconfiguration the router does so. The AS 7007 attack that we discussed last time was actually the result of a configuration error. Second, a router might be compromised by an attacker. Once a router is compromised, the attacker can reconfigure the router to, for example, advertise false routes. Finally, unscrupulous ISPs might also decide to advertise routes that they should not be advertising. To launch the attack, an attacker might reconfigure the router, which is typically the most common way an attacker might launch an attack. The attacker might also tamper with software, or an attacker could actively modify a routing message. In addition to tampering with the configuration, the attacker might tamper with the management software that changes the configuration. And the most common attack is a route hijack attack or an attack on origin authentication.\nRoute Hijacking Let\u0026rsquo;s talk about why hijacks matter. Let\u0026rsquo;s suppose that you would like to visit a particular website. To do so you first need to issue a DNS query. Now the authoritative DNS server for a particular domain might be located in a distant network. As we\u0026rsquo;ve discussed in previous lessons, the DNS uses a hierarchy to direct your query to the location of the authoritative name server, but ultimately that authoritative name server has an IP address, and you use the internet\u0026rsquo;s routing protocol, the border gateway protocol, to reach that IP address. What if an attacker were running a rogue DNS server and wanted to hijack your DNS query or to return a false IP address? Well, the attacker might use BGP to advertise a route for the IP prefix that contains that authoritative DNS server.\nAnd suddenly your DNS queries that were previously going to the legitimate server, are instead redirected to the rogue DNS server. So we might think of this as an attack whereby an attacker can use the BGP infrastructure to hijack a DNS query, and masquerade as a legitimate service. It can get even worse than this. Let\u0026rsquo;s now look at how a BGP route hijack can result in a Man in the Middle attack, whereby your traffic ultimately reaches the correct destination, but the attacker successfully inserts themselves on the path.\nThe problem with this particular route hijack is that all traffic destined for IP X is going to head for the attacker, even the traffic from the legitimate network. What we\u0026rsquo;d like to instead have happened is that traffic for IP X first goes to the hijack location and then goes to the legitimate location. So the attacker effectively becomes a Man in the Middle. The problem is that we need to somehow disrupt the routes to the rest of the internet while leaving the routes between the attacker and the legitimate location intact so that traffic along this path can still head towards the legitimate AS.\nRoute Hijacking (cont) Let\u0026rsquo;s suppose that AS200 originates a prefix and that the paths that result from the original BGP routing are shown in green. Let\u0026rsquo;s now suppose that AS100 seeks to become a man in the middle. If the original prefix being advertised was P, AS100 could also advertise the prefix P. But we want to make sure that AS100 maintains a path back to AS200. Now that path already exists, it\u0026rsquo;s right here. So what we want to do is make sure that neither AS 10 nor AS 20 accept this hijacked route. The way that we can do that is through a technique called AS-path poisoning. So, if AS 100 advertises a route that includes AS 10 and AS 20 in the AS path, both of these AS\u0026rsquo;s will drop the announcement because they will think they\u0026rsquo;ve already heard the announcement and don\u0026rsquo;t want to form a loop.\nOn the other hand, the other AS\u0026rsquo;s on the internet (in other words, every other AS that\u0026rsquo;s not on the path back from 100 to 200) will switch and now all of the traffic from other AS\u0026rsquo;s enroute to AS 200 will traverse the attacker AS100. Now a trace route might look awfully funny taking this circuitous route, but actually the attacker can hide its presence even if the sender is running a trace route. Recall that a trace route simply consists of ICMP time exceeded messages that result when a particular packet reaches a TTL of 0. Now typically each router along a path will decrement the TTL at each hop. But if the routers in the attacker\u0026rsquo;s network never decrement the TTL, then no time exceeded messages would be generated by routers in AS 100. Therefore the traceroute would never show AS on the path at all. So now that we\u0026rsquo;ve talked about the importance of origin authentication and attacks against it, let\u0026rsquo;s talk a little bit about session authentication.\nAutonomous System Session Authentication Session Authentication simply attempts to ensure that BGP Routing messages sent between routers between AS\u0026rsquo;s are authentic. Now, this turns out to be a little bit easier than it might appear, because the session between these routers is a TCP session. Therefore, all we have to do is authenticate this session. The way that this is done, in practice, is done using TCP\u0026rsquo;s MD authentication option. In such a setup, every message exchanged on the TCP connection not only contains the message, but also a hash of the message with a shared secret key. Now this key distribution is manual. The operator in AS1 and the operator in AS2, must agree on what the key is, and typically they do that out of band, for example, by calling each other on the phone and manually setting that key in the router configuration. But once that key is set, all messages between this pair of routers is authenticated.\nAnother way to guarantee session authentication, is to have AS1 transmit packets with a TTL of 255 and have the receiving AS drop any packet that has a TTL less than 254. Because most eBGP sessions are only a single hop and attackers are typically remote, it is not possible for the recipient AS to accept a packet from a remote attacker, because likely that attacker\u0026rsquo;s packets will have a TTL value of less than 254. This defense is aptly called the TTL hack defense for BGP Session Authentication.\nOrigin and Path Authentication Let\u0026rsquo;s return to the problem of guaranteeing origin and path authentication. To guarantee these properties there is a proposal to modify the existing border gateway protocol to add signatures to various parts of the route advertisement. This proposal is sometimes called Secure BGP or BGPSEC. The proposal has two different parts. The first part is an origin attestation, which is a certificate that binds the IP prefix to the organization that owns that prefix, including the origin AS. This is sometimes also called an address attestation. Now, this certificate must be signed by a trusted party. That trusted party might be, for example, a routing registry or the organization that allocated that prefix to that organization in the first place. The second part of BGPSEC is what\u0026rsquo;s called a path attestation, which are a set of signatures that accompany the AS path as it is advertised from one AS to the next. Let\u0026rsquo;s have a closer look at BGPSEC\u0026rsquo;s path attestation and the types of attacks that it can and cannot prevent.\nAutonomous System Path Attestation Let\u0026rsquo;s assume that we have a path with three ASes, one, two, and three, and that each AS has a public-private key pair. Let\u0026rsquo;s assume that we have a network with three ASes and that each AS along the path has a public-private key pair. An AS can sign a message or a route with its own private key, and any other AS can check that signature with the AS\u0026rsquo;s public key. So let\u0026rsquo;s suppose that AS1 advertises a route for prefix p. So that route would contain the prefix as well as an address attestation, which we\u0026rsquo;re not showing. Bbut let\u0026rsquo;s look at the path attestation. So, as usual, the BGP announcement would contain the prefix p, and the AS path, which so far is just 1. And, the path attestation, which is actually the path to 1 signed by the private key of AS1. When AS re-advertises that route announcement, it of course advertises the new AS path 2 1. It adds its own route attestation, 3 2 1, signed by its own private key, and it also includes the original path attestation signed by AS1. A recipient of a route along this path can thus verify every step of the AS path. AS3 can use the first part of the path attestation to verify that the path in fact, goes from AS2 to AS1, and does not contain any other AS\u0026rsquo;s in between.\nIt can use the second part of the path attestation to ensure that the path between it, AS3, and the next hop, is in fact, AS2, and that no other AS\u0026rsquo;s could\u0026rsquo;ve inserted themselves on the path between 2 and 3. This is precisely why the AS signs a path attestation with not only its own part of the AS path in the path attestation, but also, the hop of the AS that is intended to receive the BGP route advertisement.\nTo see the importance of this part of the path attestation, suppose, that these AS\u0026rsquo;s were not there in the path at station. In this case we have a very nice, well-formed BGP route advertisement for a prefix with the AS path suffix 2 1, and we have each segment signed. So an attacker could, in fact, take such an announcement and advertise sub strings of this route advertisement as their own. Thus an attacker, AS4, could claim that it was connected to prefix P via AS1 when in fact no such link existed simply by stealing or replacing the path attestation 1 that\u0026rsquo;s signed by K1.\nBut, note that in reality AS1 never generates this signature. In fact, it generates the signature, 21. Or in this case, it would somehow have to generate the signature 41 signed by AS1\u0026rsquo;s private key, whereas if AS1 only signed a message with its own AS in the message, such a segment or attestation could easily be replayed.\nThere\u0026rsquo;s actually no way that AS4 Could forge the path attestation for 1, signed by AS1\u0026rsquo;s private key because it doesn\u0026rsquo;t own this private key and AS1 never generated a path attestation with this particular signed path. This is the reason that each AS not only signs a path attestation with its own AS on the AS path, but also the next AS along the path. This particular mode of signing not only prevents the type of hijacking that we explored, but it also prevents path shortening attacks. For example, when AS4 receives the legitimate route to ASP through the path 3 2 1, it would be impossible for the AS to shorten that advertisement to say 3 because it would somehow have to generate a path attestation 4 3 1, signed by its own secret key. However, if it did that, the receiving AS would look for another path attestation with just 3 1 signed by AS3. Yet, such a path attestation would not actually exist. So, these path attestations can prevent against some kinds of hijacks (as we\u0026rsquo;ve seen), they can prevent against these path shortening attacks, and they can also prevent against modification of the AS path. However, there are certain attacks that path attestations cannot defend against. So, if an AS fails to advertise a route or a route withdrawal, there is no way for the path attestation or BGPSEC to prevent from that kind of attach. Certain types of replay attacks such as a premature re-advertisement of a withdrawn route also cannot be defended against and of course, there is no way to actually guarantee that the data traffic travels along the advertised AS path, which is a significant weakness of BGP that is yet to be solved by any routing protocol.\nDNS Security Let\u0026rsquo;s now talk about DNS security. To understand the threats and vulnerabilities of DNS, let\u0026rsquo;s take a look at the DNS architecture. So we have a stub resolver which issues a query to a caching resolver. At this point, we could have a man in the middle attack, or an attacker which observes a query and forges a response. If a query goes further than the local caching resolver, say for example to an authoritative name server, an attacker could try to send a reply back to that caching resolver before the real reply comes back to try to poison, or corrupt, the cache with bogus DNS records for a particular name. This attack is particularly virulent and we will look at a cache poisoning attack in this lecture. Masters and slaves can both be spoofed. Zone files could be corrupted. Updates to the dynamic update system could also be spoofed. We will look at some defenses to cache poisoning, including the OX20 defense, as well as DNSSEC, which can protect against some of these spoofing and man in the middle attacks. In addition to these attacks, we\u0026rsquo;ll look at an attack called DNS reflection where the DNS can be used to mount a large distributed denial of service attack.\nWhy is DNS Vulnerable So why is DNS vulnerable in the first place? So the fundamental reason is that the resolvers that issue the DNS query trust the responses that are received after they send out a query regardless of where that response comes from. So sometimes these responses can be forged. When a resolver sends out a query, it typically generates what\u0026rsquo;s called a race condition. And if the attacker replies before the legitimate responder, then the resolver is likely to believe the attacker. DNS responses can also contain additional DNS information that\u0026rsquo;s unrelated to the query. The fundamental problem is that the basic DNS protocols have no means for authenticating responses. This allows an attacker to forge responses after a resolver sends a query. A secondary reason that these types of spoofed replies are possible is that DNS queries are typically connectionless unlike BGP, where routing messages are transmitted over a reliable TCP connection. UDP queries are sent over a connectionless UDP connection. Therefore, a resolver does not have a way of mapping the response that it receives for a query other than the query ID, which can be forged by the attacker. Let\u0026rsquo;s look at how the combination of the lack of authentication and the connectionless nature of a DNS query allows the possibility of cash poisoning.\nDNS Vulnerability Quiz So as a quick quiz, which aspects of DNS make it vulnerable to attack? The fact that queries are sent over UDP? The fact that DNS names are human-readable? The fact that responses to DNS queries are not authenticated? Or, that the DNS is distributed or federated over many organizations?\nDNS Vulnerablitiy Quiz Answer As we discussed, the fact that the queries are sent over a connectionless channel and that there is no way to authenticate the query responses, makes the DNS vulnerable to various kinds of spoofing and cache poisoning attacks. The fact that DNS names are human readable does not make the DNS inherently insecure. Nor does the fact that it\u0026rsquo;s distributed. There are certainly very well understood ways of securing distributed systems and that does not inherently make DNS insecure.\nDNS Cache Poisoning To see how see how a DNS cache poisoning attack works, consider a network where a stub resolver issues a query to its recursive resolver, and the recursive resolver in turn sends that A record query to the start of authority for that domain. Now, in an ideal world, the authoritative name server for that domain would reply with the correct IP address. If an attacker guesses that a recursive resolver might eventually need to issue a query for say, http://www.google.com, the attacker can simply reply with multiple, specially crafted replies, each with different id\u0026rsquo;s. Although this query has some query id, the attacker doesn\u0026rsquo;t need to see that query because the attacker can simply flood the recursive resolver with a bunch of bogus replies, and one of them, in this case the response with id3, will match. As long as this bogus response reaches the recursive resolver before the legitimate response does, the recursive resolver will accept this bogus message, and worse, it caches the bogus message. And DNS, unfortunately, has no way to expunge a message once it has been cached. So now this recursive resolver will continue to send bogus A record responses for any query for this particular domain name until that entry expires from the cache. Now there\u0026rsquo;s several defenses against DNS cache poisoning, and we\u0026rsquo;ve already seen one, which is the query ID. But of course, the query ID can be guessed. The next defense is to randomize the ID. So rather than having a resolver send queries where the ID\u0026rsquo;s increment in sequence, the resolver can pick a random ID. This makes the ID tougher to guess, but still, the query ID is only 16 bits, which still makes it possible for an attacker to flood the recursive resolver with many possible responses. And, it\u0026rsquo;s likely that with relatively few responses, one of these bogus responses will match the ID for the real query. Due to the birthday paradox, the success probability for achieving a collision between the query ID of the query and of the response actually only requires sending hundreds of replies, not a complete 32,000. Due to the birthday paradox, the probability that such an attack will succeed, using only a few hundreds of replies, is relatively close to one. The attacker does not need to send replies with all two to the 16th possible IDs. The success of a DNS cache poisoning attack not only depends on the ability to reply to a query with a correct matching ID, but it also depends on winning this race. That is, the attacker must reply to that query before the legitimate authoritative name server replies. If the bad guy, or the attacker, loses the race, then the attacker has to wait for that correct cached entry to expire before trying again. However, the attacker can generate his own DNS query. For example, he could query one.google.com, two.google.com and so forth. Each one of these bogus queries will generate a new race. And eventually the attacker will win one of these races for an A record query. But who cares? Nobody necessarily cares to own one.google.com, or two.google.com. The attacker really wants to own the entire zone. Well the trick here is that instead of just simply responding with A records in the bogus replies, the attacker can also respond with NS records for the entire zone of google.com. So by creating one of these races using an A record query, and then responding not only with the A record response, but also with the authoritative of the NS record for the entire zone, the attacker can in fact own the entire zone. This idea of generating a stream of A record queries to generate a bunch of races and then stuffing the A record responses for each of these with a bogus authoritative NS record for the entire zone, is what\u0026rsquo;s called the Kaminsky Attack, after Dan Kaminsky, who discovered the attack. The defenses of picking a query ID and randomizing the ID, help, but remember the randomization is only 16 bits, so let\u0026rsquo;s think about other possible defenses.\nDNS Cache Poisioning Defense In addition to having a query ID and randomization of that ID, the resolver can randomize the source port on which it sends the query, thereby adding an additional 16 bits of entropy to the ID that\u0026rsquo;s associated with the query. Unfortunately, picking a random source port can be resource intensive and also a network address translator or a NAT, could derandomize the port. Another defense is called the 0x20 or the 0x20 encoding, which is based on the intuition that DNS matching and resolution is entirely case insensitive. So capitalization of individual letters in the domain name do not affect the answer that the resolver will return. This 0x20 bit, or the bit that affects whether a particular character is capitalized or in lower case can also be used to introduce additional entropy. When generating a response to a query such as this one, the query is copied from the DNS query into the response exactly as it was in the query. The mixed pattern of upper and lower case letters thus constitutes a channel. If the resolver and the authoritative server can agree on a shared key, then the resolver and the authoritative are the only ones who know the appropriate pattern of upper and lower case letters for a particular domain name. Because no attacker would know the appropriate combination of upper and lower case letters for a particular domain, it becomes even more difficult for the attacker to inject a bogus reply because not only would the attacker have to guess the ID, but the attacker would also have to guess the capitalization sequence for any particular domain name.\nDNS Security Quiz So why does the 0x20 encoding make DNS more secure? Is it because DNS names are case- sensitive? Is it because the encoding adds additional entropy to the query? Is it because the encoding make it easier to encrypt the queries and replies? Or is it because the encoding adds the requirement for an additional layer of hierarchy into the DNS resolution infrastructure?\nDNS Security Quiz Answer The 0x20 bit encoding adds additional entropy to the queries that a DNS resolver sends by tweaking the capitalization on a DNS name in such a way that only the resolver and the authoritative name server know the particular sequence of upper and lower case letters in the reply.\nDNS Amplification Attacks Let\u0026rsquo;s look at another attack called the DNS amplification attack. This attack exploits the asymmetry in size between DNS queries and their responses. So an attacker might send a DNS query for a particular domain, and that query might only be 60 bytes. In sending the query, however, the attacker might indicate that the source for this query is some victim IP address. Thus, the resolver might send a reply which is nearly two orders of magnitude larger to a victim. So the name of the attack amplification comes from the fact that the query is only 60 bytes and a reply is considerably larger. So, by simply generating a small amount of initial traffic, the attacker can cause the DNS resolver to generate a significantly larger amount of attack traffic. If we start adding other attackers, all of which specify the victim as the source, then all of these giant replies start heading towards the victim, and we have a denial of service attack on the victim. Two possible defenses against this attack are to prevent IP address spoofing in the first place, using, for example, the appropriate filtering rule, or to disable the ability for a DNS resolver to resolve queries from arbitrary locations on the Internet.\nDNSSEC DNS Security As we discussed, one of the major reason for DNSs vulnerabilities is a lack of authentication. The DNSSEC protocol adds authentication to DNS responses simply by adding signatures to the responses that are returned for each DNS reply. When a stub resolver issues a query, assuming there is no caching, the query is relayed by the recursive resolver to the root name server, which, as we know, sends a referral to .com, but this referral includes the signature by the root of the IP address and the public key of the .com server. As long as this resolver knows the public key corresponding to the route, it can check the signature and it knows then that the referral is to the correct IP address for .com. It also now knows the public key corresponding to the .com server. Thus when the .com server sends the next referral to Google.com, that referral is signed by .com\u0026rsquo;s private key. But the root has told the resolver the public key corresponding to .com, and thus the resolver can check that this referral is not bogus and in fact came from the .com server. Similarly, the .com server will return not only the IP address for Google.com, but also the IP address and public key for the Google.com authoritative name server so that when Google returns its answers, the resolver can check the signatures coming from google.com. In other words, each authoritative name server in the DNS hierarchy returns not only the referral, as it would with regular DNS, but also a signature containing the IP address for that referral, and the public key for the authoritative name server that corresponds to that referral. That public key then allows the resolver to check the signatures at the next lowest level of the hierarchy, until we finally get to the answer.\n"
},
{
	"uri": "/6250/internet-worms/",
	"title": "Internet Worms",
	"tags": [],
	"description": "",
	"content": " Internet Worms Types of Viruses and Worm Overview In this lesson we will talk about viruses and internet worms. Let\u0026rsquo;s first define what a virus is, and then define what a worm is. A virus is effectively an infection of an existing program that results in the modification of the original program\u0026rsquo;s behavior. A worm is code that propagates and replicates itself across the network. A worm is usually spread by exploiting flaws in existing programs or open services whereas viruses typically require user action to spread, for example, opening an attachment on an email or running an executable file that a friend gave you on an USB key. Worms propagate automatically. We will focus most of our attention on internet worms.\nBut before we dive into the details of internet worms, let\u0026rsquo;s first talk about the different types of viruses. A parasitic virus typically infects an existing executable file. A memory- resident virus infects running programs. A boot-sector virus spreads whenever the system is booted. A polymorphic virus encrypts part of the virus program using a randomly generated key. So one of the key differences between viruses and worms is that viruses typically spread with manual user intervention. Worms typically spread automatically by scanning for vulnerabilities and infecting vulnerable hosts when those vulnerabilities are discovered. A worm might use any of these techniques to infect a particular host before spreading further.\nIn the rest of the lesson we will first talk about a brief history of internet worms, including the first Internet worm, called the Morris worm, and other famous Internet worms from the early days of Internet worms in the early 2000\u0026rsquo;s, including Code Red and other well-known Internet worms of the time. We\u0026rsquo;ll then talk about how to model the spread of a worm in terms of scanning and infection rates, using analogies from epidemiology. Finally, we\u0026rsquo;ll talk about design techniques for designing super fast-spreading worms, and we\u0026rsquo;ll look at an example of a super fast-spreading worm.\nWorm and Virus Quiz So let\u0026rsquo;s have a quiz to review our knowledge of the difference between worms and viruses. So what\u0026rsquo;s the main difference between a worm and a virus? Is it that worms do not have destructive payloads, whereas viruses do? Is it that viruses only infect Windows machines, whereas worms can infect any kind of machine? Is it that viruses can spread more rapidly than worms? Or is that worms can spread automatically without human intervention, whereas viruses require human intervention to spread?\nWorm and Virus Quiz Answer The main difference between worms and viruses is that, worms can spread automatically by scanning for vulnerable hosts and spreading, whereas viruses typically require user intervention to spread such as clicking on an executable file in an email attachment or installing a particular program from a USB stick.\nInternet Worm Lifecycle A worm\u0026rsquo;s spread on the internet has the following life cycle. First, the infected machine might scan other machines on the internet to discover vulnerable hosts and subsequently infect the vulnerable machines that it discovers via remote exploit. Let\u0026rsquo;s take a look at a couple of well known early worms and how they spread, as well as how one might design a super fast spreading worm.\nFirst Worm The first worm was designed by Robert Morris, Jr. in 1988. The worm itself had no malicious payload, but it ended up bogging down the machines that it infected by spawning new processes uncontrollably and exhausting resources. And at the time it was released, it affected ten percent of all Internet hosts. It spread through three different propagation vectors. The worm tried to crack passwords using a small dictionary and a publicly readable password file, and also targeted hosts that were already listed in a trusted host file on the machine that was already infected. This ability to perform remote execution was one way that the worm was allowed to spread. The second way that it spread was in a buffer overflow vulnerability in the finger demon. This was a standard buffer overflow exploit. And if you don\u0026rsquo;t know about buffer overflows, I would urge you to take a computer security course, but essentially, this is a very common attack that makes remote exploits possible, effectively resulting in the ability to run arbitrary code at the root level privilege. The third way that worm spread, was via the debug command in send mail, which is a mail sending service. In early send mail versions, it was possible to execute a command on a remote machine by sending an SMTP message. The worm used this capability to spread automatically. A key theme that we\u0026rsquo;ll see In the design of other worms, is this use of multiple vectors. Now any particular worm may end up using a different set of vectors depending on the remote vulnerabilities that it\u0026rsquo;s trying to exploit. But the idea that any worm should be able to exploit multiple weaknesses in a system gives it more ways to spread and often also speeds up the propagation of the worm. This worm design also followed the following general approach, which we see showing up over and over again in worm designs. First, the worm needs to scan other hosts to find potentially vulnerable hosts. In the second step, it needs to spread by infecting other vulnerable hosts. And in the third step it needs to remain undiscoverable and undiscovered so that it can continue to operate and spread without being removed from systems.\nWorm Lifecycle Quiz So to review, what are the three steps in a worm\u0026rsquo;s life cycle? Please check three of the following options. Infecting a vulnerable host, patching the hosts vulnerability after infection, scanning for other vulnerable hosts to infect, or remaining undetectable.\nWorm Lifecycle Quiz Answer An Internet worm first scans for vulnerable hosts, then infects them, and finally, typically takes steps to remain undetectable. A worm does not necessarily need to patch the host vulnerability, although some Internet worms have been known to do so to prevent other worms from subsequently infecting the machine and interfering with the original worm infection. For example, if a worm was intending to spread to construct a botnet that launched a particular attack or was being used by a botmaster for a particular attack, then whoever had commandeered the machine probably wouldn\u0026rsquo;t want other attackers to come in behind him and also infect the machine and interfere with the planned attacks.\nWorm Outbreaks in Detail The summer of 2001 essentially saw a new era in internet security with three major worm outbreaks. These three major worms were Code Red 1, version two, Code Red 2, and Nimda. Let\u0026rsquo;s take a quick look at each of these worms. Code Red 1 was released on July 13th, 2001, and was the first modern worm. It exploited a buffer overflow in Microsoft\u0026rsquo;s IIS server. From the first through the twentieth of each month it would spread by finding new targets using a random scan of IP address space, it would spawn 99 new threads, which generated IP addresses at random, and then looked for vulnerable instances of IIS. Now version 2 of Code Red 1 was actually released six days later and fixed that random scanning bug so that each instance of the worm scanned a different part of IP address space. After the scanning bug was fixed, the worm was able to compromise 350,000 vulnerable hosts in a matter of only fourteen hours. By most estimates that was the complete set of hosts running the vulnerable version of IIS on the entire internet. The payload of this worm was to mount a denial of service attack on whitehouse.gov. But a bug in the coding caused the worm to die on the 20th of each month. If the victim\u0026rsquo;s clock was wrong, however, the worm would actually resurrect itself on the first. Fortunately in this case, the payload which launched the denial of service attack on whitehouse.gov actually was launched at a particular IP address, not at the domain name. So the operators of the website needed only to move the web server to another IP address to defend against the denial of service attack. A better worm design would have been much more catastrophic. Code Red 2 exploited the same vulnerability but had a completely different payload. It was released on August 4th, 2001, and was called Code Red 2 mainly because of a comment in the code. The worm actually only spread on Windows. The scan actually preferred nearby addresses. It would choose addresses from the same /8 with probability one half from the same /16 with probability three eighths, and randomly from the entire internet with the remaining one eighth probability. The reason for preferring nearby IP addresses is that if there was one vulnerable host on the network, there was likely to be more because the same administrator that failed to patch the compromised machine might have other machines on the same network that were also vulnerable. This notion of preferential scanning can speed up infections in some cases by increasing the probability that scanning will find another vulnerable host. The payload of this worm was an IIS backdoor, and the worm was completely dead, by design, by October 1, 2001. Nimba was released on September 18, 2001, and was interesting mostly because it spread using multiple propagation vectors. It was effectively multi-model. So in addition to using the same IIS vulnerability as Code Red 1 and Code Red 2, there were some additional vectors that it used. It could spread by bulk email as an attachment. It copied itself across open network shares. It installed an exploit code on webpages on the corresponding web server running on the machine, so that any browser that visited the webpage for that server would become infected itself and it would scan for the Code Red 2 backdoors that that worm had installed. The interesting thing about the multi-modal nature of the Nimda worm is that signature based defenses don\u0026rsquo;t necessarily help because of the many ways that it could spread, for example, by email or via a website exploit. Nimda actually needs firewalls. Most of the firewalls pass the email carrying Nimda completely untouched, using brand new infection with an unknown signature, and those scanners couldn\u0026rsquo;t detect it. This was the first instance of a worm that exploited what we would call a zero day attack which is when a worm first appears in the wild and the signature of the worm is not extracted until minutes, or hours later. Zero day attacks are particularly virulent because the worm can spread extremely quickly before any type of signature-based antivirus has a chance to catch up and prevent the infections in the first place.\nModeling Fast-Spreading Worms Here is a plot showing infection rate of the Code 1 Version Two Worm which ultimately infected 350,000 vulnerable hosts. Note the shape of this curve. The worm is effectively dormant or spreading extremely slowly for quite a period of time. And then there\u0026rsquo;s a inflection point at which point the infection rate becomes exponential. At some point then, infections slow and the infection rate ultimately plateaus, presumably after all of the infected hosts have been found. We can actually model the spread of these worms using the random constant spread model. If \u0026lsquo;K is the initial compromised rate, \u0026lsquo;N is the number of vulnerable hosts, and \u0026lsquo;a\u0026rsquo; is the fraction of hosts already compromised, we can now express the number of hosts infected at a particular time increment in terms of the machines already infected and the rate at which uninfected machines become compromised. So if \u0026lsquo;Nda\u0026rsquo; is the number of newly infected machines in dt, we can express that in terms of the number of machines already infected, which is \u0026lsquo;N\u0026rsquo; times \u0026lsquo;a\u0026rsquo;. So these are the host already capable of doing more scanning, and now \u0026lsquo;K\u0026rsquo; times 1 minus \u0026lsquo;a\u0026rsquo; is the rate at which uninfected machines become compromised in a particular time interval dt. If we solve for \u0026lsquo;a\u0026rsquo;, the fraction of hosts compromised, which is effectively the y-axis of this graph, we get the following. You get an exponential curve that is exponential where the growth rate depends only on K, or the initial compromise rate. This is very interesting because it tells us that if we want to design a very fast spreading worm, then we should design a worm such that the initial compromise rate is as high as possible. So how do we increase K? Or how do we increase that initial compromise rate?\nIncreasing Compromise Rate One possibility for increasing the initial compromise rate, or designing a very fast spreading worm, is to create a hit list, or a list of vulnerable hosts ahead of time. That curve we just saw shows that the time to infect the first 10,000 hosts dominates infection time. So if we start by performing stealthy scans or some reconnaissance to construct a list of vulnerable hosts before we start spreading, then we can get rid of that initial flat part of the curve where the worm is effectively dormant. The second approach is to use something called permutation scanning where every compromised host has a shared permutation of an IP address list to scan for vulnerabilities. Now if this list is randomly permuted and a particular host starts scanning from its own IP address in the list and works down, then different affected hosts will start scanning from different parts of this list ensuring that compromised hosts don\u0026rsquo;t duplicate each other\u0026rsquo;s work.\nOne worm that exploited these techniques to spread particularly quickly was the Slammer worm, which spread in January of 2003, exploiting a buffer overflow in Microsoft\u0026rsquo;s SQL server. In addition to using fast scanning techniques, the entire slammer code fit in a single, small UDP packet. The UDP packet contained the worm binary, followed by an overflow pointer back to itself. It was a classic buffer overflow combined with random scanning. Once the control is passed to the worm code, it randomly generated IP addresses and attempted to send a copy of itself to Port 1434 on other hosts. One brilliant aspect of the slammer worm is that because it was spread via a single UDP packet, it was connectionless, meaning that it could spread and was no longer limited by the latency of network round trip time, but only by the bandwidth of the network. The worm caused $1.2 billion dollars in damage and temporarily knocked out many elements of critical infrastructure including Bank of America\u0026rsquo;s ATM network, an entire cell phone network in South Korea, and five route DNS servers, as well as Continental Airlines\u0026rsquo; ticket processing software. The worm actually did not have a malicious payload, but the bandwidth exhaustion on the network caused resource exhaustion on the infected machines. Here\u0026rsquo;s a picture of the hosts around the world that Slammer infected. This damage was inflicted in just thirty minutes, due to the very lightweight nature in which Slammer spread.\nSlammer Worm Quiz So as a quick quiz, what allowed the Slammer worm to spread so quickly? Was it that TCPs reliable transport ensured a clean copy of the worm payload would spread to different vulnerable hosts? Was it that the worm spread by UDP, thereby enabling itself to spread with limited network overhead? Was it that it could infect many different types of operating systems including Linux and Mac OS? Or, was it that it could fit in a single packet?\nSlammer Worm Quiz Answer Slammer was able to spread quickly because it spread via connectionless transport, or UDP, and because it could fit in a single packet.\n"
},
{
	"uri": "/6250/spam/",
	"title": "Spam",
	"tags": [],
	"description": "",
	"content": " Spam Okay, in this lesson, we will talk about spam or unwanted commercial email. Now, you might not think that you receive a lot of spam, but the fact of the matter is that most of it goes to your spam folder. So one might think, what\u0026rsquo;s the problem? Well, in fact, spam remains a scourge for network operators. In particular, someone has to design the filters that separate the good traffic from the bad traffic. Additionally, even if email is classified as spam, if it\u0026rsquo;s accepted for delivery, the Internet\u0026rsquo;s mail protocols dictate that the server has to keep the mail, because it\u0026rsquo;s told the receiver that it has accepted the mail. This creates the potential for spam to consume a significant amount of storage space on email servers. Finally, spam can create security problems for users who receive spam emails. If the spam messages contain a payload that could be harmful, such as malware or a phishing attack, or an attempt to steal a user\u0026rsquo;s private or sensitive information, such as a password. Now even though you don\u0026rsquo;t see the mail because of these filters, something like 95% of all email traffic is spam. Some reports from the Anti-Phishing Working Group suggests that something like 1 in every 87 emails was a phishing attack. And there\u0026rsquo;s something like 50,000 unique fishing attacks in a month.\nA common approach for getting rid of spam messages is to filter. In other words, prevent the message from reaching the user\u0026rsquo;s inbox in the first place. Now this begs the question of how to differentiate spam, or the bad messages, from ham, or the legitimate messages. There are three different ways to construct filters. One is content-based. In other words, you can look at what\u0026rsquo;s being said in the mail. For example, if the mail contains particular words, such as Viagra or Rolex, a content-based filter might pick up on those terms and decide to filter the mail. Second, a filter might make a decision about whether an email message is spam or ham based on the IP address of the sender. This method is often called blacklisting. Third, we can construct filters based on behavioral features, or how the mail is sent. So, for example, if the mail is sent at a particular time of day, or if it\u0026rsquo;s sent in a batch of emails that are all roughly the same size, then we may be able to figure out that a message is likely spam simply based on the sender\u0026rsquo;s sending behavior. Now each of these approaches are complimentary, but content-based filtering and IP- based filtering each have problems. Content-based filters are relatively easy for attackers to evade. A recent large commercial mail operator recently told me that he saw something like 80,000 different spellings of Viagra. But additionally, messages can be carried not only in text, but in images, Excel spreadsheets, or even MP3s or movies. Therefore, spammers can easily alter the features of an email\u0026rsquo;s content and adjust those features and change them to evade content- based filters. On the flip side, those maintaining the filters suffer a relatively high cost, because the filters must be continually updated as content changes and the means of carrying the content becomes more sophisticated.\nContent-based Email Filter Quiz So, as a quick quiz, what are some problems with content-based email filters? Are they too slow? Are they easy for attackers to evade? Or are words in texts of emails difficult to parse? In this case, please choose the single best answer.\nContent-based Email Filter Quiz Answer As we discussed, content-based filters are easy for attackers to evade because they can very easily change the content of the message that is carrying the spam that they wish to deliver. They can embed their message in things like images, mp3\u0026rsquo;s, Excel spreadsheets, and so forth, making it relatively difficult for the filter maintainers to keep up.\nIP Blacklisting So we\u0026rsquo;ve talked about problems with content-based filtering. What about IP blacklists? Well, first, the way an IP blacklists works is that when a sender sends an email to the receiver, the receiver sends a query for that IP address to a blacklist or a DNS-based blacklist sometimes called a DNSBL such as spamhaus. Depending on whether or not that IP address appears in the blacklist, the receiver can then decide to accept the message, or if the IP address turns out to be on the blacklist, the receiver can decide to terminate the connection and not even accept the mail in the first place, thereby saving the operator the trouble of even having to store the message.\nThe third approach is to filter a message on how it is sent. In particular, we can look at such features as the geographic locations of the sender and receiver, the set of target recipients, the sender\u0026rsquo;s upstream ISP, or our inference as to whether the sender is a member of a botnet or a network of comprised hosts that are doing the bidding of some command and control server. Now the challenges of building a filter around this notion is first, understanding network level behavior and second, building classifiers using network level features to execute the filtering.\nSpam Blacklisting (cont) A surprising finding from our earlier work is that spammers can perform behavior on the network that is extremely uncanny and unlikely to be performed by a legitimate network user. For example, what we saw is that the spammer could hijack an IP prefix for a very short period of time, such as 10 minutes, send the spam or potentially multiple spam messages from IP addresses inside that IP prefix, and at the end of the attack, withdraw the prefix. This allows attackers to use ephemeral IP addresses, essentially rendering IP blacklists ineffective. In fact, we saw on any given day about 10% of the email senders are from previously unseen IP addresses. This ephemerality or transience of the IP addresses of the spam senders makes it particularly difficult to maintain a blacklist.\nIn fact, we\u0026rsquo;ve found many single-packet features that tended to work well. In other words, features that a receiver could make a decision on just based on the first packet that a sender sends. Such single-packet features include the distance between the sender and the receiver, the density in IP space in terms of how many other mail senders are nearby, and the local time of day at the sender. Other features, such as the AS of the sender\u0026rsquo;s IP, also worked well. If we\u0026rsquo;re willing to look beyond a single packet and look at a single message, the number of recipients, and the length of the message also prove to be effective in distinguishing spammers from legitimate senders. Finally, we can look at aggregates. For example, if we\u0026rsquo;re willing to look at a group of email messages, we can see how message length varies over time or across a group of different messages. Putting these features together in a system called SNARE, or Spatiotemporal Network Level Automated Reputation Engine, achieved a 70% detection rate for a false positive rate of about one-tenth of 1%. This level of accuracy is good enough to be used in practice. It provides comparable performance to state of the art IP-based blacklists such as spamhaus. But it only uses network-level features, thus making it less susceptible to the ephemeral nature of IP-based blacklisting.\n"
},
{
	"uri": "/6250/denial-of-service-dos/",
	"title": "Denial of Service DoS",
	"tags": [],
	"description": "",
	"content": " Denial of Service Attacks Overview So in this lesson, we will talk about Denial of service attacks and defenses. We\u0026rsquo;ll talk about what denial of service attacks are and various defenses. We\u0026rsquo;ll talk about how to infer denial of service activity, and we\u0026rsquo;ll talk about how to secure networks against denial of service attacks using Software Defined Networking.\nSo what is denial of service? Denial of service is simply an attack that attempts to exhaust various resources. One resource that a Denial of Service attack might exhaust is network bandwidth. Another is TCP connections. For example, a host might only have a limited number of TCP connections that it can open to various clients, or the Denial of Service attack might attempt to exhaust various server resources. For example, this victim might be a web server running complicated scripts to render web pages, and if the web server suddenly becomes the target of a bunch of bogus requests, the server may spend a lot of resources rendering pages for requests that are not legitimate. Before 2000, these Denial of Service attacks were typically single source. After 2000, with the rise of internet worms as we saw in an earlier lesson, these attacks could become distributed, effectively being launched from many attackers.\nLet\u0026rsquo;s talk about three different types of defenses against denial of service attacks. First we have something called ingress filtering. Then we have something called URPF, or reverse path filtering checks. And then in the case of an attack on TCP connection resources, we can use something called TCP syn cookies to defend against Denial of Service. Let\u0026rsquo;s suppose that we have a stub autonomous system whose IP prefix was 204.69.207.0/24. Now this is a stub network that has no other networks connected to it and this is the only IP address space that this network owns. Then, the router that is immediately upstream of that internet service provider can simply drop all traffic for which the source IP address is not in the IP address range of that particular network. So this is foolproof and it works at the edges of the internet where it\u0026rsquo;s very easy to determine the IP address range that\u0026rsquo;s owned by a downstream stub autonomous system. Unfortunately it doesn\u0026rsquo;t work well in the core, where a particular router might have a lot of difficulty determining whether packets from a particular source IP address could be allowed on a particular incoming interface. So the solution that operators try to use in the core is to use the routing tables to determine whether a packet could feasibly arrive on a particular incoming interface. So if a router had a routing table that said all packets for 10. 0.1.0/24, should be sent via interface one, and all packets destined for 10.0.18.0/24 should be sent via interface two, then URPF says if we see a packet for/with a particular source IP address on an incoming interface that is different than where we would have sent the packet in the reverse direction, then we should go ahead and drop this packet. So the benefits of URPF is that it\u0026rsquo;s automatic, but the\ndrawbacks are that it requires symmetric routing. And we know from earlier lessons that routing in the internet is often asymmetric. Therefore in any situation where asymmetric routing is a possibility, it is not possible or reasonable to use URPF. So we\u0026rsquo;ve talked about ingress filtering and URPF checks, and let\u0026rsquo;s now talk about the use of Syn cookies to defend against TPC based denial of service attacks.\nTCP 3-Way Handshake Review So in a typical TCP three-way handshake, the client sends a SYN packet to the server, the server responds with the SYN-ACK, and the client then returns with an ACK to the SYN-ACK, at which point the connection is established. The problem in a typical TCP three-way handshake is that the client can send a SYN and cause the server to allocate a socket buffer for that TCP connection. But then if the client never returns, the client can force the server to allocate many, many socket buffers simply by sending a lot of SYNs and never returning. In fact, these could even be from spoofed IP addresses. So in other words, the client has absolutely no accountability and no obligation to return to send the final ACK, and yet can cause the server to allocate resources.\nThis is a problem, and the solution to this is called SYN cookies. In the TCP SYN cookie approach, when the server receives a SYN from the client, the server, instead of allocating a socket buffer for the tuple associated with the connection, the server keeps no state, and instead picks an initial sequence number for the connection that\u0026rsquo;s a function of the client\u0026rsquo;s IP address and port, and the server\u0026rsquo;s IP address and port, as well as a random knots to prevent replay attacks. An honest client that returns can then reply with an acknowledgement with that sequence number in the packet. The server can check that sequence number simply by rehashing all of the information that it already has, thereby determining that the acknowledgement here corresponded to the previous SYN-ACK that it had sent the client without requiring the server to store any state. Only if the sequence number matches the one picked by the server in the SYN-ACK does the server actually establish the connection.\nTCP SYN Cookie Quiz So as a quick quiz, what are some of the advantages of TCP Syn cookies? Is it that they can be applied to filter traffic in the network core? Is it that they can prevent the server from exhausting state by setting up socket buffers after receiving a TCP Syn? Or is it that they can defend against UDP flooding attacks?\nTCP SYN Cookie Quiz Answer TCP SYN cookies can prevent a server from exhausting state after receiving the initial TCP SYN packet.\nInferring Denial of Service using Backscatter Let\u0026rsquo;s talk about how to infer denial of service activity using a technique called backscatter. The idea behind backscatter is that when an attacker spoofs a source IP address, say on a TCP SYN flood attack, that the replies to that initial TCP SYN from the victim will go to the location of the source IP address. This replies to forged attack messages are called \u0026ldquo;backscatter\u0026rdquo;. Now the interesting thing about backscatter is that if we can assume that the source IP addresses are selected by the attacker at random, and we could set up a portion of the network where we could monitor this back scatter traffic coming back as SYN-ACK replies to forged source IP addresses, if we assume that these source IP addresses are picked uniformly at random, then the amount of traffic that we see as back scatter represents exactly a fraction that\u0026rsquo;s proportional to the size of the overall attack. So, for example, if we monitor N IP addresses and we see M attack packets, then we expect to see here N over two to the 32 of the total back scatter packets, and hence of the total attack rate. If we want to compute the total attack rate, we simply invert this fraction. So for example, in this case, if our telescope were a slash eight, or two to the 24th IP addresses, we would simply multiply our observed attack rate x by two to the 32 divided by two to the 24 or 255.\nBackscatter Quiz As a quick quiz, let\u0026rsquo;s suppose that our telescope is monitoring two to the 16th IP addresses. And let\u0026rsquo;s suppose that in that telescope, we see 100,000 packets per second. What\u0026rsquo;s the total attack rate?\nBackscatter Quiz Answer Since we\u0026rsquo;re monitoring one 2 to the 16th of the entire internet, or 1 over 65,535 of the total internet, we simply need to take the rate that we\u0026rsquo;ve observed and invert that. In this case, that rate would be roughly 6.5 billion packets per second.\nAutomated Denial of Service Attack Mitigation In the assignment you will use a Pyretic controller to mitigate a DOS attack. We will use an extension of Pyretic called Py Resonance, which allows for composition of finite state machines that run various programs depending on the state of the network. Your network will start in a normal state, but will use an sFlow-based Denial of Service detector to indicate that the network has come under attack. Your sFlow event will cause the controller to change states, and hence it will install specific flow-table entries that mitigate the effects of the Denial of Service attack. The assignment that is spelled out on the home page has links to some more in-depth descriptions of this particular assignment and your task is writing a Py Resonance application to mitigate the DOS attack.\n"
},
{
	"uri": "/6250/",
	"title": "6250",
	"tags": [],
	"description": "",
	"content": " About: This is a project based course examining and replicating research in the field of computer networks. Python, mininet, and pyretic are heavily used throughout.\nStaff:  Nick Feamster, Lecturer Sean Donovan, Head TA  Resources:  Course site  Reading list:  How To Read a Paper Design Philosophy of DARPA Internet Protocols End to End Arguments in System Design BGP Routing Policies in ISP Networks Are we there yet? 20 years of IPV6 CUBIC Congestion Control TCP Fast Open Application Flow Control in YouTube Video Streams MPTCP: How Hard Can It Be? Designing and Implementing a Deployable Multipath TCP  Paper Presentation  Sizing Router Buffers Controlling Queue Delay The Road to SDN: An Intellectual History of Programmable Networks Modular SDN Programming with Pyretic Jellyfish: Networking Data Centers Randomly PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric ASwatch: An AS Reputation System to Expose Bulletproof Hosting ASes How Secure are Secure Interdomain Routing Protocols? Hold-On: Protecting Against On-Path DNS Poisoning The Crossfire Attack  "
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " OMS:Notes About This project has two goals:\n Accelerate the rate of learning for all Provide resources to help those either 1) new to OMSCS or 2) new to a technical domain   If you want to go fast, go alone. If you want to go far, go together. \u0026ndash; AFRICAN PROVERB\n Contributing Contributions are very welcome. This is first and foremost a community effort by OMSCS for OMSCS.\n If you would like to make an occasional edit, please do so with the tab on the relevent page. If you would like to contribute more, please see the contributing guide \u0026ndash; coming soon  If any material here is out of date or can be improved, click on the \u0026ldquo;Improve this page\u0026rdquo; tab.\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]