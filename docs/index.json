[
{
	"uri": "/6250/introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Introduction: Welcome to Computer Networking We\u0026rsquo;ll be covering advanced concepts in networking such as software defined networking (SDN), data center networking (DCN) and content distribution. You\u0026rsquo;ll complete projects using a state of the art network emulator called mini-net to understand and explore these advanced concepts leading up to a final project replicating actual networking research.\nComputer Networking Welcome to the graduate course on computer networking. The primary goal of this course is to provide a survey of the necessary tools, techniques, and concepts to perform research in computer communications. This is a project based course, and there will be significant emphasis on hands-on experience. In networking, perhaps more than many other subjects, realization is key. You can read about concepts or techniques in a textbook, but really the most effective way to learn networking is by doing. So, you\u0026rsquo;ll gain a lot of hands on experience in this course through the assignments. In comparison to an introductory networking course which you may have taken, this course will provide more in depth coverage of networking topics, and it will also offer a crash course in some of the available tools that are now available for performing research in computer networking. You will gain experience with many of these tools through the project based assignments in the course.\nTwo Components The course has essentially two components. In the lectures you will learn about cutting edge research problems in computer networking and you\u0026rsquo;ll also gain the ability to come up with your own problems. We\u0026rsquo;ll pick up the basics along the way as necessary. In addition to the lectures there are also a number of problem sets or assignments that you will work through as you work your way through the course. The problem sets and assignments in the course will give you proficiency with the tools and technologies that are state of the art in the research community. That will allow you to follow through on the research ideas that you may come up with as we work through various topics in the course. There are tons of exciting tools to use, and the problem sets and assignments will help you gain proficiency with them.\nWhat the Course is NOT About It\u0026rsquo;s also worth bearing in mind what this course is not about. The course is not an introduction to networking, so there are a number of basic topics that won\u0026rsquo;t be covered in this course. In particular, we\u0026rsquo;ll assume that you\u0026rsquo;re already familiar with the basics of things like TCP, Socket programming, and so forth. Anything that you might have picked up in an introductory networking course, we are just going to assume as a prerequisite for this course. So before you proceed, it may be worth revisiting some of your old undergraduate networking course material. The course is also not providing any introduction to programming. However, many assignments in the course will make use of some amount of programming. So some knowledge of scripting languages like Ruby, Python or Perl will certainly be helpful in some assignments. We\u0026rsquo;ll be making a lot of use of a network emulation toolkit called Mininet, and to use that tool most effectively, you will certainly want to learn some Python if you don\u0026rsquo;t already know it. Don\u0026rsquo;t worry if you don\u0026rsquo;t know these languages already, though. There\u0026rsquo;s plenty of time to learn in the course since the deadlines are fairly spread out. And the assignments aren\u0026rsquo;t focused on knowledge of programming per say, but rather, the concepts that you are going to realize in the programming languages.\nCourse Structure The course is broken into three smaller sub-courses. The first course will cover topics including architectural principles, switching, routing, naming, addressing, and forwarding. The second part of the course will cover congestion control, streaming, rate limiting, and content distribution. And the third part of the course will have modules on software defined networking, traffic engineering, and network security. There will be about three assignments per sub course, plus a final project.\n"
},
{
	"uri": "/7646/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": " Python: Numpy \u0026amp; Pandas Dataframes A dataframe is a data structure in pandas that allows multiple datasets to be mapped to the same indices. For example, a data frame that maps dates to closing prices could be:\n    SPY AAPL GOOG GLD     2000-01-09 101.01 50.89 NaN NaN   2000-01-10 100.05 50.91 NaN NaN   \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip;   2015-12-31 200.89 600.25 559.50 112.37    The indices in this dataframe are the dates on the left, and the closing prices for that date are stored in each column. The ”NaN”s appear because GOOG and GLD were not publicly traded during those periods.\nReading CSVs into Dataframes To begin using the dataframes, you need data first. Historical stock data from Yahoo is provided in the form of a CSV file, which can be easily read into a dataframe using pandas’s function read csv().\nimport pandas as pd def test_run(): df = pd.read_csv(\u0026quot;data/AAPL.csv\u0026quot;) print df if __name__ == \u0026quot;__main__\u0026quot;: test_run()  This example reads in a CSV corresponding to the historic data for AAPL (Apple, Inc) into the variable df. df is a DataFrame object, which means any DataFrame methods may be used on it.\nAn example of a method that can be used is max(), which returns the maximum value in the range.\nimport pandas as pd def get_max_close(symbol): df = pd.read_csv(\u0026quot;data/{}.csv\u0026quot;.format(symbol)) return df ['Close'].max() def test_run(): for symbol in ['AAPL','IBM'] print \u0026quot;Max close \u0026quot; print symbol, get_max_close(symbol) if __name__ == \u0026quot;__main__\u0026quot;: test_run()  Plotting Matplotlib can be used to plot the data in the dataframes, as pandas can conveniently tap into the matplotlib API. Plotting data in a dataframe is as simple as calling plot() on one of the series in the frame.\nExample: Plotting the Adjusted Closea price of AAPL\nimport pandas as pd import matplotlib.pyplot as plt def test_run(): df = pd.read_csv('data/AAPL.csv') print df['Adj Close'] df[['Adj Close', 'Close']].plot() plt.title('Comparison') plt.show() if __name__ == '__main__': test_run()     index AdjClose     0 669.79   1 660.59   2 662.74   3 680.44   4 676.27   \u0026hellip; \u0026hellip;   3173 24.60   3174 24.96    Issues There are some issues with the data that need to be solved to effectively use it in the way we want.\n Trading days: The NYSE only trades for a certain number of days per year, which means that indexing by dates will return some results when the exchanges were not open. This poses problems for trying to pull out certain date ranges from the dataframe. Multiple stocks: One of the dataframe’s powers is to be able to contain multiple ranges, which means that we need to be able to retrieve multiple datasets and store them into the dataframe. Date order: The data in the Yahoo CSV are in reverse chronological order (most recent at the top), so any analysis on the dataframe will be going backwards in time, which is not ideal.  Solution to the issues To solve the trading days problem, we’ll use an Exchange-Traded Fund (ETF) called SPY (S\u0026amp;P 500) to serve as a basis for what days the stock market is open. The only days that exist in the dataset for this ETF are the days the stock market traded, so if we use this as a reference and use joining on the dataframes, we can recover data on only the days which had trading.\nExample: Using joins to get only traded days\nstart_date = '2010-01-22' end_date = '2010-01-26' dates = pd.date_range(start_date, end_date) df1 = pd.DataFrame(index=dates) # build empty dataframe  If we were to print out df1, the output would be:\nEmpty DataFrame Columns : [] Index : [2010-01-22 00:00:00, 2010-01-23 00:00:00, 2010-01-25 00:00:00, 2010-01-26 00:00:00]  This empty dataframe will be the basis for the data we want to retrieve. The next step is to join this dataframe with a dataframe with the data for SPY. This will keep only indices of the SPY dataframe that also exist in the empty one.\nExample: Reading in the new dataframe and joining them\ndfSPY = pd.read_csv(\u0026quot;data/SPY.csv\u0026quot;, index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=[ 'Date','Adj Close'], na_values =['nan']) df1 = df1.join(dfSPY)  The output would now be:\n Adj Close 2010-01-22 104.34 2010-01-23 NaN 2010-01-24 NaN 2010-01-25 104.87 2010-01-26 104.43  To get rid of the ”NaN”s, you can call dropna() on the newly joined dataframe, but there is a better way of joining them such that the ”NaN”s don’t appear in the first place. The join type is called an inner join, which joins at the intersection of the two dataframes. This way, only the dates which are in both will be kept as indices. Everything else will be thrown away.\nExample: The inner join\ndf1 = df1.join(dfSPY, how='inner')  Multiple stocks Reading in multiple stocks is as easy as just adding a for loop:\nExample: Reading in multiple stocks into a single dataframe\ndfSPY = dfSPY.rename(columns={'Adj Close': 'SPY'}) df1 = df1.join(dfSPY, how='inner') symbols = ['GOOG', 'IBM', 'GLD'] for symbol in symbols: df_tmp = pd.read_csv(\u0026quot;data/{}.csv\u0026quot;.format(symbol), index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=['Date', 'Adj Close'], na_values=['nan']) # rename to prevent name clash (multiple columns with same name) df_tmp = df_tmp . rename(columns={'Adj Close': symbol}) df1 = df1.join(df_tmp, how='inner')  Here’s an example of reading and plotting multiple stocks’ closing price on one plot\nExample: Reading and plotting multiple stocks\nimport os import pandas as pd import matplotlib.pyplot as plt def plot_selected(df, columns, start_ind, end_ind): print df.ix[start_ind: end_ind, columns] plot_data(df.ix[start_ind:end_ind, columns]) def symbol_to_path(symbol, base_dir=\u0026quot;data\u0026quot;): return os.path.join(base_dir, \u0026quot;{}. csv\u0026quot;. format(str(symbol))) def get_data(symbols, dates): df = pd.DataFrame(index=dates) if 'SPY' not in symbols: symbols.insert(0, 'SPY') for symbol in symbols: tmp = pd.read_csv(symbol_to_path(symbol), index_col=\u0026quot;Date\u0026quot;, parse_dates=True, usecols=['Date', 'Close'], na_values=['nan']) tmp = tmp.rename(columns={'Close': symbol}) df = df.join(tmp) if symbol == 'SPY': df = df.dropna(subset=['SPY']) return df def plot_data(df, title=\u0026quot; Stock prices\u0026quot;): ax = df.plot(title=title, fontsize=12) ax.set_xlabel(\u0026quot;Date\u0026quot;) ax.set_ylabel(\u0026quot;Closing price\u0026quot;) plt.show() def test_run(): dates = pd.date_range('2010-01-01', '2010-12-31') symbols = ['IBM ', 'GLD '] df = get_data(symbols, dates) plot_selected(df, ['SPY', 'IBM '], '2010-03-01', '2010-04-01') if __name__ == '__main__': test_run()  Normalizing Sometimes when plotting, the values of a stock will be significantly different from the other stocks such that it becomes difficult to tell some of them apart. Normalizing the data allows all of them to start at the same point and then show divergences from the initial point, making it easier to compare them at the same time.\nNormalizing the dataframe is as simple as dividing the entire dataframe by its first row\nExample: Normalizing a dataframe\ndef normalize_data(df): df = df / df.ix[0 ,:] return df  NumPy The actual data in the dataframe is actually an ndarray in NumPy (A multidimensional homogeneous array). That means we can do operations on the data using NumPy. For example, if you have a dataframe df1, the ndarray would be extracted by doing\nnd1 = df1.values  Accessing a cell in the array is as simple as:\nval = nd1[row, col]  You can also access subarrays by indexing with the colon:\nsub = nd1[0:3, 1:3]  would capture the rectangular subarray from the first to the third rows and the second to third columns.\nIndexing Note that the second part of the index is 1 past the actual index that will be the last, so 0:3 only pulls out 0, 1, and 2. Like in MATLAB, you can pull out everything using just the colon. For example:\nsub = nd1[3 ,:]  would retrieve all columns of row 3.\nNegative indexing To get the last index, you can use negative numbers (the last index would be -1, second to last -2, etc.)\nsub = nd1[-1, 1:3]  would get columns 1,2 of the last row\nBoolean indexing/masking Suppose we want to get the values in an array, a, which are all less than the mean. NumPy’s masking feature makes it really intuitive, as all you need to do is:\nlessThanMean = a[a \u0026lt; a.mean()]  The array a \u0026lt; a.mean() would be a boolean array, which might look like\n[[ True, True, False, False]]  Assignment Assigning values in an array is easy using the NumPy notation. For example, say we wanted to replace the values in the first 2x2 square of nd1 with the 2x2 square in nd2 with columns 2 and 3, and rows 3, and 4. The operation would be:\nnd1[0:2, 0:2] = nd2[-2:, 2:4]  Creating an array Creating a numpy array is as easy as passing in a normal python list into the array method:\nimport numpy as np print np.array([1, 2, 3])  Creating a 2D m x n array is as simple as passing in a m-long list of n-tuples.\nprint np.array([(1, 2, 3), (4, 5, 6)])  would output\n[[1 ,2 ,3] [4 ,5 ,6]]  More initializers You can also create arrays with certain initial values.\nnp.empty((5, 3, 2))  initializes an ”empty” 5x3x2 dimensional array. The values in the array are actually whatever was in the memory locations of the array pointers, so the output could look like garbage.\nnp.ones((5, 4), dtype=np.int)  creates a 5x4 array, where the value in each cell is the integer 1.\nnp.random.random((5, 4))  creates a 5x4 array with random numbers from a uniform distribution in [0.0,1.0). An example result could be:\n[[ 0.82897637 0.36449978 0.91209931 0.96307279] [ 0.63777312 0.24482194 0.5817991 0.18043012] [ 0.85871221 0.98874123 0.68491831 0.53831711] [ 0.52908238 0.81083147 0.97440602 0.81032768] [ 0.98566222 0.38902445 0.16922005 0.0873198 ]]  Other methods or fields, such as sum() or size() can be looked up in online documentation.\n"
},
{
	"uri": "/42/",
	"title": "42",
	"tags": [],
	"description": "",
	"content": " About Georgia Tech\u0026rsquo;s online Master of Science in Computer Science (OMS CS) comprises a curriculum of courses taught by the world-class faculty in the Georgia Tech College of Computing.\nGetting Started TODO: Resources/guidance for new students\u0026hellip;\nCommunities  Slack Reddit Facebook  Resources:  OMSCS FAQ: here and here Course Reviews Awesome-OMSCS Repo  "
},
{
	"uri": "/6250/architecture-and-principles/",
	"title": "Architecture and Principles",
	"tags": [],
	"description": "",
	"content": " Architecture \u0026amp; Principles We\u0026rsquo;ll begin our foray into networking by reviewing the history of the internet and its design principles. Networking today is an eclectic mix of theory and practice in large part because the early internet architects set out with clear goals and allowed flexibility in achieving them.\nWith all that flexibility, does that mean we\u0026rsquo;ll see the rollout of IPv6 soon? Only in your dreams.\nA Brief History of the Internet In this lesson we will cover a brief history of the internet. The internet has its roots in the ARPA Net which was conceived in 1966 to connect big academic computers together. The first operational ARPA Net nodes came online in 1969 at UCLA, SRI, UCSB, and Utah. Around the same time, the National Physical Laboratory in the UK also came online. By 1971 there were about 20 ARPANet Nodes and the first host-to-host protocol. There were two cross country links, and all of the links were at 50 KBPS.\nHere is a rough sketch of the ARPANet as drawn by Larry Roberts in the late 1960s. You can see the four original Nodes here, as well as some other well known players such as Berkeley, the MAC project at MIT, BBN, Harvard, Carnegie-Mellon, Michigan, Illinois, Dartmouth, Stanford, and so forth. This is what the ARPANET looked like in the late 1960s.\nHere\u0026rsquo;s a picture of the ARPANET in June 1974. And you can see not only some additional networks that have come online, but also a diagram of the machines that are connected at each of the universities. You can also see a connection here between the ArpaNet and MPLnet. Of course, the ArpaNet wasn\u0026rsquo;t the only network. There were other networks at the time. Sat Net operated over satellite. There were packet radio networks, and there were also Ethernet local area networks. Work started in 1973 on replacing the original network control protocol with TCP/IP where IP was the Internetwork Protocol and TCP was the Transmission Control Protocol.\nTCP/IP was ultimately standardized from 1978 to 1981 and included in Berkley UNIX in 1981. And on January 1st, 1983 the internet had one of its flag days, where the ArpaNet transitioned to TCP/IP. Now the internet continued to grow, but the number of computers on the internet really didn\u0026rsquo;t start to take off until the mid 90s. You can see here that around August 1995 there were about 10 million hosts on the internet, and five years later there was an order of magnitude more hosts on the internet—more than 100 million. During this period the Internet experienced a number of technical milestones. In 1982 the internet saw the rollout of the domain name system which replaced the host.txt file containing all the world\u0026rsquo;s machine names with a distributed name lookup system. 1988 saw the rollout of TCP Congestion Control after the net suffered a series of congestion collapses. 1989 saw the NSF net and BGP inter-domain routing including support for routing policy. The 90s, on the other hand, saw a lot of new applications. In approximately 1992 we started to see a lot of streaming media including audio and video. Web was not soon after, in 1993, which allowed users to browse a mesh of hyperlinks. The first major search engine was Altavista, which came online in December of 1995, and peer to peer protocols and applications including file sharing, began to emerge around 2000.\nProblems and Growing Pains Now, today\u0026rsquo;s internet is experiencing considerable problems and growing pains, and it\u0026rsquo;s worth bearing some of these in mind and thinking about them, as many of them give rise to interesting research problems to think about as we work through the material in the course. One of the major problems is that we\u0026rsquo;re running out of addresses. The current version of the internet protocol, IPV4, uses 32-bit addresses, meaning that the IPV4 internet only has 2 to the 32 IP addresses, or about 4 billion IP addresses. Furthermore, these IP addresses need to be allocated hierarchically and many portions of the IP address space are not allocated very efficiently. For example, the Massachusetts Institute of Technology has one two fifty sixth of all the Internet address space. Another problem is congestion control. Now congestion control\u0026rsquo;s goal is to match offered load to available capacity. But one of the problems with today\u0026rsquo;s congestion control algorithms is that they have insufficient dynamic range. They don\u0026rsquo;t work very well over slow and flaky wireless links and they don\u0026rsquo;t work very well over very high speed intercontinental paths. Now, some solutions exist but change is hard and all solutions that are deployed must interact well with one another. And deployment in some sense requires some amount of consensus. A third major problem is routing. Routing is the process by which those on the internet discover paths to take to reach another destination. Today\u0026rsquo;s interdomain routing protocol, BGP, suffers a number of ills, including a lack of security, ease of misconfiguration, poor convergence, and non-determinism. But it sort of works and it\u0026rsquo;s the most critical piece of the internet infrastructure in some sense because it\u0026rsquo;s the glue that holds all of the internet service providers together. Another major problem in today\u0026rsquo;s internet is security. Now while we\u0026rsquo;re reasonably good at encryption and authentication, we are not actually so good at turning these mechanisms on. And we\u0026rsquo;re pretty bad at key management, as well as deploying secure software and secure configurations. The fifth major problem is denial of service. And the internet does a very good job of transmitting packets to a destination even if the destination doesn\u0026rsquo;t want those packets. This makes it easy for an attacker to overload servers or network links to prevent the victim from doing useful work. Distributed denial of service attacks are particularly commonplace on today\u0026rsquo;s Internet. Now, the thing that all of those problems have in common is that they all require changes to the basic\ninfrastructure, and changing basic infrastructure is really difficult. It\u0026rsquo;s not even clear what the process is to achieve consensus on changes. So as we work our way through the course, it will be interesting to see the problems that we encounter in each of these areas, various solutions that have been proposed, and also to think about ways in which new protocols and technologies can be deployed. In later parts of the course we\u0026rsquo;ll learn about a new technology called software defined networking, or SDN. That makes it easier to solve some of these problems by rolling out new software technologies, protocols, and other systems to help manage some of these issues.\nArchitectural Design Principles In this lecture we will talk about the Internet\u0026rsquo;s original design principles. These design principles were discussed in the paper reading for today, the Design Philosophy of the DARPA Internet Protocols, by Dave Clark, dated 1988. The paper has many important lessons, and we will go through many of them as we revisit many of the design decisions. Before we jump into any details let\u0026rsquo;s talk about some of the high level lessons. One of the most important conceptual lessons is that the design principles and priorities were designed for a certain type of network. And as the internet evolves, we are feeling some of the growing pains of some of those choices. In the last lesson we talked about a number of the problems and growing pains of the internet. And it\u0026rsquo;s worth bearing in mind that many of the problems that we are seeing now, are a result of some of the original design choices. Now that\u0026rsquo;s not to say that some of these design choices are right or wrong, but rather that they simply reflect the nature of our understanding at the time, as well as the environment and constraints that the designers faced for the particular network that existed at that time. Now needless to say, some of the technical lessons from the original design have turned out to be fairly timeless. One concept is packet switching, which we will discuss in this lesson. And another is the notion of fate sharing, or soft state, which we will discuss in a subsequent lesson in the course.\nGoal The fundamental design goal of the internet was multiplexed utilization of existing interconnected networks. There are two important aspects to this goal. One is multiplexing or sharing. So one of the fundamental challenges that the internet technologies needed to solve was the shared use of a single communications channel. The second major part of this fundamental goal is the interconnection of existing networks. These two sub problems had two very important solutions. Statistical multiplexing, or packet switching, was invented to solve the sharing problem, and the narrow waist was designed to solve the problem of interconnecting networks. Let\u0026rsquo;s talk about each of these now in turn. We\u0026rsquo;ll first talk about packet switching\nPacket Switching In packet switching, the information for forwarding traffic is contained in the destination address of every datagram or packet. Similar to how you would write a letter and specify the destination to where you want the letter sent, and that letter might wend its way through multiple intermediate post offices en-route to the recipient, packet switching works much the same way. There is no state established ahead of time, and there are very few assumptions made about the level of service that the network provides. This assumption about the level of service that the network provides, is sometimes called best effort. So how does packet switching enable sharing? Just as if you were sending a letter, many senders can send over the same network at the same time, effectively sharing the resources in the network. A similar phenomenon occurs in packet switching when multiple senders send network traffic or packets over the same set of shared network links. Now this is in contrast to the phone network, where if you were to make a phone call, the resources for the path between you and the recipient are dedicated and are allocated until the phone call ends. The mode of switching that the conventional phone network uses is called circuit switching, where a signaling protocol sets up the entire path, out-of-band. So this notion of packet switching and statistical multiplexing, allowing multiple users to share a resource at the same time, was really revolutionary. And it is one of the underlying design principles of the internet that has persisted. Now, an advantage of statistical multiplexing of the links and the network means that the sender never gets a busy signal. The drawbacks include things like variable delay and the potential for lost or dropped packets. In contrast, circuit switching provides resource control, better accounting and reservation of resources, and the ability to pin paths between a sender and receiver. Packet switching provides the ability to share resources and potentially better resilience properties.\nPacket Switching vs Circuit Switching Quiz Let\u0026rsquo;s take a quick quiz on packet switching versus circuit switching. Which of the following are characteristics of packet switching and circuit switching: variable delay, busy signals, sharing of network resources like an end-to-end path among multiple recipients, and dedicated resources between the sender and receiver? Each of these options only has one correct answer.\nPacket Switching vs Circuit Switching Solution Variable delay is a property of statistical multiplexing, or packet switching. Circuit switch networks can have busy signals. Packet switch networks share network resources. And circuit switch networks typically have dedicated resources along a path between the sender and receiver\nNarrow Waist Let\u0026rsquo;s now take a look at the second important fundamental design goal on the internet, interconnection, and how interconnection is achieved with the design principle called the Narrow Waist. Let\u0026rsquo;s keep in mind that one of the main goals was to interconnect many existing networks, and to hide the underlying technology of interconnection from applications. This design goal was achieved using a principle called the narrow waist. The internet architecture has many protocols that are layered on top of one another. At the center is an interconnection protocol called IP, or the internet protocol. Now every internet device must speak IP or have an IP stack. Given that a device implements the IP stack, it can connect to the internet. This layer of the network is sometimes called the network layer. Now this layer provides guarantees to the layers above. On top of the network layer sits the transport layer. The transport layer includes protocols like TCP and UDP. The network layer provides certain guarantees to the transport layer. One of those guarantees is end to end connectivity. For example, if a host has an IP address, then the network layer, or IP, provides the guarantee that a packet with that host destination IP address should reach the destination with the corresponding address with best effort. On top of the transport layer sits the application layer. The application layer includes many protocols that various internet applications use. For example, the web uses a protocol called the hypertext transfer protocol or HTTP. And mail uses a protocol called SMTP or simple mail transfer protocol. Transport layer protocols provide various guarantees to the application layer including reliable transport or congestion control. Now below the network layer we have other protocols. The link layer provides point-to-point connectivity, or connectivity on a local area network. A common link layer protocol is Ethernet. Below that, we have the physical layer, which includes protocols such as sonnet or optical networks and so forth. The physical layer is sometimes called layer 1. The link layer is sometimes called layer 2 and the network layer is sometimes called layer 3. We tend to not refer to layers above the network layer by number. The most critical aspect of this design is that the network layer essentially only has one real protocol in use, and that\u0026rsquo;s IP. That means that every device on the network must speak IP, but as long as the device speaks IP it can get on the internet. This is sometimes called IP over anything, or anything over IP, now the advantage to the narrow waist, as I mentioned, is that it is fairly easy to get a device on the network if it runs IP, but the drawback is that because every device is running IP, it\u0026rsquo;s very difficult to make any changes at this layer. However, people are trying to do so, and later in the course, when we discuss software defined networking, we will explore how various changes are being made to both the IP layer, and other layers that surround it.\nGoals: Survivability So we talked about how the internet satisfies the goals of sharing and interconnection and now let\u0026rsquo;s talk about some of the other goals that are discussed in the DARPA Design Philosophy Paper. As we discuss some of these other goals it\u0026rsquo;s worth considering and thinking about how well the current internet satisfies these other design goals in the face of evolving applications, threats, and other challenges. One of the goals discussed is survivability, which states that the network should continue to work if even some devices fail, are comprised, and so forth. There are two ways to achieve survivability. One is to replicate. So one could keep state at multiple places in the network, such that when any node crashes there\u0026rsquo;s always a replica or hot standby waiting to take over for the failure. Another way to design the network for survivability is to incorporate a concept called fate sharing. Fate sharing says that it\u0026rsquo;s acceptable to lose state information for some entity, if that entity itself is lost. For example, if a router crashes all of the state on the router, such as the routing tables, are lost. If we can design the network to sustain these types of failures, where the state of a particular device shares the fate of the device itself, then we can withstand failures better. So fate sharing makes it easier to withstand complex failure scenarios and engineering is also easier. Now it\u0026rsquo;s worth asking whether the current internet still satisfies the principle of fate sharing. In a subsequent lesson, we\u0026rsquo;ll talk about network address translation and how it violates the notion of fate sharing. There are other examples where the current internet\u0026rsquo;s design violates fate sharing and it\u0026rsquo;s worth thinking about those.\nHeterogeneity The internet supports heterogeneity through the TCP/IP protocol stack. TCP/IP was designed as a monolithic transport, where TCP provided flow control and reliable delivery, and IP provided universal forwarding. Now it became clear that not every application needed reliable, in-order delivery. For example, streaming voice and video often perform well, even if not every packet is delivered. And the domain name system, which converts domain names to IP addresses, often also doesn\u0026rsquo;t need completely reliable, in-order delivery. Fortunately, the narrow waste of IP allowed the proliferation of many different transport protocols, not just TCP. The second way that the internet\u0026rsquo;s design accommodates Heterogeneity is through a best-effort service model, whereby the network can lose packets, deliver them out of order, and doesn\u0026rsquo;t really provide any quality guarantees. It also doesn\u0026rsquo;t provide information about failures, performance, et cetera. On the plus side, this makes for a simple design, but it also makes certain kinds of debugging and network management more difficult.\nDistributed Management Another goal of the internet was distributed management. And there are many examples where distributed management has played out. In addressing, we have routing registries. For example, in North America we have ARIN, or the American Registry for Internet Numbers. And in Europe that same organization is called RIPE. DNS allows each independent organization to manage its own names and BGP allows each independently operated network to configure its own routing policy. This means that no single entity needs to be in charge and thus allows for organic growth and stable management. On the downside, the internet has no single owner or responsible party. And as Clark said, some of the most significant problems with the internet relate to the lack of sufficient tools for distributed management, especially in the area of routing. In such a network where management is distributed it can often be very difficult to figure out who or what is causing a problem, and worse, local action such as misconfiguration in a single local network can have global effects. The other three design goals that Clark discusses are cost effectiveness, ease of attachment, and accountability. It\u0026rsquo;s reasonable to argue that the network design is fairly cost effective as is and current trends are aiming to exploit redundancy even more. For example, we will learn about content distributions and distributed web caches that aim to achieve better cost effectiveness for distributing content to users. Ease of attachment was arguably a huge success. IP is essentially plug and play. Anything with a working IP stack can connect to the internet. There\u0026rsquo;s a really important lesson here, which is that if one lowers the barrier to innovation, people will get creative about the types of devices and applications that can run on top of the internet. Additionally, the narrow waist of IP allows the network to run on a wide variety of physical layers ranging from fiber, to cable, to wireless and so forth. Accountability, or the ability to essentially, bill, was mentioned in some of the early papers on TCP/IP but it really wasn\u0026rsquo;t prioritized. Datagram networks can make accounting really tricky. Phone networks had a much easier time figuring out how to bill users. Payments and billing on the internet are much less precise, and we\u0026rsquo;ll talk about these more in later lectures.\nWhat\u0026rsquo;s Missing It\u0026rsquo;s also worth noting what\u0026rsquo;s missing from Clark\u0026rsquo;s paper. There\u0026rsquo;s no discussion of security. There\u0026rsquo;s no discussion of availability. There\u0026rsquo;s no discussion of mobility or support for mobility. And there\u0026rsquo;s also no mention of scaling. There are probably a lot of other things that are missing and it\u0026rsquo;s worth thinking about on your own, some of the other things that current internet applications demand, that are not mentioned in Clark\u0026rsquo;s original design paper.\nDARPA Paper Quiz So as a quick quiz, can you quickly check all of the design goals in the list that were mentioned in Clark\u0026rsquo;s original design goals paper? Security, support for heterogeneity, support for interconnection, support for sharing and support for mobility.\nDARPA Paper Solution Clark\u0026rsquo;s original design goals, paper, mentions the need to support heterogeneity, interconnection and sharing.\nEnd-to-End Argument In this lesson, we\u0026rsquo;ll cover the End-to-End Argument as discussed in the paper, End-to-End Arguments in System Design by Saltzer, Reed, and Clark in 1981. In a nutshell, the End-to-End Argument reads as follows, \u0026ldquo;The function in question can completely and correctly be implemented only with the knowledge and application standing at the end points of the communication system. Therefore, providing that questioned function as a feature of the communication system itself is not possible.\u0026rdquo; Essentially, what the argument says is that the intelligence required to implement a particular application on the communication system should be placed at the endpoints, rather than in the middle of the network. Commonly used examples of the end-to-end argument include error handling and file transfer, encrypting end-to-end versus hop-by-hop in the network, and the partition of TCP and IP of error handling, flow control, and congestion control. Sometimes the end-to-end argument is summarized as, \u0026ldquo;the network should be dumb and minimal and the end points should be intelligent.\u0026rdquo; Many people argue that the end- to-end argument allowed the internet to grow rapidly, because innovation took place at the edge in applications and services, rather than in the middle of the network, which can be hard to change sometimes. Let\u0026rsquo;s look at one example of the end-to-end argument, error handling in file transfer.\nFile Transfer Let\u0026rsquo;s suppose that computer A wants to send a file to computer B. The file transfer program on A asks the file system to read the file from the disk. The communication system then sends the file, and finally the communication system sends the packets. On the receiving side, the communication system gives the file to the file transfer program on B, and that file transfer program asks to have the file written to disk. So what can go wrong in this simple file transfer setup? Well, first, reading and writing from the file system can result in errors. There may be errors in breaking up and reassembling the file. And, finally, there may be errors in the communication system itself. Now, one possible solution is to ensure that each step has some form of error checking, such as duplicate copies, redundancy, time out and retry, so forth. One might even do packet error checking at each hop of the network. One could send every packet three times. One might acknowledge packet reception at each hop along the network. But the problem is that none of these solutions are complete. They still require application level checking. Therefore it may not be economical to perform redundant checks at different layers and at different places of this particular operation. Another possible solution is an end-to-end check and retry where the application commits or retries based on the check sum of the file. If errors along the way are rare, this will most likely finish on the first try. Now, this is not to say that we shouldn\u0026rsquo;t take steps to correct errors at any one of these stages. Error correction at lower levels can sometimes be an effective performance booster. And the trade off here is based on performance, not correctness. So whether or not one should implement additional correctness checks at these layers depends on whether or not the amount of effort put into the reliability gains are worth the extra trouble. Another example where the intend argument applies is with encryption, where keys are maintained by the end applications, and cipher text is generated before the application sends the message across the network. Now one of the key questions in the end-to-end argument is identifying the ends. The end-to-end argument says that the complexity should be implemented at the ends but not in the middle, but the ends may vary depending on what the application is. So for example, if the application or protocol involves Internet routing, the ends may be routers, or they might be ISPs. If the application or protocol is a transport protocol, the ends might be end hosts. So, identifying the ends in the end-to-end argument is always a thorny question that you have to answer first.\nEnd-to-End Argument Violations Now, when talking about the end-to-end argument, it is worth remembering that the end-to-end argument is just that. It\u0026rsquo;s an argument. Not a theorem, or a principle, or a law. And there are many things that have come to violate the end-to-end principle. Network address translators, which we\u0026rsquo;ll talk about in the next lesson, violate the end-to-end argument. VPN tunnels, which tunnel traffic between intermediate points on a network, violate the end-to-end argument. Sometimes TCP connections are split at an intermediate node along an end-to-end path, particularly when the last hop of the end-to-end path is wireless. This is sometimes done to improve the performance of the connection because loss on the last hop lossy wireless hop may not necessarily reflect congestion, and we don\u0026rsquo;t necessarily want TCP to react to losses that are not congestion related. Even spam, in some sense, is a violation of the end-to-end argument. For e-mail the end user is generally considered to be a human, and by the end-to-end argument, the network should deliver all mail to the user. Does this mean that spam control mechanisms are in violation of end-to-end, and if so are these violations appropriate? What about peer to peer systems where files are exchanged between two nodes on the Internet but are assembled in chunks that are often traded among peers? What about caches, and in-network aggregation? So, when considering the end-to-end argument, it\u0026rsquo;s worth asking whether or not the argument is still valid today and in what cases. There are questions about what\u0026rsquo;s in versus out, certainly, and what functions belong in the dumb minimal network. For example, routing is currently in the dumb minimal network. Do we really believe that it belongs? What about multicast? Mobility quality of service? What about NAT\u0026rsquo;s? And it\u0026rsquo;s worth considering whether the end-to-end argument is constraining innovation of the infrastructure by preventing us from putting some of the more interesting or helpful functions inside the network. In the third course, we will talk about software defined networking, which in some sense reverses many aspects of this end-to-end argument.\nViolation NAT Part 1 A fairly pervasive violation of the end-to-end argument are home gateways, which often perform something called network address translation. Now on a home network we have many devices that connect to the network, but when we buy service from our internet service provider we\u0026rsquo;re typically only given one public IP address. And yet we have a whole variety of devices that we may want to connect. Now the idea behind network address translation is that we can give each of these devices a private IP address and there are designated regions of the IP address space that are for private IP addresses. One of those is 192.168.0.0/16 and there are others, which you can go read about in RFC 3130. Each one of these devices in the home gets its own private IP address. The public internet, on the other hand, sees a public IP address which typically is the IP address provided by the internet service provider. When packets traverse the home router, which is often running a network address translation process, the source address of every packet is rewritten to the public IP address. Now when traffic comes back to that public IP address, the network address translator needs to know which device behind the NAT the traffic should be sent to. So it uses a mapping of port numbers to identify which device the return traffic should be sent to in the home network. So the NAT or the network address translator maintains a table that says packets with the source IP address of 192.168.1.51 and source port 1000 should be rewritten to a source address of the public IP address and a source port of 50878. Similarly, packets with a source IP address of 192.168.1.52 and source port of 1000 should be rewritten to the public IP address and a source port of 50879. Then when traffic returns to the NAT to one of these addresses the NAT knows that it needs to rewrite the destination address on the return traffic to the appropriate destination IP address and port that\u0026rsquo;s in the private network. So for outbound traffic, the NAT device creates a table entry mapping the computer\u0026rsquo;s local IP address and port number to the public IP address at a different port number and replaces the sending computer\u0026rsquo;s non-routable IP address with the gateway or the NAT public IP address. It also replaces the sender\u0026rsquo;s source port with a different source port that allows it to de-mutiplex the packets sent to this return address and port. For inbound traffic to the home network, the NAT checks the destination port on the packet, and based on the port, it rewrites the destination IP address and port to the private IP address in the table before forwarding the traffic to a local device in the home network.\nViolation NAT Part 2 Now the NAT clearly violates the end-to-end principle, because machines behind the NAT are not globally addressable, or routable, and other hosts on the public Internet cannot initiate inbound connections to these devices behind the NAT. Now there are ways to get around this, there\u0026rsquo;re various protocols. One is called STUN, or signaling and tunneling through UDP-enabled NAT devices. And in these types of protocols, the device sends an initial outbound packet somewhere, simply to create an entry in the NAT table and once that entry is created we now have a globally routable address and port to which devices on a public Internet can send traffic. Now these devices somehow have to learn that public IP address and port that corresponds to that service and this might be done using DNS for example. It\u0026rsquo;s also possible to statically configure these tunnels or mappings on your NAT device at home. Needless to say, even with these types of hacks and workarounds for NAT, it\u0026rsquo;s clear that network address translation is a violation of the end-to-end principle because by default two hosts on the Internet, one on the home network and one on the public Internet, cannot communicate directly by default.\n"
},
{
	"uri": "/7646/statistical-analysis-of-time-series-data/",
	"title": "Statistical Analysis of Time Series Data",
	"tags": [],
	"description": "",
	"content": " Statistical Analysis of Time Series Data Pandas makes it simple to perform statistical analysis on dataframes, which is extremely important in determining different indicators and acting as inputs to the learning algorithms.\nGlobal statistics For example, if you had a dataframe df1 which had the closing prices for various stocks over a given time period, you can retrieve an ndarray with the mean of the columns by just calling df1.mean().\nFigure 2.1: Example output array for mean() (called on closing prices for January 2010 through December 2012)\nIn addition to mean, there around 32 other global statistics that are available in pandas.\nRolling statistics Instead of doing analysis on the entire dataset, you might want to do a rolling analysis, which only looks at certain snapshots of the data to sample. For example, you could have a 20-day moving mean, which you would calculate day-by-day by averaging the last 20 days’ data. In later sections, this moving average will be explained in more detail, but some critical points of interest are when the moving average crosses the data.\nBollinger bands Some analysts believe that significant deviations from the moving mean will result in movement back towards the mean. If the price dips far below the mean, then it might be a buy signal, whereas if it goes too high, it could indicate a time to sell. Bollinger bands are a way of measuring this deviation.\nBollinger observed that if you look at the volatility of the stock, and if it’s too large, then you discard the movements above and below the mean, but if it’s not, then it might be worth paying attention to.\nWhat he did was place two new moving means, one $2\\sigma$ above, and another $2\\sigma$ below the moving average. If you look at deviations near to $2\\sigma$, then they’re worth paying attention to. If the price drops below $2\\sigma$, and then rises back up through it, then it could be a buy signal. (the price is moving back towards the average).\nConversely, if the price rises above $2\\sigma$, then falls back down, it could be a sell signal.\nComputing rolling statistics in pandas Pandas provides some methods to easily calculate rolling mean (rolling mean()) and rolling standard deviation (rolling_std()).\nExample: Calculating a 20-day rolling mean\nrm_SPY = pd.rolling_mean(df['SPY'], window=20)  The Bollinger bands are calculated as follows:\ndef get_bollinger_bands(rm, rstd): return rm + 2*rstd, rm - 2*rstd  Daily returns Daily returns are how much a stock’s price went up or down on a given day. They are an extremely important statistic as they can be a good comparison between different stocks.\n$dailyReturn[t] = \\dfrac{price[t] - price[t - 1]}{price[t - 1]} = \\dfrac{price[t]}{price[t - 1]} - 1$  daily_ret = (df / df.shift(1).values) - 1 daily_ret.ix[0,:] = 0  Cumulative Returns Cumulative return is calculated by finding the gain from the beginning of the range to the current time, i.e.\n$CumlativeReturn[t] = \\dfrac{price[t]}{price[0]} - 1$  For example, if the price at the beginning was \\$125, and the current price is \\$142, then the gain/cumulative return is $\\dfrac{142}{125} - 1 = .136 = 13.6\\%$\nCumulative returns are essentially the original dataset normalized.\nIncomplete data People assume that financial data is extremely well-documented and that perfect data is recorded minute by minute. They also believe that there are no gaps or missing data points. However, for any particular stock, it might have different prices on different stock exchanges! It’s difficult to know who’s right all the time. Also, not all stocks trade every day (they might suddenly start trading or stop trading).\nYou might think you can just interpolate the data between breaks, but that’d cause statistical errors and a side-effect of ”looking into the future” when doing analysis on that subset of data. The better way of doing it to minimize error is to fill forward and backwards.\nFilling To fix the ”NaN”/empty data, you can use filling to maintain the last known value until known data is reached. For example, if you had a stock that didn’t have data until 2001 and then stopped having data in 2006 but then started having data again in 2012, you could fill forward from 2006-2012 and then fill backwards from 2001 back to whenever you want your data to start.\nExample: Filling in missing data using fillna()\ndf = get_data(symbols, dates) df.fillna(method=\u0026quot;ffill\u0026quot;, inplace=True) df.fillna(method=\u0026quot;bfill\u0026quot;, inplace=True)  Histograms and scatter plots It’s difficult to draw conclusions directly from daily returns plots, so histograms make it easier to see what’s going on. A histogram allows you to see how many occurrences of each return happens relative to other returns. This histogram typically follows a Gaussian over large periods of time.\nHistograms From the histogram we can determine a few key statistics: mean, standard deviation, and kurtosis. Kurtosis is a measure of how close the curve is to a Gaussian. In stock data, there are usually more occurrences at high deviations (causing sort of ”fat tails”), which would be reflected as a positive kurtosis. Skinny tails would mean a negative kurtosis.\nExample: Getting a histogram The histogram above was generated by just calling hist() on the daily returns dataframe as such:\ndaily_returns.hist(bins=20)  The bins parameter is essentially the resolution of the histogram. The domain is divided into 20 bins and anything within those bins counts for that bin’s count in the histogram. Other statistics like mean and standard deviation are easily calculated:\nmean = daily_returns['SPY'].mean() print \u0026quot; mean =\u0026quot;, mean std = daily_returns['SPY'].std() print \u0026quot;std deviation =\u0026quot;, std  Which outputs:\nmean = 0.000509326569142 std deviation = 0.0130565407719  We can now plot the mean and standard deviations on the plot to make analysis easier:\nplt.axvline(mean, color='w', linestyle='dashed', linewidth=2) plt.axvline(mean+std, color='r', linestyle='dashed', linewidth=2) plt.axvline(mean-std, color='r', linestyle='dashed', linewidth=2)  Showing this:\nprint daily_returns.kurtosis() \u0026gt;\u0026gt;\u0026gt; SPY 3.376644  which means that the data has fat tails since it’s positive.\nThe utility of these histograms comes when plotting them together. It’s easy to compare multiple stocks in terms of their returns and volatility. If stock A’s curve is skewed more positive and is thinner than stock B, then it has a low volatility with higher returns vs stock B.\nBy looking at this chart, you can see that SPY and XOM are about the same in volatility\n$\\sigma_{SPY} = 0.013057$, $\\sigma_{XOM} = 0.013647$. However, SPY would have higher returns since $R_{SPY} = 0.000509$ whereas $R_{XOM} = 0.000151$\nScatter plots Scatter plots are another way of visualizing the correlation between two stocks. Say you had a dataframe with the daily returns for SPY and XYZ. If you took just the ndarray containing the y-axis values, and then plotted SPY on the x-axis and XYZ on the y-axis, you would see a bunch of points that might have a certain trend.\nIf you take a linear regression of this data, the slope would be called the beta ($\\beta$) value. If the $\\beta$ value for SPY and XYZ is 1, it means that, on average, if SPY (the market) moves up by 1%, then XYZ also moves up by 1%.\nThe y-intercept of the line is called $\\alpha$. It describes how the stock on the y-axis performs with respect the stock on the x-axis. If the $\\alpha$ value of XYZ with respect to SPY is positive, then, on average, XYZ is returning more than the market overall.\nCorrelation: If there isn’t any correlation in the dataset, then the linear regression doesn’t tell you anything about the relationship. A common method for calculating the correlation is by finding the sample Pearson correlation coefficient, $r_{xy}$. It’s calculated by the following:\n$r_{xy} = \\dfrac{cov(X, Y)}{\\sigma_X\\sigma_Y} = \\dfrac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 }\\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}$  where $cov$ is the covariance. In this case, $X$ would be the daily return for SPY and $Y$ would be the daily return for XYZ. If $|r X,Y| = 1$, then the two are perfectly correlated (either positively or negatively, depending on the sign of $\\rho$). If $|r| \u0026lt; 1$, then there is possible correlation, but a value closer to 1 means better correlation. If $r = 0$, there is no correlation.\nExample: Plotting scatter plots, getting $\\alpha$ and $\\beta$ values, and determining correlation\n# scatter plot for XOM vs SPY daily_ret.plot(kind='scatter', x='SPY', y='XOM', title=\u0026quot;Scatterplot\u0026quot;) beta_XOM,alpha_XOM = np.polyfit(daily_ret['SPY'],daily_ret['XOM'],1) plt.plot(daily_ret['SPY'], beta_XOM*daily_ret['SPY'] + alpha_XOM,'-',color='r') plt.show() # scatterplotforGLDvsSPY daily_ret.plot(kind='scatter', x='SPY', y='GLD', title=\u0026quot;Scatterplot\u0026quot;) beta_GLD, alpha_GLD = np.polyfit(daily_ret['SPY'], daily_ret['GLD'], 1) plt.plot(daily_ret['SPY'], beta_GLD*daily_ret['SPY'] + alpha_GLD, '-', color='r') plt.show() print \u0026quot;BetaXOM: \u0026quot;, beta_XOM print \u0026quot;AlphaXOM: \u0026quot;, alpha_XOM print \u0026quot;BetaGLD: \u0026quot;, beta_GLD print \u0026quot;AlphaGLD: \u0026quot;, alpha_GLD # calculate correlation using pearson method print \u0026quot;Correlationmatrix:\\n\u0026quot;, daily_ret.corr(method='pearson')  Which results in:\nBeta XOM: 0.85753872112 Alpha XOM: -0.000285580653638 Beta GLD: 0.0663816850634 Alpha GLD: 0.000660583984316 Correlation matrix: SPY XOM GLD SPY 1.000000 0.820423 0.074771 XOM 0.820423 1.000000 0.079401 GLD 0.074771 0.079401 1.000000  Looking at the $\\beta$ values, you can see that XOM is more responsive to market changes, while GLD is relatively unresponsive. However, GLD tends to perform better than the market on average, since its $\\alpha$ is positive.\nBut these values are meaningless without seeing what their correlations are. Looking at the correlation matrix, XOM is pretty well correlated with SPY, whereas GLD has a very low correlation, so changes GLD aren’t really correlated with changes in the market.\nLooking at the plots, it’s easy to see that the points for XOM are more correlated and match the line better than do those of GLD.\nSharpe ratio and other portfolio statistics The portfolio is the collection of all stocks currently owned by a person. It’s important to know various statistics associated with the portfolio to make informed decisions on what to sell/buy.\nSuppose you begin with a portfolio p consisting of the following parameters:\nstart_val = 1000000 start_date = '2009-01-01' end_date = '2011-12-31' symbols = ['SPY','XOM','GOOG','GLD'] allocs = [0.4,0.4,0.1,0.1] # @ beginning , 40% to SPY , 40% to XOM, etc  Now suppose we want to find the value of this portfolio day-by-day. If we normalize the portfolio dataframe, we essentially have a dataframe containing cumulative returns for each index. If we multiply this by allocs, we get returns scaled by each percentage of the total portfolio. Then, multiply by start val to get each stock’s total value. Finally, take the sum of this penultimate dataframe to get a single-column dataframe with the total portfolio value at each point in time. In Python,\n# get cumulative returns df = get_data(symbols, pd.date_range(start_date,end_date)) df = normalize(df) # get changes for each stock by their percentages of the starting value alloced = df * allocs # get dollar value of changes vals = alloced * start_val # sum to get total value portfolio_value = vals.sum(axis=1)  We may now compute various statistics on the portfolio’s value.\n Daily returns: Obviously, daily returns of the entire portfolio would be an important statistic, as they indicate how the portfolio changes over time. For some statistics, we need to get rid of the 0 at the beginning of the daily return or else it’ll throw off the values.  daily_rets = daily_rets[1:]   Cumulative returns: The total cumulative return of the portfolio is another interesting statistic, as you can see if the overall gain was positive or negative.  cum_ret = (port_val[-1]/port_val.ix[0,:]) - 1   Avg. and Std. Deviation: These two are the main statistics that get thrown off by the 0 at the beginning. If it were there, the mean would be closer to 0, even though technically 0 isn’t actually one of the returns.  avg_daily_ret = daily_rets.mean() std_daily_ret = daily_rets.std()  Sharpe Ratio The Sharpe Ratio is a metric that adjusts return for risk. It enables a quantitative way to compare two stocks in terms of their returns and volatility. The Sharpe Ratio is calculated based on the assumption that, Ceteris paribus,\n Lower risk is better Higher return is better  Being an economic indicator, it also takes into account the opportunity cost/return of putting the money in a risk-free asset such as a bank account with interest. A sort of risk-adjusted return may be calculated as follows:\n$R_{adj} = \\dfrac{R_p-R_f}{\\sigma_p}$  where $R_p$ is the portfolio return, $R_f$ is the risk-free rate of return, and $\\sigma_p$ is the volatility of the portfolio return.\nThis ratio is a sort of basis for how the Sharpe Ratio is calculated. The Sharpe Ratio is as follows:\n$S = \\dfrac{E[R_p-R_f]}{std(R_p - R_f)}$  Since we’re looking at past data, the expected value is actually the mean of the dataset, so this becomes:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{std(R_p - R_f)}$  One question is where $R_f$ comes from. There are three main ways of getting the data for the risk-free rate:\n The London Inter-Bank Offer Rate (LIBOR) The interest rate on the 3-month Treasury bill 0% (what people have been using recently\u0026hellip;)  LIBOR changes each day, and the Treasury bill changes slightly each day, but interest in bank accounts are typically paid in 6-month or yearly intervals. Using this simple trick, you can convert the annual/biannual amount to a daily amount:\nSuppose the yearly interest rate is $I$. If we start at the beginning of the year with a value $P$, the new value after interest is paid will be $P\u0026rsquo;$. To find the equivalent daily interest value, $I_{eq}$,\n$P\u0026#39; = P(1 \u0026#43; I_{eq})^{252}$  $P(1 \u0026#43; I) = P(1 \u0026#43; I_{eq})^{252}$  $1 \u0026#43; I = (1 \u0026#43; I_{eq})^{252}$  $(1 \u0026#43; I_{eq}) = \\sqrt[252]{1\u0026#43;I}$  $I_{eq} = \\sqrt[252]{1\u0026#43;I} - 1$ \nTherefore, $R_f$, the daily risk-free rate, is just $\\sqrt[252]{1 + I} - 1$. The reason it’s 252 instead of 365 is because there are only 252 trading days in a year.\nSince we’re treating $R_f$ as constant, the standard deviation in the denominator just becomes $std(R_p)$, so the final equation for the Sharpe Ratio becomes:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{\\sigma_{R_p}}$  Sampling rate The Sharpe Ratio can vary widely depending on the sampling frequency. Since $SR$ is an annual measure, any calculations that are done with samples more frequent than yearly need to be scaled to get the annual ratio. To adjust the calculated Sharpe Ratio to be ”annualized”, you just multiply by a factor of $\\sqrt{\\textrm{#samples per year}}$. So if you sample daily, the Sharpe Ratio would become:\n$S = \\dfrac{\\overline{(R_p - R_f)}}{\\sigma_{R_p}} \\sqrt{252}$  Example: Given 60 days of data with the following statistics:\n $R_p = 10bps$ $R_f = 2bps$ $\\sigma R_p = 10bps$,  what is the Sharpe Ratio? One bps is one hundredth of a percent.\nOptimizers An optimizer can: - Find minimum/maximum values of functions - Build parameterized models based on data - Refine allocations to stocks in portfolios For example, say you have the function $f(x) = (x - 1.5)^2 + .5$, and you want to find the minimum. It’s trivial to use calculus and find the minimum analytically, but you can’t always do so if you don’t have an analytical model of the data. Let’s put this in Python:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import scipy . optimize as spo def f(x): y = (x - 1.5)**2 + .5 print \u0026quot;x = {}, y = {}\u0026quot;.format(x, y) return y def test_run(): guess = 2.0 min_result = spo.minimize(f, guess, method='SLSQP', options={'disp': True}) print \u0026quot; minima found at:\u0026quot; print \u0026quot;x = {}, y = {}\u0026quot;.format(min_result.x, min_result.fun) if __name__ == \u0026quot; __main__ \u0026quot;: test_run()  outputs:\nx = [2.], y = [0.75] x = [2.], y = [0.75] x = [2.00000001], y = [0.75000001] x = [0.99999999], y = [0.75000001] x = [1.5], y = [0.5] x = [1.5], y = [0.5] x = [1.50000001], y = [0.5] Optimization terminated successfully. (Exit mode 0) Current function value: [0.5] Iterations: 2 Function evaluations: 7 Gradient evaluations: 2 minima found at: x = [1.5], y = [0.5]  Pitfalls Optimizers aren’t perfect, and since the method used above uses the gradient of the current point to move to the next point, it can be tripped up by various abnormalities in the function it’s trying to minimize, such as:\n Flat ranges: If a portion of the graph is flat (the slope is close to or is 0), then the solver will either take a lot of iterations to solve for the minimum or it might not ever be able to move to a new point, unless it can find a way out. Discontinuities: If there are discontinuities, the gradient might not be defined well enough for the solver to continue. Multiple minima: Say you have a function $f(x) = x^4 − 2x^2 + x^2$. This function has 2 minima at $(0, 0)$ and $(1, 0)$. If the solver starts at $x = 1.5$, it’ll find the minimum at $(1, 0)$, but it won’t ever reach the other minimum. Conversely, if the solver starts at $x = −1.5$, it’ll find the minimum at $(0, 0)$. Therefore, it’s easy to get trapped in a local minimum that may not be the actual global minimum.  Convex problems A real-valued function $f(x)$ defined on an interval is called convex if the line segment between any two points on the graph of $f(x)$ on that interval lies above the graph. Otherwise, it’s called non-convex.\nLeft: a convex function. Right: a non-convex function. It is much easier to find the bottom of the surface in the convex function than the non-convex surface. (Source: Reza Zadeh)\nBuilding a parameterized model If you have a set of data points representing rainfall and humidity that were gathered, you might want to find a function that best fits those points. Say you wanted to fit a line $f(x) = mx + b$ to the points. In this case, you can use linear algebra and find the leastsquares solution, but you can also use an optimizer to find the best parameters $m$ and $b$. What does ”best” mean? Well, we can devise a measure for the error for each point:\n$e_i = (y_i - f(x_i))^2 = (y_i - (mx_i \u0026#43; b))^2$  which is just the difference between the actual value and our model’s predicted value. The reason it’s squared is to ensure that negative errors don’t reduce the total error when we sum up every $e_i$.\n$E = \\sum_{i=1}^{n} (y_i - (mx_i \u0026#43; b))^2$  Now that we have what we want to minimize, $E$, we can use a minimizer to find the best $m$ and $b$. To make the parameters nicer to work with in Python (and allow generalization to higher degrees of polynomials), we’ll rename $m$ and $b$ to $C_0$ and $C_1$. Now, $f(x) = C_0x+C_1$.\nFor example in python:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import scipy . optimize as spo # line is a tuple (C0 , C1) def error(line, data): return np.sum((data[:, 1]-(line[0]*data[:, 0]+line[1]))**2) def fit_line(data, error_func): # initial guess for parameters l = np.float32([0, np.mean(data[:, 1])]) return spo.minimize(error_func, l, args=(data,), method='SLSQP', options={'disp': True}).x def test_run(): original = np . float32([4, 2]) print \u0026quot; original line : C0 = {} , C1 = {}\u0026quot;. format(original[0], original[1]) Xoriginal = np . linspace(0, 10, 40) Yoriginal = original[0] * Xoriginal + original[1] plt.plot(Xoriginal, Yoriginal, 'b--', linewidth=2.0, label=\u0026quot;Originalline\u0026quot;) # add some random noise to the data noise_sigma = 4.0 noise = np.random.normal(0, noise_sigma, Yoriginal.shape) data = np.asarray([Xoriginal, Yoriginal+noise]).T plt.plot(data[:, 0], data[:, 1], 'go', label=\u0026quot;Data points\u0026quot;) l_fit = fit_line(data, error) print \u0026quot; Fitted line : C0 = {}, C1 = {}\u0026quot;.format(l_fit[0], l_fit[1]) plt.plot(data[:, 0], l_fit[0]*data[:, 0] + l_fit[1], 'r- -', linewidth=2.0, label=\u0026quot; Fitted line \u0026quot;) plt.legend(loc='upperright') plt.show() if __name__ == '__main__': test_run()  Portfolio Optimization Now that wee have the tools to optimize a function, we can use it to optimize our portfolio! We can choose to optimize/minimize/maximize various measures, such as daily returns, cumulative returns, or Sharpe Ratio based on the percent allocation of all of the stocks in the portfolio.\nFraming the problem First we need three things:\n a function, $f(x)$, to minimize an initial guess for $x$ the optimizer  In our case, $x$ is actually the set of allocations for the stocks. Also, since we want to maximize Sharpe Ratio, we need to multiply $f(x)$ by $-1$ to call the minimizer.\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/7646/",
	"title": "7646",
	"tags": [],
	"description": "",
	"content": " About: This course is part of the OMSCS ML specialization and is taught by the Quantitative Software Research Group at Georgia Tech. It covers pythons and introductory numerical computing, computational investing, and applied machine learning.\nResources:  Course website Calendar Software Setup Q\u0026amp;A Study Guide    Credit: much of this section comes from Ryan Babaie \u0026amp; Neil Hardy\u0026#39;s guide. Download it here:   ml4t_guide.pdf  (1401 ko)    "
},
{
	"uri": "/6250/switching/",
	"title": "Switching",
	"tags": [],
	"description": "",
	"content": " Switching At its core, a network serves to route packets between machines on the network. Let\u0026rsquo;s take a look at how packets are moved across networks. It\u0026rsquo;s more complicated than it sounds at first, but quite fascinating.\nThat\u0026rsquo;s right. To even reach your screen, the packets that make up this video likely traveled across at least four or five networks, if not more.\nYou\u0026rsquo;ll learn how that works in the routing videos on BGP, a routing protocol.\nSwitching and Bridging In this lesson, we will learn about switching and bridging. In particular, we will learn about how hosts find each other on a subnet and how subnets are interconnected. We will also learn about the difference between switches and hubs, and the difference between switches and routers. And we\u0026rsquo;ll talk about the scaling problems with Ethernet and mechanisms that can be used to allow it to scale better.\nBootstrapping Networking Two Hosts To start let\u0026rsquo;s talk about how you would network two machines, each with a single interface, to each other. So Host 1 and Host 2 would be connected by two Ethernet adapters or network interfaces. And each of these would have a LAN, or physical, or MAC address. Now a host that wants to sent a datagram to another host can simply send that datagram via its Ethernet adapter with a destination MAC address of the other host that it wants to receive the frame. Frames can also be sent to a broadcast destination MAC address which would mean that the datagram would be sent to every host that it was connected to on the local area network. Now, of course, typically what happens is a host knows a DNS name or an IP address of another host, but it may not know the hardware or MAC address of the adapter on the host that it wants to send it\u0026rsquo;s datagram to. So we need to provide a way for a host to learn the MAC address of another host. The solution to this is a protocol called ARP or the address resolution protocol.\nARP: Address Resolution Protocol In ARP, a host queries with an IP address, broadcasting that query to every other node on the network. That query will be of a form, \u0026ldquo;who has a particular IP address,\u0026rdquo; such as 130.207.160.47, and that particular host who has that IP address on the LAN will respond with the appropriate MAC address. So the ARP query is a broadcast that goes to every host on the LAN from the host that wants the answer to the query and the response is a unicast response with the MAC address as the answer. That\u0026rsquo;s returned to the host that issued the query. When the host that issues the query receives a reply, it starts to build what\u0026rsquo;s called an ARP table. It\u0026rsquo;s ARP table then maps each IP address on the local area network to the corresponding MAC address. Now, instead of broadcasting a ARP query to discover the MAC address corresponding with this IP address, the host can simply consult its local ARP table.\nLet\u0026rsquo;s now take a look at what the host does with this information. When the host wants to send a packet to the destination with a particular IP address. It takes that IP packet and encapsulates it in an Ethernet frame with the corresponding destination MAC address. Essentially, it puts that IP packet inside of an Ethernet frame. So before it sends the IP packet with that destination IP address, it first puts the packet inside a larger Ethernet frame with its own source MAC address and the destination MAC address from its local ARP table.\nARP Quiz So let\u0026rsquo;s consider what we learned about ARP. So what are the formats of the queries and responses in ARP? Is the query a broadcast where a host is asking about an IP address, and the response is a unicast with a MAC address? Is the query a unicast message asking about an IP address and the response is broadcast with a MAC address? Or is the query a broadcast asking about a particular MAC address, where the response is a unicast with the response of a particular IP address?\nARP Solution The purpose of ARP is to allow a host to discover the MAC address corresponding to a particular IP address. And the host doesn\u0026rsquo;t know which host on the LAN owns that particular MAC address. So, ARP allows the host to send a broadcast query asking about who owns a particular IP address. And the response comes from the owner of that particular IP address and the response is the MAC address.\nInterconnecting LANs with Hubs The simplest way that a LAN can be connected is with something called a hub. Hubs are the simplest form of interconnection and in some sense they don\u0026rsquo;t even exist in networks anymore today, because you can build a switch for essentially the same price. But for the sake of example, let\u0026rsquo;s just take a look at how a LAN would be connected with a Hub. Now, a hub essentially creates a broadcast medium among all of the connected hosts where all packets on the network are seen everywhere. So if a particular host sends a frame that\u0026rsquo;s destined for some other host on the LAN, then a hub will simply broadcast that frame that it receives on an incoming port out every outgoing port. So all packets are seen everywhere. There is a lot of flooding and there are many chances for collision. The chance of collision of course, introduces additional latency in the network because collisions require other hosts or senders to back off and not send as soon as they see the other senders trying to send at the same time. LANs that are connected with hubs are also vulnerable to failures or misconfiguration because even one misconfigured device can cause problems for every other device on the LAN. Suppose that you had a misconfigured device that was sending a lot of rogue or unwanted traffic. Well, on a network that\u0026rsquo;s connected with hubs, every other host on the network would see that unwanted traffic. So, we need a way to improve on this broadcast medium by imposing some amount of isolation.\nSwitches Traffic Isolation So in contrast, switches perform some amount of traffic isolation so that the entire LAN doesn\u0026rsquo;t become one broadcast medium. But instead, we can partition the LAN into separate broadcast domains or collision domains. Now a switch might break the subnet into multiple LAN segments. Typically a frame that is bound for a host in the same part or segment of the LAN is not forwarded to other segments. So, for example if we had a network with three hubs, all connected by a switch, then each of these would be its own broadcast domain. And if a host here wanted to send a frame to another destination in the same segment, well that frame would be broadcast within that domain. But the switch would recognize that the destination was in the same segment and would not forward the packet on output ports destined for other LAN segments where the destination was not. Now enforcing this kind of isolation, requires constructing some kind of switch table, or state, at the switch, which maps destination MAC addresses to output ports.\nLearning Switches Let\u0026rsquo;s take a quick look at how learning switches work. A learning switch maintains a table between destination addresses and output ports on the switch, so that when it receives a frame destined for a particular place it knows what output port to forward the frame. Initially the forwarding table is empty, so if there\u0026rsquo;s no entry in the forwarding table the switch will simply flood. Let\u0026rsquo;s look at a quick example. If host A sends a frame destined for host C, then initially the switch has nothing in its table to determine where that frame should be sent, so it will flood the frame on all of its outgoing ports. On the other hand, because the frame has a source address of A, and arrived on input port one, the switch can now make an association between address A and port one. In other words, it knows that the host with address A is attached to port one, so that in future, when it sees frames destined for host A, it no longer needs to flood, but can instead send the frames directly to port one. So, for example, when C replies with a frame destined for A, the switch now has an entry that tells it that it doesn\u0026rsquo;t need to flood that packet. But instead, can simply send the packet directly to the output port. Note also that when C replies, the switch learns another association between address C and port three. So future frames destined for host C, no longer need to be flooded, either. They can simply be forwarded to output port three. So, in summary, if a learning switch has no entry in the forwarding table, it must flood the frame on all outgoing ports. But otherwise, it can simply send that frame to the corresponding output port in the table. Note that learning switches do not eliminate all forms of flooding. The learning switch must still flood in cases where there is no corresponding entry in the forwarding table, and also, these switches must forward broadcast frames, such as ARP queries. Now because learning switches still sometimes need to flood, we still have to take care when the network topology has loops. Now most underlying physical topologies have loops for reasons of redundancy. If any particular link fails, you\u0026rsquo;d still like hosts on the LAN to remain connected.\nBut let\u0026rsquo;s see what happens when the underlying physical topology has a loop. Let\u0026rsquo;s suppose a host on the upper LAN broadcasts a frame. Each learning switch will hear that frame and broadcast it on all of its outgoing ports. When that broadcast occurs, the other learning switches that are in the topology that contains a loop will hear the rebroadcast. They in turn will not know that they shouldn\u0026rsquo;t rebroadcast the packet that they just heard. So each of those switches will in turn rebroadcast the packet on their outgoing ports. And, of course, this process will continue, creating both packet loops and what are known as broadcast storms. So, cycles in the underlying physical topology can create the potential for learning switches to introduce forwarding loops and broadcast storms. So we need some kind of solution to ensure that even if the underlying physical topology has cycles, which it often needs for redundancy, that the switches themselves don\u0026rsquo;t always flood all packets on all outgoing ports. In other words, we need some kind of protocol to create a logical forwarding tree on top of the underlying physical topology.\nLearning Switches Quiz So, as a quick quiz about learning switches, let\u0026rsquo;s suppose that initially the switch forwarding table is empty and host D sends a frame that is destined for host B. Fill out the entry in the switch forwarding table that is populated as a result of this message.\nLearning Switches Solution When the switch sees the frame from host D, destined for host B, it doesn\u0026rsquo;t know what to do with the frame, so it forwards that frame on all of its output ports. However, because it sees a frame arrive from source D, it knows that future frames that are destined for source D should be output on port four.\nSpanning Trees The solution to this problem is to construct what\u0026rsquo;s called a Spanning Tree, which is a loop-free typology that covers every node in the graph. The set of edges, shown in blue, constitutes what\u0026rsquo;s known as a Spanning Tree. The collection of edges in the blue typology covers every node in the underlying physical typology, and yet, there are no loops in the blue topology. Now, instead of flooding a frame, a switch in this topology would simply forward packets along the spanning tree. So for example, this switch would only send a frame along the port corresponding to the blue edge and would not forward the frame out any edges that were not part of the spanning tree. Other switches that receive the frame, would flood in the same fashion, along all edges that were part of the spanning tree, while omitting edges that were not members of the spanning tree.\nLet\u0026rsquo;s take a look at how to construct the spanning tree. First the collection of switches must elect a root, since every tree must have a root. Typically this is the switch with the smallest ID. In this case, the switch at the top of the topology is the root. Then each switch must decide which of its links to include in the spanning tree. And it excludes any link if that link is determined to be not on the shortest path to the root. For example, let\u0026rsquo;s consider the switch in the lower right. It has three lengths, this length takes it on a path that\u0026rsquo;s three hops from the root. This length takes it on a path that\u0026rsquo;s two hops to the root, and this length takes it on a path, that\u0026rsquo;s one hop to the root. Any link that\u0026rsquo;s not on a shortest path to the route is excluded and any link that\u0026rsquo;s on a shortest path on a route is included. Similarly here, this edge is on a path that\u0026rsquo;s one hop away from the route and this edge is on a path that\u0026rsquo;s two hops away. So this node will include this link from the spanning tree. Now, each switch repeats this process to exclude links from the underlying topology. And ultimately, this yields a forwarding topology that looks like the blue graph. And of course there is an issue which is how do we determine the root in the first place? Well initially, every node think it\u0026rsquo;s the root. And the switches run an election process to determine which switch has the smallest ID. And if they learn of a switch with a smaller ID, they update their view of the root, and they compute the distance to the new root. Whenever a switch updates its view of the root, it also determines how far it is from that root. So that when other neighboring nodes receive those updates they can determine their distance to the new root simply by adding one to any message that they receive.\nSpanning Tree Example Let\u0026rsquo;s take a quick example. Suppose the message format is as follows. Y, d and x, where x is the origin of the message, Y is the node being claimed as root, and d is the distance of the particular node sending this message, x, from a claimed root. So, initially every switch in the network broadcasts a message like x,0,x to indicate that the node that it thinks itself is the root. When other switches hear this message, they compare the ID of the sender to their owner ID, and they update their opinion of who the root is based on the comparison of these IDs. Let\u0026rsquo;s suppose that we have the following graph and switch number 4 thinks it\u0026rsquo;s the root. So we will send a message 4, 0, 4 to nodes 2 and 7. But 2 also thinks it is the root, so 4 is going to receive the message 2,0, from node 2, and then it\u0026rsquo;s going to realize that 4 is just one hop away from node 2. So node 4 will update its view of the root to be node 2. Eventually 4 will also hear a message 2,1,7 from node 7; indicating that node 7 thinks it is one hop away from its view of the root, which is node 2. It will realize that the path through node 7 is a longer path to the root, and it will remove the link 4-7 from the tree. We can repeat this process and ultimately we will end up with a spanning tree.\nSwitches vs Routers Let\u0026rsquo;s do a quick comparison of switches and routers. Switches typically operate at layer two. A common protocol at layer two is Ethernet. Switches are typically automatically configuring, and forwarding tends to be quite fast since packets only need to be processed through layer two on flat look ups. Routers, on the other hand, typically operate at layer three where IP is the common protocol. And router level topologies are not restricted to a spanning tree. One can even have multipath routing, where a single packet could be sent along one of multiple possible paths in the underlying router level topology. So, in many ways Ethernet, or layer two switching, is a lot more convenient, but one of the major limitations is broadcast. The spanning tree protocol messages and ARP queries both impose a fairly high load on the network. So this raises the question of whether it\u0026rsquo;s possible to get many of the benefits of the auto configuration and fast forwarding of layer two without facing these broadcast limitations. As it turns out, there are ways to strike this balance. And in the third part of the course, when we talk about network management, we will look at some ways to scale Ethernet to very large topologies. For example, in data center networks. We\u0026rsquo;ll also explore how an emerging technology called Software Defined Networking, or SDN, is effectively blurring the boundary between the layer two and layer three.\nBuffer Sizing So in this lesson, we\u0026rsquo;ll look at an important question in switch design which is, how much buffering do routers and switches need? It\u0026rsquo;s fairly well known that routers and switches do need packet buffers to accommodate for statistical multiplexing. But it\u0026rsquo;s less clear how much packet buffering is really necessary. Now given that queuing delay is really the only variable part of packet delay on the internet, you\u0026rsquo;d think we\u0026rsquo;d know the answer to this question already. And for quite some time there have been some well understood rules of thumb but it turns out that we\u0026rsquo;ve recently revisited this question and come up with some different answers. So let\u0026rsquo;s first look at the universally applied rule of thumb. Now for the sake of the examples in this lesson, I\u0026rsquo;m going to use routers and switches interchangeably because it doesn\u0026rsquo;t really matter. All that matters here is that we have a network device that\u0026rsquo;s a \u0026lsquo;store and forward\u0026rsquo; packet device that has the capability of storing a frame or a packet and then later sending it on. So let\u0026rsquo;s suppose that we have a path between a source and a destination, and the round-trip propagation delay is 2T and the capacity to bottleneck link is C. Now the commonly held view is that this router needs a buffer of 2T times C. It should be clear why this rule of thumb exists. C is the capacity to the bottleneck link in say, bits per second and T is the time of units second, so this works out to bits, and the meaning of this quantity is simply the number of bits that could be outstanding along this path at any given time. It effectively represents the maximum amount of outstanding data that could be on this path between the source and destination at any time. Now this rule of thumb guideline was mandated in many backbone and edge routers for many years. It appears in RFCs and ITF Architectural guidelines and it has major consequences for router design simply because this can be a lot of router memory and memory can be expensive. The other thing of course is that the bigger these buffers, not only the bigger the cost but also the bigger the queuing delay that could exist at any given router. And hence, the more delay the interactive traffic may experience and the more delay that feedback about congestion will experience. The longer these delays are, the longer it will take for the source to hear about congestion that might exist in the network. Now to understand why this guideline is incorrect, let\u0026rsquo;s first re-derive the rule of thumb a bit more formally and then we\u0026rsquo;ll understand why it does not always apply in practice.\nBuffer Sizing for a TCP Sender Let\u0026rsquo;s suppose that we have a TCP sender that\u0026rsquo;s sending packets, where the sending rate is controlled by the window W, and it\u0026rsquo;s receiving ACKs (acknowledgements). Now at any time if the window is W, only W unacknowledged packets may be outstanding. So the sender\u0026rsquo;s sending rate, R, is simply the TCP window, W, divided by the round trip time (RTT) of the path. So the rate is W over RTT. Now remember that TCP uses additive increase, multiplicative decrease, or AIMD, congestion control. So for every W ACKs received, we send W plus one packets, and our TCP saw tooth will look something like this. We\u0026rsquo;ll start at a rate W_max over 2, increase the window to W_max and then when we see a drop we will apply multiplicative decrease and reduce the sender\u0026rsquo;s sending rate to W_max over 2 again. So here, right at the point of a packet drop, this represents the maximum number of packets that can be in flight. So again, the required buffer is the maximum number of packets that can be in flight, or simply the height of this TCP saw tooth. Now we know the rate is W over RTT, and we\u0026rsquo;d like the sender to send at a common rate, R. And if we\u0026rsquo;d like the sender to be sending at the same rate before and after it experiences a loss, then we know that the rate before the drop must equal the rate after the drop. So then we can set these two rates equal. We know that the RTT is part transmission delay T, and part queuing delay which is the maximum buffer size of the bottleneck link, divided by the capacity of the bottleneck link. We also know that after reducing the window, the queuing delay is zero. So we can replace the term on the left with W_old over 2T plus B over C and we can replace the term on the right with W_old over 2, because the congestion window has been reduced half divided by 2T, simply the propagation delay with no queuing delay. Now if we solve this equation we find that the required buffering is simply 2T times C. Now the rule of thumb makes sense for a single flow, but a router in a typical backbone network has more than 20,000 flows. And it turns out that this rule of thumb only really holds if all of the those 20,000 flows are perfectly synchronized. If the flows are desynchronized, then it turns out that this router can get away, with much less buffering.\nIf TCP Flows are Synchronized Now, if TCP flows are synchronized, the dynamics of the aggregate window as shown in the upper part of the graph, would have the same dynamics as any individual flow. The quantities on the Y axis here would simply be different. Specifically, the number of pockets occupying the buffer would be the sum of all of the TCP flows windows, rather than the window of any individual flow. Now if there are only a small number of flows in the network then these flows may tend to stay synchronized, and the aggregate dynamics might mimic the dynamics of any single flow, as shown. But as the network supports an increasingly large number of flows, these individual TCP flows become de-synchronized. So instead of all of the flows lining up with the saw tooth as shown in the bottom part, individual flows might see peaks at different times. As a result, instead of seeing a huge saw tooth that\u0026rsquo;s the sum of a bunch of synchronized flows, the aggregate instead might look quite a bit more smooth, as a result of the individual flows being desynchronized. And we can represent this sum, which is the buffer occupancy, as a random variable. At any given time, it\u0026rsquo;s going to take a particular range of values. The range of values that this buffer occupancy takes can actually be analyzed in terms of the central limit theorem.\nThe central limit theorem tells us that the more variables that we have, and, in this case the number of variables are the number of unique congestion windows of flows that we have, the narrower the Gaussian will be. In this case, the Gaussian is the fluctuation of the sum of all of the congestion windows. In fact, the width decreases as 1 over root N, where N is the number of unique, congestion windows of flows that we have. And therefore, instead of the required buffering, needing to be 2T times C, we can get away with much less buffering, in particular, 2T times C divided by the square root of N, where N, is the number of flows, passing through the router.\n"
},
{
	"uri": "/7646/essential-economics/",
	"title": "Essential Economics",
	"tags": [],
	"description": "",
	"content": " Essential Economics This chapter discusses terminology, stock market dynamics, and important indicators in economics. This will allow us to more accurately judge the value of an economic decision and make predictions on the market.\nFunds We’ll discuss three different types of funds: Exchange-Traded Fund (ETF), mutual funds, and hedge funds. Different types of funds are governed under different rules. ETFs are similar to stocks in that they are bought and sold at will like stocks- very liquid. However, ETFs typically represent baskets of stocks, and it is known to the trader what the fund represents. Mutual funds can only be bought and sold at the end of the day, and the holdings within a mutual fund are only disclosed every quarter. The least transparent holdings are that of a hedge fund. Before investors can buy shares in the fund, they must sign a long term agreement and holdings are rarely disclosed.\nAbout Funds For stocks and ETFs, having a large ”cap” means that the total value of stocks (number of stocks × price of a stock) in a company is worth many billions of dollars. Moreover, the price of a stock doesn’t reflect the value of a company, but the price at which they are selling shares. ETFs, like stocks, can easily be traded through individuals alone, whereas shares in mutual funds require a broker and hedge fund shares require more of a one on one relationship. Managers of ETFs and mutual funds are compensated based on expense ratios, which denote a percentage of Assets Under Management (AUM). For an ETF, expense ratios range from 0.01% to 1%, and in mutual funds from 0.5% to 3%. Hedge funds follow a ”two and twenty” policy, where managers get 2% of the AUM and 20% of the profits.\nThe type of a fund can be more easily recognized by how it’s named. For example, an ETF has a ticker, or stock symbol, with three or four letters, like AAPL. A mutual fund has five letters, like VTINX, and a hedge fund doesn’t have a ticker because shares are much less liquid. How much money is managed by a fund is known as the AUM, and shares represent percentages of the AUM.\nIt’s fairly clear to see that hedge funds are very different from ETFs and mutual funds. Hedge funds typically have no more than 100 investors, whereas ETFs and mutual funds have thousands. Those that invest in hedge funds are typically very wealthy individuals, institutions, and funds of funds. Funds of funds typically take large sums of money from potentially many places and invest in several hedge funds. This is a bridge for smaller investors to participate in hedge fund. The goal of a hedge fund typically falls along the lines of two ideals. The hedge fund may be out to beat a bench mark which is to say that the hedge fund aims to outperform an index of stocks. A hedge fund could also aim for absolute return, which translates to net positive profit no matter what, but usually takes more time and has fewer returns as a trade off for stability. We’ll be focusing on hedge funds because they are the most computationally demanding.\nFund Metrics and Operations Measuring the performance of a fund is vital for making financial decisions in the market, so here we’ll discuss a few. Overall success can be measured by cumulative return, which is the percentage of an original value made in a given time: $\\dfrac{\\textrm{end - start}}{start}$. However, this means little if the portfolio is rapidly and wildly fluctuating. Hence it’s also useful to measure the volatility of a portfolio. This is simply measured by the standard deviation of daily returns; it’s best to have low volatility. Another important measure is the return on risks. This is done by calculating the Sharpe Ratio (SR), also called risk-adjusted reward.\n$SR = \\sqrt{252} \\dfrac{mean(\\textrm{daily returns} - \\textrm{risk free rate})} {volatility}$  The factor of $\\sqrt{252}$ comes from the number of trading days in a year. These factors can give us an idea of how well a portfolio is performing.\nAs previously mentioned, hedge funds are very computationally intensive environments. Let’s delve into the details of how a typical hedge fund works. Central to the operation of a hedge fund is its trading algorithms. Normally, a target portfolio is decided upon, then historical stock data and the target portfolio are fed to the trading algorithms to produce orders. The orders are sent to the market to alter the live portfolio, which is again fed back into trading algorithms.\nTrading algorithms work to place certain orders at the proper time. For example, an order for everything in the target portfolio shouldn’t be placed all at once because the price of the stock will go up and more money is spent than if strategic ordering were implemented. Additionally, there is another set of computational structures for determining the target portfolio.\nHistorical data, current portfolio, and prediction algorithms are fed into an optimization program to produce a target portfolio. The majority of machine learning comes into play when determining the market forecast.\nMarket Mechanics Ordering The live portfolio is altered by giving orders to a broker in the stock market, so it serves to know what exactly is in an order. The broker needs to know whether to buy or sell and of which stock(s) by their market symbols. The order must also contain the number of shares and the type of order. Stock exchanges only consider limit orders and market orders, but note that orders can be increasingly complex based on instructions given to a broker. A market order is an order at the current market price, and ordering at a limit price tells the broker to only buy or sell at a certain price; for example, some may not want to buy beyond some value or sell below some price. Of course, a limit order must also include the desired price.\nAfter the broker sends the order to the stock exchange, it is made public in the style of an ”order book”. Others can see the stocks and collective bids that have been made on them, but not who has placed the orders. The order book contains a list for each stock of the orders within it including whether the order asks for others to buy or bids on the stock. Both types include a price at which orders are allowed to be bought/sold at and the size of the order. Orders of the same type and price are lumped together. Market orders always sell at the highest bid and buy at the lowest asking price.\nThe example order book suggests that the price of the stock will decrease because there is much more selling pressure- more are selling than buying.\nDynamics of Exchange There are many stock exchanges, and each has its own order book. When an order is placed, say by an individual, the order is sent to the broker and the broker chooses between the stock exchanges to execute that order. The broker takes information from all the stock exchanges and makes a transaction based on which one has the stock with the best price. Fees are associated with making transactions in stock exchanges and with using a broker. A broker typically has many clients; the broker can observe clients who want to buy and sell at the same price and circumvent stock exchanges entirely. The law ensures that this trade can only happen if both the buyer and seller get prices that are at least as good as at an exchange. Even if this transaction cuts out the stock exchange, it must still be registered with one, and it’s usually with the exchange the stock is housed.\nOrders can be handled internally or moved through what’s called a ”dark pool”. A dark pool is a place where investors can trade without the transparency of an order book outside of a stock exchange. The results of the trade, like internal trades, still need to be registered with public exchanges after they’ve occured. A dark pool can act as an intermediary between all types of investors that may want to escape the transparency of stock exchanges. Brokers like this because they don’t have to pay fees associated with trading at a stock exchange. They also argue that it’s fair because clients are getting prices that are as good as at the market. However, hedge funds and dark pools can heavily exploit this system as it stands if they have well-placed, fast computers.\nExploiting the Market These days the market is entirely digital and computers automate transactions all across the country. As a result, orders can be processed in fractions of a second, and timing is everything. A hedge fund may pay stock exchanges enough money to house computers very close to the exchanges, which gives them a distinct advantage. For example, let’s say that someone places an order for a stock, and it’s sent to multiple markets. A hedge fund close to those markets can see that order reach one of them first and buy up that stock from the other exchanges through the high speed infrastructure they have in place. Then when that order reaches other exchanges, the hedge fund has already bought those shares and sells it back at a higher price. This is one of many strategies in High Frequency Trading (HFT) that takes place on the order of milliseconds.\nThis can also happen on an international scale. A hedge fund may have computers collocated with international markets rapidly observing and comparing order books between them. If a difference occurs in a stock between the two markets, all that needs to be done is to sell in the market with a higher price and buy in the market with a lower one. This happens very quickly, so the price of the stock is not very different at different markets. HFT strategies usually trade high volumes to turn a large profit in small price differences.\nThose that operate on HFT strategies can not only manipulate a transparent market, but also a dark one. First, let’s explain why someone would want to use a dark pool. For example, an investor who wants to sell a high volume of shares in a transparent market would want to do so in small chunks so as not to upset the price all at once and get less for the shares. However, others will see this and lower their bids knowing that a high volume is to be sold and the investor still gets less for their shares. In a dark pool, others can’t see those that want to buy or sell, so the investor may get a better price. There are many ways to exploit a dark pool, but it always stems from information leakage. Knowing the order book of a dark pool means a world of advantage. Dark pool operators or constituents may secretly participate in their own pool or leak information about it to others for a price. The private nature of a dark pool allows those who operate it to make their own rules about who can participate and how trading works. Since information is at the discretion of the operator, it’s fairly easy for those with direct access to exploit a dark pool. Those that don’t have direct access can ”game” the pool by probing it with many small volume orders. This yields some idea of the size and prices of bids, which gamers can exploit by selling when they find the bids are highest and buying when asking is lowest.\nOther Orders and Shorting Although exchanges only take market and limit orders, other orders can be made through a broker. Often a broker implements them for clients without their knowledge to benefit both themselves and the client. The most simple order above a limit order is a stop-loss order. The broker holds stock until the price drops below a certain price, then sells at market price. Similarly, a stop-gain order waits until the price climbs to a point at which the client wants to sell. Slightly more complex is the trailing stop. Here the broker sets a stop-loss criteria trailing a stock that’s increasing in price. As the price increases, so does the stop-loss criteria; when that stock starts to decrease, the stop-loss criteria is met and stocks are sold.\nWhat if someone wanted to bet that a stock will decrease and still profit? Instead of just selling high and buying low, those stocks can be borrowed and sold, so the value of those stocks is gained at a high point but the stocks are still owed. Then when the price of the stock decreases, the stock can be bought at a lower value, and the shares returned to whom they were borrowed while a profit on the difference was made. This is called shorting. As long as the price of the stock goes down, this is a good strategy; however, if the price goes up, then the difference results in a net loss.\nWorth: Company Valuation The price of a company’s stock is intended to reflect the value of the company. Ergo, if the price of a company’s stock deviates significantly from its predicted value based on the company’s predicted worth, then there’s a profitable opportunity for when it returns to reflect the company’s worth. The value of the company can be estimated several different ways. One way is to estimate its intrinsic value, which is based on the future dividends the company will give; these are annual payments to stockholders. This doesn’t really describe what the company has though. The book value of a company is founded in the company’s assets like its facilities and resources. A company’s market cap is yet another way to estimate a company’s worth, and it’s easiest to calculate. This is effectively what the stock market thinks what the company is worth and it’s the value of a stock multiplied by the total number of stocks.\nIntrinsic value may not make sense if we try to imagine the value of a company that will pay dividends consistently as long as it stands. However, the company can never be 100% reliable, so the value of its dividends amount to the value of a promise. The promise of some money in a year is worth less than the same amount given right now because of this principle. Thus, the value of a those promised dividends decreases as the time they’re promised is longer, so the total value will converge to a calculable value. Similarly, we can calculate the present value (PV) of a dollar that is promised after a certain time. It makes sense that the PV of a dollar promised right now is a dollar, but what about in a year? The PV is some fraction of its future value (FV) based interest rate (IR) and the length of time, $t$.\n$PV = \\dfrac{FV}{(1\u0026#43;IR)^t}$  In this way we have a conversion between the present value and future value of some amount of money. The interest rate is also called the discount rate and it reflects the risk involved with investment. A more stable company will have a lower discount rate because they’re more reliable. The intrinsic value, IV, of a company can be calculated knowing its discount rate and dividend payments by\n$IV = \\dfrac{FV}{IR}$  Thus, if a hypothetical company pays dividends of \\$5 a year and has a discount rate of 1%, then the value of this company is $ \\dfrac{$5}{0.01} = \\$500 $. Book value of a company is simple to calculate because it is just what the company has versus what the company owes. If a company only has a factory worth \\$1 million, a patent worth \\$500,000, and a loan of \\$200,000, then the company is worth \\$1 million − 200,000 = \\$0.8 million. The patent is considered an intangible asset and isn’t counted in calculating the book value.\nNews about companies can drastically change some of these measures. Investors reflect their opinions on the worth of a company through stocks- if they feel the company is worth less, they will sell and vice versa. Let’s say bad news about a company comes up; investors will see that as increased risk in investing in the company. The company will have to increase their IR to appease investors and the intrinsic value of the company will reduce. This would also reduce the stock price of the company, which decreases the market capitalization of the company. News can affect singular companies, sectors of business, and the market as a whole depending on the scope of the news.\nMarket strategies are based on deviations in the estimated values of a company. For example, if the intrinsic value of a company drops and the stock price is relatively high given its history, then it would probably be a good idea to short that stock because the price will almost certainly go down. The book value of a company provides somewhat of a minimum for the market cap; that is because if the market cap goes below the book value, then a predatory buyer typically buys the whole company, breaks it apart, and sells its parts for the book value to turn a profit.\nThe Capital Assets Pricing Model The Capital Assets Pricing Model (CAPM) is a model that is used to predict the return of stocks. To understand this model, a portfolio must be understood in more depth. The term portfolio has been used throughout this text, but has yet to be clearly defined; a portfolio is a set of weighted assets, namely stocks. A portfolio is a set of stocks that are weighted by their value, and all the weights add to 1. Some stocks might be shorted, so technically their portfolio value is negative and really the sum of the absolute value of their weights is mathematically written, where $ w_i$ is the weight of a stock in a portfolio\n$\\sum_{i} |w_i| = 1$  and the return of the portfolio for a given day is\n$\\sum_{i} w_i r_i$  where $r_i$ is the return for a stock in a day. As an example, lets say a portfolio is composed of two stocks, A and B, and their respective weights are 0.75 and -0.25 because stock B is shorted. Then if on a given day, stock A increases by 1% and stock B decreases by 2%, the result is a portfolio return of (0.75)(0.01) + (−0.25)(−0.02) = 1.25%.\nA similar portfolio can be made for entire markets. Although, it’s typically limited to an index which includes the largest companies in a market, like the S\u0026amp;P 500 for the US market. These companies are weighted by their market caps, so a company’s weight in the market is approximately that company’s cap, c, divided by the sum of all caps.\n$w_i = \\dfrac{c_i}{\\sum_{j} c_i}$  The CAPM predicts the return on stocks within a certain market with the simple equation\n$r_i[t] = \\beta_i r_m[t] \u0026#43; \\alpha_i[t]$  This says that the return of a stock is largely based on the return of the market as a whole, $r_m$. The degree to which a stock is affected is based on that stocks particular $\\beta$ value. Fluctuations that deviate from this are represented in the (theoretically) random variable, $\\alpha$, which (theoretically) has an expected value of zero. The $\\beta$ and $\\alpha$ values of a stock are calculated based on the historical data of daily returns. The daily returns of a stock are plotted against that of the market and the slope of the fitted line constitutes the $\\beta$ value. The y-intercept and random deviations describe the $\\alpha$ value.\nCAPM and Related Theories The nature of CAPM suggests a specific strategy when approaching the market. CAPM says that the relationship of stocks to the market is linear with an average fluctuation of zero from this relationship. This suggests that the best tactic is to simply choose a set of stocks that will perform well with a certain market environment and sit on them. Active management is a way of thinking that believes the $\\alpha$ value is not entirely random and can be predicted. This mindset promotes carefully choosing and trading stocks on a regular basis depending on predicted $\\alpha$ values. This is the dichotomy between active and passive portfolio management. If we assume that $\\alpha$ is entirely random, then the only way we can beat the market is by predicting the return on the market. However, this is not entirely true, and CAPM will be used to eliminate market risk entirely.\nCAPM gives $\\beta$ values to each stock, but there are other theories that say it’s more complicated. One of these is the Arbitrage Pricing Theory (APT), which says that there is not a contribution based on the whole market, but based on its sectors. Moreover, a stock is affected by what happens in the ten different sectors of the economy, and the CAPM equation becomes\n$r_i = \\sum_{j} \\beta_{ij}r_j[t] \u0026#43; \\alpha_i[t]$  where $j$ denotes the different sectors. This provides a more in-depth prediction of stock return.\nUsing the CAPM Now we want to use the CAPM to create a lucrative portfolio. If we say that the return on the market can never be predicted, then any component associated with market return is risk. Applying the CAPM equation to get an overall portfolio return yields\n$r_p = \\sum_{i} w-I[\\beta_ir_m(t)\u0026#43;\\alpha_i(t)] $  The only way to remove the market return component is to choose weights such that $\\sum_{i}w_i\\beta_i = 0$. At this point it may seem that there will not be an expected return for the portfolio because CAPM predicts $\\alpha$ to be random. It is here that the assumptions of CAPM are wrong. Using some information, we can predict whether a stock will perform better or worse than the market, which will yield a profit regardless of which way the market goes. This information can come from expertise, some analysis, or, in our case, machine learning.\nTechnical Analysis There are effectively two kinds of analysis: fundamental and technical. Fundamental analysis looks at the properties of a company to determine market decisions. Technical analysis looks at patterns in stock prices to make market decision; since technical analysis is clearly more useful aside from predatory buying, that’s what will be discussed.\nTechnical analysis only focuses on stock price and volume. Using these to calculate statistics gives indications on what economic decisions to make. Technical analysis is most useful on shorter time scales and when combinations of indicators point to the same decision. HFT trade at the millisecond timescale and fundamental analysis firms operate at the timescale of years; humans and computers tend to work together at a timescale in between.\nSome Good Indicators It’s useful to develop strategies based on time-series indicators, so here are a few. Momentum is a scale dependent indicator that suggests an upward or downward trend depending on slope. Moreover, an n-day momentum, $m$, for a stock with price function, $p[t]$, is calculated as\n$m = \\dfrac{p[t]-p[t-n]}{p[t-n]} = \\dfrac{p[t]}{p[t-n]} - 1$  This is simply the difference in price as a ratio to the price n days ago. Typically 5, 10, or 20 day momentum is used with values ranging from -0.5 to 0.5. Another indicator is a simple moving average (SMA); SMA is also scale dependent looking over an n-day window. The price of a stock over n days is averaged and plotted over time where points are placed at the leading end of the window. However, to get a useful value, this needs to be compared to the real time price. Like momentum, it is done as a ratio\n$SMA[n] = \\dfrac{p[t]-E(p[t-n:t])}{E(p[t-n:t])} = \\dfrac{p[t]}{E(p[t-n:t])} - 1$  Momentum and SMA together often prove to be strong indicators. For example, if there is strong positive momentum and a crossing of the price from the price being lower than the average to above- this is a good indicator the price will increase. Larger than normal deviations from the moving average are expected to return back to the average and indicate opportunities. Thus, if SMA is positive, it’s a selling opportunity, and a buying opportunity if SMA is negative.\nHow those decision are made depends on the state of the stock, or its volatility. The standard deviation of the stock’s fluctuations provides an excellent measure for when to make decisions. It depends on how certain we want to be that the price is an outlier. The farther the decision threshold is from the SMA, the more certain we are that the price is an anomaly. From basic statistics, if the decision threshold is placed two standard deviations away from the SMA, then we are 95% sure the price is an anomaly. These bands, typically at two standard deviations away from SMA, are called Bollinger bands. The way this is written mathematically is, again, a ratio, which is between the price difference and the $2\\sigma$ length where $\\sigma$ is the standard deviation.\n$BB[t] = \\dfrac{p[t] - SMA[t]}{2\\sigma}$  When making economic decisions, a Bollinger band value greater than 1 denotes a selling opportunity, and less than -1 denotes a buying opportunity. However, it’s better to trade when the price crosses the band for the second time because that signals the price moving in a profitable direction.\nWhen using these values in a machine learner, it’s important that indicators are normalized. To normalize values, follow\n$normal = \\dfrac{value-mean}{\\sigma}$  This provides a z-score by which to compare everything.\nAdjusted Prices Analysis of historical data is crucial for determining patterns and making economic decisions, but some things drastically change the price of stocks without having any effect on the real value of the stock. Dividends and stock splits are two things that do just that. The adjusted price accounts for these events and corrects the computational problems that would occur if only the price were taken into account.\nStock splits occur when the price of a stock is too high and the company decides to cut the price, but increase the volume so that the overall market cap is the same. This is a problem when dealing with data because it’s seen as a large drop in price. The adjusted price is calculated going backwards in time; moreover, the given and adjusted price are the same for a certain starting present day and adjusted going backward in time. If the price is ever split, say by 3, then at the time of the split, the price is divided by 3 so that there is no discontinuity.\nAt the time a company announces the date for payment of dividends, the price of the stock will increase by the amount of a dividend until they’re paid at which point the price rapidly decreases by that amount. This is adjusted looking back in time, and on the day a dividend is paid, the prices preceding are decreased by the proportion of the dividend payment.\nAs a note for machine learning, the data that is chosen for the learner is very important. If the stocks from today are chosen and analyzed starting from 7 years ago, then those stocks will of course do well because they’ve survived. That’s using a biased strategy, so what needs to be done is to take index stocks from 7 years ago and run with those. For adjusted price, it’s also important to note that the adjusted price will be different depending on where the starting point is chosen, and that should also be taken into account.\nEfficient Markets Hypothesis Until now, we’ve been assuming, for technical analysis, that there is information in historical data that we can exploit to determine what the market is going to do. The same goes for fundamental analysis in terms of fundamental data. However, the Efficient Markets Hypothesis says that we’re wrong on both accounts!\nHere are some assumptions of the Efficient Markets Hypothesis:\n Large number of investors: The most important assumption of EMH is that there are a large number of investors for-profit. They have incentive to find where the price of a stock is out of line with its true value. Because there are so many investors, any time new information comes out, the price is going to change accordingly. New information arrives randomly Prices adjust quickly Prices reflect all available information  The three forms of the EMH There are 3 forms of the EMH, ranging from weak to strong.\n Weak: Future prices cannot be predicted by analyzing historical prices. This leaves room for fundamental analysis, however. Semi-strong: Prices adjust rapidly to new public information Strong Prices: Prices reflect all information, public and private  Is the EMH correct? If the EMH is correct, a lot of what we’re trying to do is impossible, so we should cut our losses and go home. Luckily, there is evidence for why certain versions of the hypothesis are incorrect. The existence of hedge funds indicates that you can profit by investing in stocks other than the market portfolio.\nThe strong version is the weakest of the three, considering there are many examples of insiders using esoteric information for their own benefit. And in many cases, these people have gone to jail!\nThere is also data that shows that the semi-strong version isn’t too likely to be correct. You can see trends in 20-year annualized returns versus 10-year P/E ratio data, which means that you most likely can use fundamentals to predict future performance.\nThe Fundamental Law of Active Portfolio Management Richard Grinold was trying to find a way of relating performance, skill, and breadth. For example, you might have lots of skill to pick stocks well, but you might not have the breadth to use that skill. So he developed the following relationship:\n$performance = skill\\sqrt{breadth}$  So we need some way of measuring skill and breadth. Performance is summarized by something called the information ratio:\n$IR = IC\\sqrt{BR}$  where IC is the information coefficient, and BR is the number of trading opportunities we have.\nThe Coin-flipping Casino As a thought experiment, instead of buying and selling stocks, we’re going to flip coins, and bet on the outcome. This is analogous to buying a stock and holding it– either you earn or lose money.\nThe coin is biased (like $\\alpha$) to P(heads) = .51. The uncertainty of the outcome is like $\\beta$.\nBetting: Betting works by betting on N coins. If we win, we now have 2N coins. If we lose, we now have 0 coins, so this is an even-money bet (you either gain N coins or lose N coins.)\nThe Casino: The casino has 1000 tables, each with a biased coin, and you have 1000 tokens, so you can bet them in any way you like: 10 tokens each on 100 tables, 1 token on each table, or 1000 tokens on 1 table. Once bets have been placed, the coins are all flipped in parallel, and for each game you either lose your chips or win. So now the question is what scenario is going to net you the best outcome? Let’s take the following two bets for example:\n 1000 tokens on one table and 0 on the other 999 1 token on each of 1000 tables  Which is better? Or are they the same? In fact, the expected return of both bets are the same, but bet 2 is much less risky, as with bet 1, if you lose, you lose all of your money, but with bet 2, you might lose around half your money. In fact, the chance of losing all of your money (if you bet tails) is:\n$(.49)^{1000} \\approx 10^{-310}$  To determine which is best, we need to consider risk and reward. In this case, the reward is our expected return. If $P_w$ is the chance we would win, and $P_l$ is the chance that we would lose, and $W$ is the amount we would win, whereas $L$ is the amount we’d lose, expected return for a single bet is calculated as follows:\n$E[R] = P_wW \u0026#43; P_lL$  For the biased coin where we place all of our bets on one table, this would be:\n$E[R] = .51(\\$1000) \u0026#43; .49(−\\$1000) = \\$20$  If we placed 1 token on each table, the expected return would be:\n$E[R] = \\sum_{i=1}^{1000} .51(\\$1) \u0026#43; \\sum_{i=1}^{1000} .49(−\\$1000) = \\$20$  So, in terms of reward, neither is better or worse. So how do we choose how to allocate the tokens? It turns out that the risk makes it easy to choose.\nFirst, what’s the chance that we lose it all? For the case where all of our tokens are on one table, the chance is 49%. For the second table, it’s around $10^{-308}%, which is quite a bit smaller\u0026hellip;\nAnother way to look at the risk is by looking at the standard deviation of the bets. An example of the outcomes for situation 2 is:\n$−1, 1, 1, 1, −1, −1, 1, −1, ..., 1$  The standard deviation of which is just 1. Now, for the case where we put all of our tokens on one table, the outcomes look like this:\n$1000, 0, 0, 0, 0, ..., 0$  or\n$−1000, 0, 0, 0, 0, ..., 0$  The standard deviation (risk) in both cases is $\\sqrt{1000} \\approx 31.62$, which is much higher than the standard deviation for putting bets on each table. Now, we can create a risk-adjusted reward (Sharpe Ratio) for the single-bet case:\n$R_s = \\dfrac{\\$20}{\\$31.62} = 0.63$  For the multi-bet scenario, it’s:\n$R_m = \\dfrac{\\$20}{\\$1} = 20.0$  Clearly, the second case wins based on this ratio. Something interesting about these results is that:\n$20 = .63\\sqrt{1000}$  It turns out that this can be generalized to\n$SR_{multi} = SR_{single}\\sqrt{bets}$  Which shows that as we increase the number of bets (diversify), the Sharpe Ratio increases. This is the relationship described in the Fundamental Law of Active Portfolio Management; to increase performance, you can either increase skill or diversify (bets), although diversification only goes as the square root.\nNow, back to equation 3.1. Let’s define what information ratio means. If we consider the CAPM equation:\n$r_p[t] = \\beta_p r_m[t] \u0026#43; \\alpha_p[t]$  we can associate the first term, $\\beta_p r_m[t]$ with the market, and the second term, $\\alpha_p[t]$ with skill. Information ratio is defined as:\n$IR = \\dfrac{\\overline{\\alpha_p[t]}} {\\sigma_{\\alpha_p[t]}}$  Information ratio can be thought of as a Sharpe Ratio of excess return (due to skill). Now, the information coefficient, IC, is the correlation of forecasts to returns. IC can range from 0 (no skill) to 1 (perfect skill). BR, or breadth, is the number of trading opportunities per year. For example, if you are Warren Buffet, and hold only 120 stocks for a whole year, BR is just 120. However, if you have 120 stocks and trade them daily, $BR = 120∗365$.\nLet’s do an example: say that James Simons and Warren Buffet both have the same information ratio, and that Simons’ algorithm is 1\u0026frasl;1000 as smart as Buffet’s. Buffet only trades 120 times per year. How many trades per year must Simons execute to have the same information ratio (performance)?\n$IR_S = IC_S\\sqrt{BR_S}$  $IR_B = IC_B\\sqrt{BR_B}$  $IR_S = 1/1000IC_B$  $\\dfrac{IR_B}{IR_S} = \\dfrac{IC_S\\sqrt{BR_S}}{IC_B\\sqrt{BR_B}} = 1$  $= \\dfrac{\\dfrac{1}{1000}\\sqrt{BR_S}}{\\sqrt{BR_B}}$  $\\Rightarrow BR_S = (1000)^2 BR_B$  $= 120,000,000$ \nSo Simons must execute 120 million trades, whereas Buffet only needs to execute 120. That’s quite a difference! Indeed, skill is an extremely important factor in performance.\nPortfolio Optimization Now we wish to optimize a portfolio, and what this means is minimizing risk for a given target return. Risk is largely defined as the volatility of a stock. A portfolio is composed of some stocks that individually have their own return-risk ratios, but it is possible to weight them such that the return-risk ratio of the portfolio is higher than that of any individual stock.\nThis is done through combining correlated and anti-correlated stocks to highly reduce volatility. In the case of a highly correlated group of stocks, their combination results in a similar volatility, but if they’re combined with highly anti-correlated stocks, then with accurate weighting, fluctuations cancel out and volatility is minimal while yielding similar returns. A useful algorithm to find the best weighting is mean variance optimization (MVO). This algorithm is not explained, but we should find it. MVO and similar algorithms find the minimal risk for a given target return, and if this is plotted over all target returns, we get a curve called the efficient frontier. On a return-risk plot, a line tangent to the efficient frontier with an intercept at the origin also points to the portfolio with the minimal Sharpe ratio.\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/6242/",
	"title": "6242",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/routing/",
	"title": "Routing",
	"tags": [],
	"description": "",
	"content": " Routing Overview With your head wrapped around routing we\u0026rsquo;ll now take a look at the nuts and bolts that make routing possible: naming, addressing and forwarding.\nAnd you\u0026rsquo;ll start your first significant Mininet project. In the project you\u0026rsquo;ll investigate switched buffer sizing which can have an important effect on network performance.\nInternet Routing The next few lessons will cover internet routing. Contrary to what you might think, the internet is not a single network, but rather a collection of tens of thousands of independently operated networks, or autonomous systems, sometimes simply called ASes. Networks such as Comcast, Georgia Tech, and Google, are different types of autonomous systems. An autonomous system might be internet service provider, a content provider, a campus network, or any other independently operated network. Now when you\u0026rsquo;re sitting at home on Comcast and trying to reach content in Google or Georgia Tech, your traffic actually traverses multiple autonomous systems. This process of internet routing actually involves two distinct types of routing. One is intradomain routing, which is the process by which traffic is routed inside any single autonomous system. The other is interdomain routing, which is the process of routing traffic between autonomous systems. So computing a path between a node in an ISP like Comcast and another node in a network like Georgia Tech\u0026rsquo;s involves computation of both intradomain paths and interdomain paths. In this part of the lesson we\u0026rsquo;ll look at intradomain routing. Then we\u0026rsquo;ll study interdomain routing, as well as the business relationships that make interdomain routing so complicated. So let\u0026rsquo;s jump into our study of intradomain routing and topology.\nAutonomous Systems (AS) AS Quiz As a quick quiz, which of the following types of routing protocols are responsible for routing within an autonomous system?\nAS Solution Intradomain routing protocols are responsible for routing within an autonomous system. Interdomain routing protocols, on the other hand, are responsible for routing traffic between autonomous systems.\nIntra AS Topology Before we jump into intradomain routing, let\u0026rsquo;s take a look at what a topology might look like inside a single autonomous system. A topology inside an AS consists of nodes and edges that connect them. The nodes are sometimes called points of presence, or PoPs. A PoP is typically located in a dense population center, so that it can be close to the PoPs of other providers for easier interconnection and also close to other customers for cheaper backhaul to customers that may be purchasing connectivity from this particular AS. The edges between pops are typically constrained by the location of fiber paths, which for the sake of convenience typically parallel major transportation routes such as railroads and highways.\nHere\u0026rsquo;s an example of a single AS topology which is the Abilene Network, which is a research network in the United States. Each of these locations would be considered a PoP, and each of these PoPs may have one or more edges between them. Georgia Tech is an autonomous system that connects at the Atlanta PoP of the Abilene Network.\nHere\u0026rsquo;s a close up of the Abilene Network in the south eastern U.S. The Abilene network connects to other universities in the southeast near Atlanta and an internet exchange point called SOX, or southern crossroads. Now, thus far we\u0026rsquo;ve just talked about the topology of an autonomous system, which essentially defines the graph. The next step is to compute paths over that topology, a process called routing. Routing is the process by which nodes discover where to forward traffic so that it reaches a certain node. There are two types of intradomain routing. One is called distance vector, and the other is called link state. In the rest of this lesson we\u0026rsquo;ll explore the two different types of intradomain routing and the advantages and disadvantages of each of them. Let\u0026rsquo;s first take a look at distance vector routing.\nDistance Vector Routing In distance vector routing, each node sends multiple distance vectors to each of its neighbors, essentially amounting to copies of its own routing table. Routers then compute costs to each destination in the topology based on shortest available path. Distance vector routing protocols are based on the Bellman-Ford algorithm. A node X\u0026rsquo;s forwarding table is based on the solution to the following equation. Suppose that node X is trying to find a shortest cost route to node Y. In this case node X is trying to find a path through some intermediate node, V, that minimizes the cost between X and V, and the already known shortest cost path between V and Y. Again, the solution to this equation for all destinations, Y, in the topology is X\u0026rsquo;s forwarding table. Let\u0026rsquo;s now take a look at distance vector routing by way of example.\nExample of Distance Vector Routing 1 Let\u0026rsquo;s suppose that we have a three node network with the costs on the edges as shown. Initially, each node has a single distance vector representing the shortest path cost to each other incident node in the graph. For example, the distance between x and x is obviously zero. And the shortest known distance between x and y from x\u0026rsquo;s perspective is one, the direct path. Similarly, the shortest known distance between x and z to x at the outset is five because all it knows is the direct path. Note that a shorter path between x and z exists via y, but x simply doesn\u0026rsquo;t know about it yet. Now in distance vector routing, every node send its vectors to every other adjacent node. And each node then updates its routing table according to the Bellman-Ford equation. Let\u0026rsquo;s look at what happens when node x learns of y\u0026rsquo;s distance vectors. Well in this case, the distance from x to z will be computed as the minimum of the sums of all distances to z through any intermediate node. So the cost between x and y is one, and the distance between y and z as discovered by y\u0026rsquo;s distance vector is two. Therefore, x can update its shortest cost distance to z as three. Similarly, x will receive a distance vector from z, five two zero, but of course, when it uses the Bellman-Ford equation to update its distances, again the distance between z and x will be updated from five to three. We can repeat this exercise at other nodes, as they receive distance vectors from other nodes in the topology, and quickly, every node in the network has a complete routing table. Now when costs decrease, the network converges quickly, but one problem is that when failures occurs, bad news can actually travel slowly.\nExample of distance Vector Routing 2 Let\u0026rsquo;s look at a different example. So for the sake of illustration, I\u0026rsquo;ve increased the cost between x and z to 50, and now everyone starts with a different set of initial distance vectors. Now eventually, after running the distance vector protocol, we would see the tables converge as such. Let\u0026rsquo;s suppose that the cost of the link between x and y suddenly increased from 1 to 60. Well now in this case, y would need to update its view of the shortest path between y and x. Now it\u0026rsquo;s no longer one, but it\u0026rsquo;s not 60 either. To see why let\u0026rsquo;s go back to our Bellman-Ford equation. We can see that y thinks it can get to z with a cost of two, and that z can get to x with a cost of three. So in fact it\u0026rsquo;s going to update this entry from one to five. Then it will tell it\u0026rsquo;s neighbor z its new distance vector. In other words, that now its distance to x is no longer one but five. At this point, z needs to re-compute it\u0026rsquo;s shortest path to x. Now, it knows that it can get to y with a cost of two but it thinks still that y can get to x with a cost of five. Therefore, this entry is no longer three but seven. And now z sends its new distance vector back to y. Y then updates it\u0026rsquo;s distance vector for z and this process continues. So, then y thinks it is now nine units away from x. So z has to do this all over again and now z thinks that its shortest path is two plus nine or 11. Now this process repeats of course until z finally realizes that it has a shorter path of 50 directly through x after this counting up process exceeds the value of 50.\nThis problem is called the count to infinity problem, and the solution is called poison reverse. The idea here is that if y must route through z to get to x in its table, as it did here, then y advertizes an infinite cost for the destination x to z. So instead of sending five, zero, two, y would send infinity, zero, two. This would thus prevent z from routing back through y, and immediately, it would choose the shortest path to x, of path cost 50.\nRouting Information Protocol An example of a distance vector routing protocol is the routing information protocol or RIP. The first version of RIP was defined in 1982 where edges had unit cost, and infinity for the count to infinity problem was 16. Table refreshes occur every 30 seconds and when an entry changes, it sends a copy of that update to all of its neighbors except for the one that induced the update. This rule is sometimes called the split horizon rule. The small value for infinity ensures that the count to infinity doesn\u0026rsquo;t take very long and every round has a time out limit of 180 seconds which is basically reached when a router hasn\u0026rsquo;t received an update from a next hop for six 30 second periods. In practice, when a router or link fails in RIP, things can often take minutes to stabilize. So because of problems such as slow convergence and count to infinity, protocol designers look to other alternatives.\nLink State Routing The prevailing alternative and the one that is used in most operational networks today is link state routing. In link state routing, each node distributes a network map to every other node in the network and then each node performs a shortest path computation between itself and all other nodes in the network. So, initially each node adds the cost of its immediate neighbors, D(v), and every other distance to a node that is infinite. Then each node floods the cost between nodes u and v to all of its neighbors. And the distance to any node v becomes the minimum of the cost between u and w plus the cost to w, or the current shortest path to v. The shortest path computation is often called the Dijkstra shortest path routing algorithm. Two common link state routing protocol are open shortest paths first or OSPF and intermediate system- intermediate system or IS-IS. In recent years, IS-IS has gained increasing use in large internet service providers and is the more commonly used link state routing protocol in large transit networks today. One problem with link state routing is scale. The complexity of a link state routing protocol grows as n cubed where n is the number of nodes in the network.\nCoping with Scale Hierarchy One way of coping with scale is to introduce hierarchy. OSPF has a notion of areas, and IS-IS has an analogous notion of levels. In a backbone network, the network\u0026rsquo;s routers may be divided into levels, or areas, and the backbone itself may have its own area. In OSPF, the backbone area is called area zero, and each area in the backbone that\u0026rsquo;s not in area zero has an area zero router. The area zero routers perform shortest path computations and the routers in each of the other areas independently perform shortest path computations. Now paths are computed by computing the shortest path within an area, or, if the path must leave an area, it\u0026rsquo;s computed by stitching together the shortest path to the area zero backbone router, and then the shortest path across area zero followed by another intra-area shortest path.\nInterdomain Routing We\u0026rsquo;re now moving on to cover interdomain routing or routing between ASes. Recall that internet routing consists of routing between tens of thousands of independently operated networks, or autonomous systems. Each of these networks operates in their own self-interest and have independent economic and performance objectives, and yet they must cooperate to provide global connectivity so that when you\u0026rsquo;re sitting at home, you can retrieve content that might be hosted at the Georgia Tech network.\nNow, each independently operated network is called an autonomous system, or AS. And each AS advertises reachability to some destination by sending what are called route advertisements or announcements.\nThe protocol that ASes use to exchange these route advertisements is called the Border Gateway Protocol, or simply, BGP. A route advertisement has many important attributes, but for now, let\u0026rsquo;s just talk about three. Now a router here, let\u0026rsquo;s say on the Comcast network, might receive a route advertisement, typically from its neighboring AS. That route advertisement might contain a destination prefix, such as the IP prefix for Georgia Tech. Then it might contain what\u0026rsquo;s called a next hop IP address, which is the IP address of the router that the Comcast router must send traffic to, to send traffic along that route. Typically that next hop IP address is the IP address for the first router in the neighboring network. And the Comcast router knows how to reach that next hop IP address because its border router and the border router in the neighboring AS are on the same subnet. Typically this might be a /30 subnet, therefore this IP address is reachable from Comcast\u0026rsquo;s border. A third important attribute is what\u0026rsquo;s called the AS path, which is a sequence of what are called AS numbers that describe the route to the destination. Now strictly speaking, the AS path is nothing more than the sequence of ASes that the route traversed to reach the recipient AS. So for example, Georgia Tech\u0026rsquo;s AS number is 2637 and Abilene\u0026rsquo;s is 10578 so the AS path that Comcast would hear if it received a route advertisement from Abilene for Georgia Tech, would be 10578 followed by 2637. So in the remainder of the lesson we\u0026rsquo;ll look at other BGP route attributes. But these are essentially the three most important because they describe how to stitch together an interdomain path to a global destination. So we have the destination IP prefix for the destination that a router might want to send traffic to; the next hop, which is the IP address for the router for the next hop along the path; and finally, the AS path, which is the sequence of ASes that the route traversed en route to the AS that\u0026rsquo;s hearing the announcement. The last AS number on the AS path is often called the origin AS, because that is the AS that originated the advertisement for this IP prefix. In this case, the origin AS is 2637, or Georgia Tech, because it is the AS that originated the announcement for this prefix.\nInterdomain Routing 2 Now thus far, we\u0026rsquo;ve talked about interdomain routing BGP, or the border gateway protocol, as consisting of route advertisements solely between border routers of adjacent autonomous systems. In fact, this is a specific type of BGP called external BGP, or eBGP. But in fact, as we know, each one of these autonomous systems has routers of its own, inside. Those routers also need to learn routes to external destinations. The protocol that is used to transmit routes inside an autonomous system for external destinations, is called internal BGP or iBGP. Okay, so to review, external BGP is responsible for transmitting routing information between border routers of adjacent ASes about external destinations. And internal BGP is responsible for disseminating BGP route advertisements about external destinations to routers inside any particular AS. Note the distinction between iBGP and an intra-domain routing protocol or an IGP.\nIGP vs iBGP The IGP or the intra-domain routing protocol, disseminates routes inside an AS to internal destinations whereas iBGP or internal- border gateway protocol, disseminates routes inside an AS to external destinations. So let\u0026rsquo;s suppose that a router inside AS A is trying to reach a destination inside AS B. AS A would learn the route via eBGP and the next hop of course, at this router, would be the border router at B. And now a router inside autonomous system A would learn the route to B via iBGP. Now the BGP next stop, would be the border router. And so, this router inside AS A, needs to use the IGP, to reach the iBGP next hop.\nProtocol Quiz So as a quick quiz, which routing protocol is responsible for disseminating routes inside an AS to external destinations? Is it the IGP? Is it iBGP. Or is it eBGP?\nProtocol Solution iBGP is responsible for disseminating routes inside an AS about destination IP prefixes that are located outside that AS. The iBGP next hop is typically a next hop IP address that is reachable via the ASes intradomain routing protocol, or IGP.\nBGP Route Selection Let\u0026rsquo;s now take a quick look at BGP route selection. It is often the case that a router on a particular autonomous system might learn multiple routes to the same destination. In this case, a router on autonomous system one, might learn a route to a destination in AS4 via both AS2 and AS3. In this situation. The router in AS one must select a single best route to the destination among the choices. The selection among multiple alternatives is known as the BGP route selection process. Let\u0026rsquo;s now take a quick look at that process.\nBGP Route Selection Process The first step in the BGP route selection process is to prefer a route with the higher local preference value. The local preference value is simply a numerical value that a network operator in the local AS can assign to a particular route. This attribute is purely local. It does not get transmitted between autonomous systems, so it is dropped in eBGP route advertisements. But it allows a local network operator the ability to explicitly state that one route should be preferred over the other. Among routes with equally high local preference values, BGP prefers routes with shorter AS path length. The idea is that a path might be better if it traverses a fewer number of autonomous systems. The third step involves comparison of multiple routes advertised from the same autonomous system. The multi-exit discriminator (MED) value allows one AS to specify that one exit point in the network is more preferred than another. So lower MED values are preferred, but this step only applies to compare routes that are advertised from the same autonomous system. Because the neighboring AS sets the MED value on routes that it advertises to a neighbor, MED values are not inherently comparable across routes advertised from different ASes. Therefore this step only applies to routes advertised from the same AS. Fourth, BGP speaking routers inside an autonomous system will prefer a BGP route with a shorter IGP path cost to the IGP next up. The idea here is that if a router inside an autonomous system learns two routes via iBGP then it wants to prefer the one that results in the shortest path to the exit of the network. This behavior results in what is called \u0026ldquo;hot potato\u0026rdquo; routing, where an autonomous system sends traffic to the neighboring autonomous system via a path that traverses as little of its own network as possible. Finally, if there are multiple routes with the highest possible local preference, the shortest AS path and the shortest IGP path, the router uses a tiebreak to pick a single breaking route. This tiebreaking step is arbitrary. It might be the most stable, or the route that\u0026rsquo;s been advertised the longest. But often, to induce determinism, operators typically prefer that this tie breaking step is performed based on the route advertisement from the router with the lowest router ID, which is typically the neighboring router\u0026rsquo;s IP address. Let\u0026rsquo;s now take a closer look into local preference, AS path length, multi-exit discriminator, and hot potato routing. Now as I mentioned, the first step in the router selection process is for routers to prefer routes with higher local preference values. Now an operator can actually set the local preference value on incoming BGP route advertisements to affect which route a router ultimately selects. Let\u0026rsquo;s see how this works.\nLocal Preference Now, a router in AS1 might learn two routes to a destination, one via the AS path 2-4 and the other via the AS path 3-4. Local preference, or simply, local pref, allows an operator to configure the router to assign different preference values to each of the routes that it learns. The default local preference value is 100. But if the operator prefers that this router select the path through AS two, it can configure the router to set a higher local preference for that route such as 110. This results in this router selecting the route through AS two and sending traffic to the destination in AS four via AS two. In this way an operator can adjust local preference values on incoming routes to control outbound traffic or to control how traffic leaves its autonomous system en route to a destination. This is extremely useful in configuring primary and back up routes. For example, here the route though AS two might be the primary route ,and the route through AS three, is the backup route. Now typically, as I mentioned, local preference is used to control outbound traffic. But sometimes autonomous systems can attach what\u0026rsquo;s called a BGP community to a route to affect how a neighboring autonomous system sets local preference. A community is nothing more but a fancy jargon word for a tag on a route. So let\u0026rsquo;s suppose that AS four wanted to control inbound traffic by affecting how AS two or AS three set local preference. In this case, let\u0026rsquo;s suppose that AS two wanted traffic to arrive via AS three, its primary, rather than by AS two, its backup. In this case, AS two might advertise its BGP routes with primary and backup communities. The backup community value might cause a router in AS two to adjust its local preference value, thus affecting how AS two\u0026rsquo;s outbound traffic choices are made. So, again local preference is used to control outbound traffic, in this case AS two\u0026rsquo;s outbound traffic decision. But the use of a BGP community on the route advertisement can sometimes be used to cause a neighboring AS to make different choices regarding it\u0026rsquo;s outbound traffic, thereby, allowing an AS to specify a primary or back up path for incoming traffic. This type of arrangement requires prior agreement.\nMultiple Exit Discriminator Let\u0026rsquo;s suppose that two autonomous systems connect in two different cities, San Francisco and New York. Let\u0026rsquo;s further suppose that AS 1 wants traffic to destination d to enter via New York City, rather than via the peering link in San Francisco. Well, remember that all things being equal, routers inside AS 2 will select the BGP route with the shortest IGP path cost to the next hop, resulting in hot potato routing. So some routers will select the San Francisco egress, and other routers might select the New York egress. To override this default hot potato routing behavior, AS1 might advertise its BGP routes to AS2 with MED values. For example, if the MED value on the route learned at the border router in New York was 10, and the MED value from the route learned from the router in San Francisco was 20, then instead of performing hot potato routing, all of these routers that would ordinarily be closer to the San Francisco egress, would instead pick the route learned via the New York egress because the preference for a lower MED value comes before the preference for a next hop with the lower IGP path process. So all of these routes would instead be carried over AS 2\u0026rsquo;s backbone network and exit via New York. Thus MED overrides hot potato routing behavior allowing an AS to explicitly specify that it wants another neighboring AS to carry the traffic on its own backbone network, rather than dumping the traffic at the closest egress and forcing traffic across the neighbor\u0026rsquo;s backbone. MEDs are typically not used in conventional business relationships, but they\u0026rsquo;re sometimes used, for example, if AS 1 does not want AS2 free riding on AS 1\u0026rsquo;s backbone network. So effectively MED allows AS 1 to say, yes, I will connect or peer with you, but it is your job to carry the traffic long distances across the country. This mechanism is sometimes used when a transit provider peers with a content provider, and the transit provider doesn\u0026rsquo;t want the content provider essentially getting free transit through the neighboring AS.\nIn the absence of MED overriding any behavior, typically what will happen is a router inside AS 2 would learn multiple routes via internal BGP to different egress points for the same destination d, and it would simply pick the next hop, or the egress router with the lowest IGP path cost, in this case, 5. It\u0026rsquo;s very common practice to set these IGP costs in accordance with distance, or propagation delay, thus resulting in routers inside the AS picking shorter paths. Now one problem with this notion of hot potato routing is that a very small change in IGP path cost can result in a lot of BGP routing changes. Remember that it\u0026rsquo;s probably not just one destination that\u0026rsquo;s being routed through the San Francisco egress, but maybe tens of thousands of routes. So a single IGP path cost change can result in rerouting of tens of thousands of IP prefixes in BGP. People have looked at various ways to improve the stability of BGP routing by decoupling the IGP and the BGP in this part of the route selection process.\nInterdomain Routing Business Models So now we\u0026rsquo;re going to look at Interdomain Routing Business Models. So the one thing to remember about interdomain routing is that it\u0026rsquo;s really all about routing money. Let\u0026rsquo;s consider this AS that wants to send traffic to a particular destination. Well, in the internet there are two different types of business relationships: a customer-provider business relationship, where money flows from customer to provider regardless of the direction that traffic flows; the other type of business relationship is a peering relationship where an AS can exchange traffic with another AS free of charge. This is sometimes also called settlement-free peering. So already you can see given three possible ways to reach the destination. This AS is first going to prefer a route through its customer, because regardless of the direction of traffic on this link, money is always flowing from the customer. The peering link is the second most preferable because it\u0026rsquo;s free. And the least preferable route is through the provider, because the AS has to pay money every time it sends traffic on this link. This leads to the basic rules of preference in interdomain routing, where customer routes are preferred over peer routes, which are in turn preferred over provider routes.\nThe other consideration that an AS has to make is filtering, or export decisions. In other words, given that an AS learns a route from its neighbor, to whom should it re-advertise that route? To understand filtering and export decisions, let\u0026rsquo;s add a couple more AS\u0026rsquo;s to the graph. Let\u0026rsquo;s add another peer, and let\u0026rsquo;s add another provider. Let\u0026rsquo;s call this AS in the middle of the picture Cox Communications. This ISP might have smaller regional customers and it might also buy transit connectivity from other providers. Now let\u0026rsquo;s suppose that this AS learns routes to a destination via its customer, its peer, and its provider. Now we already have established that it would prefer the customer route, so that it can make money by sending traffic to that destination. But what about filtering decisions? Well, routes that are learned from a customer, Cox of course would want to re-advertise to everyone else, because the more people use that route, the more money Cox makes. Therefore a route that\u0026rsquo;s learned from a customer, gets advertised to everybody else. On the other hand, a route that\u0026rsquo;s learned from a provider, if it were actually selected, would of course, only be advertised to customers. It wouldn\u0026rsquo;t make any sense to take a route like this and advertise it to another provider. The reason, of course, is that money is flowing in the direction of the providers. So any route that\u0026rsquo;s learned from a provider would never be advertised to another provider, because it would result in Cox essentially becoming a transit provider between two of its own providers and paying them both for the privilege of carrying that traffic. So routes learned from a provider would only ever be advertised to other customers. And similarly, routes from peers would only be advertised to other customers, not to other peers or other providers. So to summarize, interdomain routing has both ranking rules, where, given multiple choices, an AS might prefer a customer route over a peer route over a provider route. And then, given that it selected a particular route from either a customer, a provider, or a peer, it makes different decisions about where to re-advertise that route to other neighboring ASes. Now as it turns out, if every AS in the internet followed these rules exactly, then routing stability is guaranteed. Now you might wonder, isn\u0026rsquo;t routing stability guaranteed already? And it turns out that it isn\u0026rsquo;t.\nInterdomain Routing Can Oscillate! In fact, interdomain routing can oscillate indefinitely. To see why, consider the following 4 AS topology, where each AS specifies preferred paths, presumably via local preference. So each AS prefers the AS in the clockwise direction, rather than the shorter, direct path. Now it\u0026rsquo;s pretty easy to see that there\u0026rsquo;s no stable solution. Let\u0026rsquo;s suppose that we started off with everybody selecting the direct path. Well, in this case, any one of these ASes would notice that it has a more preferred path. So for example, AS 1 would see that because AS 3 has picked the direct path, then, in fact, it could prefer a situation where oscillations can occur indefinitely. Similarly, here now AS 3 sees that it has a more preferred path, 3 2 0, so it might switch to that.\nIn doing so, it breaks AS 1\u0026rsquo;s path. 1 3 0 no longer works. So AS1 has to switch back to its less preferred direct path, but now we\u0026rsquo;re in the same situation all over again because now AS2\u0026rsquo;s preferred path becomes available via 1, so AS 2 now reroutes, and AS 3\u0026rsquo;s most preferred path, 3 2 0, no longer works so it must switch to the direct path.\nNow, it\u0026rsquo;s very easy to see that this oscillation continues ad infinitum. This particular pathology was first discovered by Varadhan, Govindan, and Estrin, in a paper called persistent route oscillation in interdomain routing, in 1996. Later, Tim Griffin formalized this pathology and derived conditions for stability. Those stability conditions came to be known as a BGP correctness property called safety. It turns out that if ASes follow the ranking and export rules that we discussed, that safety is guaranteed. But, there are various times when those rules are violated. Business relationships, such as regional peering and paid peering, can occasionally cause those conditions to be violated. So as it turns out, to this day, BGP is not guaranteed to be stable in practice, and many common practices result in the potential for this type of oscillation to occur.\n"
},
{
	"uri": "/7646/machine-learning-algorithms/",
	"title": "Machine Learning Algorithms",
	"tags": [],
	"description": "",
	"content": " Machine Learning Algorithms In most cases, machine learning algorithms are focused on building a model. Then, the model can be used to take inputs and give outputs based on the model. The model is a tool to predict outputs based on the inputs. In our case, we will be using models to take information about stocks and predict their future prices.\nSo we use machine learning to take historical data and generate a model. When we want to use it, we give the model observations, $\\vec{x_i}$, and it gives us predictions, $y$. Examples of good inputs (predictive factors) are:\n Price momentum Bollinger value Current price  while examples of outputs would be:\n Future price Future return  We’ll first talk about Supervised Regression Learning.\nRegression and Modeling Supervised Regression Learning means that we’ll provide (supervised learning) a bunch of example data $(x, y)_i$ and allow the model to make a numerical prediction (regression). There are two main types of regression techniques:\n Linear regression (parametric) k-nearest neighbor (kNN) (instance-based), the more popular approach Decision trees Decision forests  Assessing a Model Assessing a model is much like predicting prices as it uses indicators to judge the effectiveness of the model. The first indicator is root mean square error (RMSE), which is as follows\n$\\sqrt{\\dfrac{\\sum (ytest - ypredict)^2}{N}}$  The error that is important is that of test data, which is outside of the training data. Typically 60% of the data is used for training, and 40% is used for testing. However, sometimes there isn’t enough data to adequately evaluate a learning algorithm, in which case a method called cross validation is used. This method slices the data into chunks, typically fifths. One is chosen as test and the rest are for training, then a different chunk is chosen to be the test and another trial is run. For financial data, we don’t want to accidentally look forward in time, so we would only use roll forward cross validation. This simply demands that all the training data is before the test data.\nThe second metric for how well an algorithm is working is the correlation of the test data and predicted values. Strong correlation, close to $\\pm1$, indicates a good algorithm whereas a weak correlation, close to zero, indicates a poor algorithm. Correlation and RMSE are excellent indicators on how well an algorithm is doing, but we might also want to fine tune an algorithm; we want to answer the question ”when are we trying too hard to fit data?”. This is where overfitting comes into play. Overfitting is the point at which error for training data is decreasing while error for test data is increasing.\nTypes of Learners Ensemble Learners Ensemble learners are composed of several different learners, which could include kNN, regression, and decision tree learners in one. The output of this learner is then simply a combination of the learners’ answers, which is typically an average of the outputs.\nBagging and Boosting Boot-strap aggregating, or bagging, only uses one algorithm, but many different models. If the training data is separated into learning instances where there’s a total of $n$ instances, then each model is fed a bag of these instances. Each bag is composed of $n\u0026rsquo;$ learning instances that are randomly selected with replacement, so instances may show up more than once in the same bag. These are used to train m models and the result is the average of all the outputs. Boosting builds each subsequent bag based on the results of the last. Training data is also used to test the models, and a model’s predicted data showing significant error is weighted to more likely be in the next bag for the next model. The process is continued for the desired number of models, and the results are averaged. Although this could be advantageous in predicting outliers, it’s also more susceptible to overfitting.\nReinforcement Learning As seen in figure 4.1, reinforcement learning describes the interaction of a robot with its environment. The robot performs an action, which has an effect on the environment and changes its state. The robot observes the change in state and its associated reward and makes decisions to maximize that reward.\nReinforcement learning also describes the problem that is how to go about maximizing the reward. In the stock market, the reward is return on trades, and we want to find out how to maximize returns. This problem is complicated by time constraints. The value of future gains diminishes with time, so it’s unreasonable to use an infinite horizon on which to base returns. However, optimizing returns over too short a time may limit rewards from seeing a much larger overall gain.\nMarkov Decision Problems What we’ve been talking about is called a Markov decision problem. Here’s how the problem is formalized.\nWe have:\n Set of states $S = {s1, \u0026hellip; , s_n}$ Set of actions $A = {a1, \u0026hellip; , a_n}$ Transition function $T[s, a, s_0]$ Reward function $R[s, a]$  What we’re trying to find is a policy $\\pi(s)$ that will maximize the reward. Unfortunately, we don’t know the $T$ or $R$, since that’s defined by the environment. So, the learner has to interact with the world and see what happens. Based on the reward, it can start generating policies.\nA way to encode this information is using experience tuples. Experience tuples are as follows: given a state $s_1$ and an action $a_1$ that we took, we were put into state $s'_1$ and got reward $r_1$. The tuple is shown like this:\n$\\langle s_1, a_1, s_1\u0026#39;, r_1 \\rangle$  Now, we can rename $s_1\u0026rsquo;$ to $s_2$, since that’s our new state, and then take a new action and see what happens, and we get a new tuple:\n$\\langle s_2, a_2, s_2\u0026#39;, r_2 \\rangle$  And we repeat this for many different combinations of states and actions, and then we’ll use the tuples to generate the policy. There are two ways to generate the policy:\n Model-based: For this method, we generate a model of $T[s, a, s_0]$ based on statistical analysis of the tuples. We look at a particular state and a particular action and see the probability of transitioning to another state. Same thing with $R[s, a]$. Then, we can use policy or value iteration to solve it. Model-free: This method keeps the data around and uses the original tuples to determine what the new state will be for a certain action. This is Q-learning.  Q-Learning Q-Learning is a model-free approach, which means that it doesn’t need to have any sort of model of the transition function $T$ or the reward function $R$. It builds a table of utility values as the agent interacts with the world. These are the Q values. At each state, the agent can then use the Q values to select the best action. Q-Learning is guaranteed to give an optimal policy, as it is proven to always converge.\n$Q$ represents the value of taking action $a$ in state $s$. This value includes the immediate reward for taking action $a$, and the discounted (future) reward for all optimal future actions after having taken $a$.\nHow do we use Q? What we want to find for a particular state is what policy, $\\prod(s)$ we should take. Using Q values, all we need to do is find the maximum $Q$ value for that state.\n$\\prod(s) = \\underset{a}{\\arg\\max}(Q[s,a])$  So, we go through each action a and see which action has the maximum $Q$ value for state $s$. Eventually, after learning enough, the agent will converge to the optimal policy, $\\pi^*(s)$, and optimal $Q$ table, $Q^*[s, a]$.\nHow do we get Q? To use the Q table, first we must generate it by learning. How do we go about that? Well it’s similar to previous learning algorithms, in that we provide it training data for which we know the outcomes. We then iterate over time and take actions based on the current policy (Q values), and generate experience tuples $\\langle s, a, s\u0026rsquo;, r \\rangle$ and generate the Q values based on the experience.\nIn a more detailed fashion:\n Initialize the Q table with small random values Compute $s$ Select $a$ Observe $r, s$ Update $Q$ Step forward in time, then repeat from step 2.  To update Q, we first need a formula to decide what it should be. What we can do is assign a learning rate, $\\alpha$, to weight the new observations. We can therefore update $Q$like this:\n$Q\u0026#39;[s, a] = Q[s, a] \u0026#43; \\alpha(\\textrm{improved estimate} - Q[s, a])$  As you can see, $Q$ converges as the improved estimate is the same as the current estimate (we’re at the best $Q$). Now, we need to know what the improved estimate is:\n$\\textrm{improved estimate} = \\textrm{immediate returns} \u0026#43; (\\textrm{discounted future rewards})$  Then, replacing discounted future rewards with the actual way of calculating it, and rearranging to only use the current value of $Q$ once, we find that the formula to calculate the new value of $Q$ for a state-action pair hs, ai, the formula is:\n$Q\u0026#39;[s, a] = (1 - \\alpha)Q[s, a] \u0026#43; \\alpha(r \u0026#43; \\gamma Q[s\u0026#39;, \\underset{a\u0026#39;}{\\arg\\max}(Q[s\u0026#39;, a\u0026#39;])])$  where:\n $r = R[s, a]$ is the immediate reward for taking an action $a$ in the state $s$ $\\gamma \\in [0, 1]$ is the discount factor to reduce the value of future rewards $s\u0026rsquo;$ is the resulting next state $\\underset{a\u0026rsquo;}{\\arg\\max}(Q[s\u0026rsquo;, a\u0026rsquo;])$ is the action which maximizes the Q-value among all possible actions $a\u0026rsquo;$ from $s\u0026rsquo;$, and $\\alpha \\in [0, 1]$ is the learning rate used to vary the weight given to new experiences compared to past Q-values. It’s typically around 0.2.  Exploration The success of a Q-learning algorithm depends on the exploration of the state-action space. If you only explore a small subset of it, you might not find the best policies. One way to ensure that you explore as much as possible is to introduce randomness into selecting actions during the learning phase. So basically, you see first whether you want to take the action with the maximal Q value or choose a random action, then if you take a random action, each action gets a probability which decreases over subsequent iterations.\nQ-Learning for Trading Now that we know what Q-learning is, we need to figure out how to apply it to the context of trading. That means that we need to define what state, action, and reward mean. Actions are straightforward, as there are basically three of them:\n Buy Sell Do Nothing  Our rewards can be daily returns or cumulative returns after a trade cycle (buy→sell). However, using daily returns will allow the agent to converge on a Q value more quickly, because if it waited until a sell, then it would have to look at all of the actions backwards until the buy to get that reward.\nNow, we just need to figure out how to determine state. Some good factors to determine state are:\n Adjusted Close/Simple Moving Average Bollinger Band value P/E ratio Holding stock (whether or not we’re holding the stock) Return since entry  Discretization Our state must be a single number so we can look it up in the table easily. To make it simpler, we’ll confine the state to be an integer, which means we need to discretize each factor and then combine them into an overall state. Our state space is discrete, so the combined value is the overall state. Say we have a state like this: The discretized state could be: 2950.\nTo discretize, what we do is take the data for a factor over its range, then divide it into n bins. Then we find the threshold by iterating over the data by the step size and taking the value at each position.\nstepsize = size(data) / n data.sort() for i in range(0, steps): threshold[i] = data[(i +1) * stepsize]  Problems with Q-Learning One main problem with Q-Learning is that it takes a lot of experience tuples to converge to the optimal Q value. This means the agent has to take many real interactions with the world (execute trades) to learn. The way this has been addressed is by using Dyna.\nDyna-Q Dyna is designed to improve the convergence of Q learning by building a model of $T$ and $R$ and then using Q learning to make decisions based on the model. However, the Q learning portion is still model-free, so it’s a mix of both.\nSo we do the Q-Learning steps, but after we take an action, we update the model of $T$ and $R$ with the new data, simulate a bunch of experiences based on the model, then update $Q$ based on these simulated experiences. To simulate the experiences, we basically generate random states and actions, and then find the new states/rewards based on the transition function and reward function.\nLearning T To figure out a model for $T$, what we can do is count the number of times that a transition to $s\u0026rsquo;$ by using the action $a$ in state $s$ occurred, then divide that by the total number of transitions to figure out the probability where $T_c$ is the number of times the transition occurred.\n$T[s, a, s\u0026#39;] = \\dfrac{T_c[s, a, s\u0026#39;]}{\\sum_{i} T_c[s, a, i]}$  Learning R To finalize the model, we need to find our expected reward, $R[s, a]$. Whenever we interact with the world, we get an immediate reward, $r$. We can use this to update our model for $R$ in a similar way to updating the $Q$ values where $\\alpha$ is again the learning rate:\n$R\u0026#39;[s, a] = (1 - \\alpha)R[s, a] \u0026#43; \\alpha r$  Conclusion So a summary of how Dyna-Q works is the following:\n MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], displayMath: [['$$','$$'], ['\\[','\\]']], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], TeX: { equationNumbers: { autoNumber: \"AMS\" }, extensions: [\"AMSmath.js\", \"AMSsymbols.js\"] } } });  MathJax.Hub.Queue(function() { // Fix  tags after MathJax finishes running. This is a // hack to overcome a shortcoming of Markdown. Discussion at // https://github.com/mojombo/jekyll/issues/199 var all = MathJax.Hub.getAllJax(), i; for(i = 0; i "
},
{
	"uri": "/6601/",
	"title": "6601",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/naming-addressing-and-forwarding/",
	"title": "Naming Addressing and Forwarding",
	"tags": [],
	"description": "",
	"content": " Naming, Addressing \u0026amp; Forwarding IP Addressing In this lesson we will be covering IP addressing. In particular we will be covering IPv4 address structure and allocation. IP stands for Internet Protocol, and version four is the version of the protocol that is widely deployed on the Internet to date. Each IP address is a 32-bit number. And that 32-bit number is formatted in what is called dotted quad notation. For example, the IP address for http://www.cc.gatech.edu, is 130.207.7.36. And this is just a convenient way of writing a 32 bit number. So 130 represents 8 bits, and 207 is another eight bit number, 7 is another eight bit number, as is 36. This structure allows for two to the 32, or about 4 billion Internet addresses. Now that sounds like a lot of addresses. As it turns out it\u0026rsquo;s actually not enough, and we\u0026rsquo;re running out of IP addresses, as I\u0026rsquo;ll discuss in a later lesson. But even if we only had to deal with two to the 32 Internet addresses, that\u0026rsquo;s still a lot. Think of it if you have to store every single IP address as an entry in a table. Very quickly that becomes an extremely large table where look- ups can be slow and the memory required to store such a large table might be expensive. So what we need is a more concise way of representing groups of IP addresses. There are different ways to group IP addresses and we\u0026rsquo;ll look at the prevailing method in the next part of the lesson. But first, let\u0026rsquo;s look at how this was done before 1994.\nPre 1994 Classful Addressing Before 1994, addresses was divided into a Network ID portion and a Host ID portion. So if we take our 32-bits, suppose the first bit is a zero. Note that that\u0026rsquo;s half of all IPv4 address space. Anything that starts with a 0 is going to be known as a class A address and the next 7 bits will represent the network ID or the network that owns this portion of address space. The remaining portion of the address space is dedicated for hosts on that network. In this case, any class A network can support up to two to the twenty-fourth hosts. Addresses that started with one zero were designated as class B networks, where the first 16 bits signified the Network ID and the remaining 16 bits signified the Host ID for that network. Note here that each class B address range represents about one sixty-five thousandth of all internet address space. So, discounting the first two bits which indicate that this is a class B network, we have about 2 to the 14th, unique class B\u0026rsquo;s, each of which can have two to the sixteenth, or 65,000 hosts on each network. Class C\u0026rsquo;s use the first 24 bits for the net ID and the remaining 8 for host ID. So each class Cnetwork essentially can have only 255 hosts on it.\nThis plot shows the BGP routing table size as a function of the year, starting in 1989, and going up to the internet routing table is quite small. It started at less than 5,000 prefixes. By comparison now we have about 500,000 IP prefixes in the internet routing table. But we can see in this period, that internet routing table growth began to accelerate, in particular the growth rates were exceeding the advances in hardware and software capabilities. And, in particular, we began to run out of the class C address allocation. There were far more networks that needed just a handful of IP addresses, such as a class C address space could provide, and yet because only a certain range of the IP address space could be used for class C addresses we began to run out fairly quickly. So there began to be a need for more flexible allocation. The solution to this problem is something called classless interdomain routing, or CIDR. Something that we\u0026rsquo;ll cover in the next lesson.\nIP Addressing Quiz As a quick quiz, suppose you have a class A address space which means that the network ID is eight bits. How many hosts can a single class A network support? Is it to the 2 to the 8th, 2 to the 16th, 2 to the 24th or 2 to the 32nd?\nIP Addressing Solution Each Class A address space has a network ID of 8 bits meaning that there are 24 bits that remain for the host ID. This means that each Class A network can support up to 2 to the 24th hosts.\nIP Address Allocation Let\u0026rsquo;s take a look at how IP address space is allocated to Internet service providers. At the top of the hierarchy is an organization called the Internet Assigned Numbers Authority, or IANA. IANA has the authority to allocate address space to what are called regional routing registries. For Africa, the regional registry is called AFRINIC, for Asia and Australia the registry is called APNIC, for North America, ARIN, for Latin America it\u0026rsquo;s LACNIC, and for Europe it\u0026rsquo;s RIPE. ARIN in turn allocates address space to individual networks, like Georgia Tech. The address space across registries is far from even.\nNow, this graph is from a journal article that\u0026rsquo;s a little bit dated now, but it gives you an idea of how many /8 address allocations have been allocated to each of the regional registries. So as of 2005 North America had 23 /8s allocated to it but the entire continent of Africa had only one. And the recent news is that IANA actually finished allocating all remaining /8 Internet address blocks, essentially meaning that we\u0026rsquo;re out of IPv4 address space. So when you hear that we\u0026rsquo;re out of IPv4 addresses, it doesn\u0026rsquo;t mean that you can no longer attach a new device to the Internet. There are various ways for coping with this pressure on address space. What that means is that IANA no longer has any more /8 blocks to give to these regional registries.\nQuerying an IP address using Whois and a routing registry, such as ra.net, will tell you the owner of that particular prefix. For example, if we run a Whois query on an IP address at Georgia Tech, it will tell us that that IP address is from a /16 allocation, that Georgia Tech is the owner of the prefix, and it\u0026rsquo;s associated with this autonomous system number. The routing registry entry also gives us some additional information, such as who to contact if we need to contact the owner of this address space.\nClassless Interdomain Routing The pressure on address space usage spurred the adoption of classless interdomain routing or CIDR which was adopted in 1994. The idea is that instead of having fixed network ID and host ID portions of the 32 bits, instead, we would simply have an IP address, and what is known as a mask, where the mask is variable length and indicates the length of the network ID. So, for example, suppose we have an IP address like 65.14.248.0/22. Well in this case our 32 bits look like so, but this doesn\u0026rsquo;t tell us how long the network IDand how long the host ID should be. The /22 indicates the mask length, which says that the first 22 bits should represent the network ID.\nNow the key is that this mask can be variable length. And the mask length no longer depends on the range of IP addresses that are being used. This allows those allocating IP address ranges, to both allocate a range that\u0026rsquo;s more fitting to the size of the network and also not have to be constrained about how big the network ID should be depending on where in the IP address space the prefix is being allocated from. Of course now the complication is that it\u0026rsquo;s possible to have overlapping address prefixes. For example, 65.14.248.0/24 overlaps with 65.14.248.0/22. The red prefix is actually a subset of the black one. So supposing these two entries both show up in an Internet routing table, what are we supposed to do? The solution is actually to forward on what\u0026rsquo;s called the longest prefix match, meaning that if a routing table has two overlapping entries that it should forward according to the entry that has the longest prefix, or the longest mask length. Intuitively that makes sense because the prefix with the longer mask length is more specific than the prefix with the shorter mask, or the larger prefix.\nLongest Prefix Match Let\u0026rsquo;s take a closer look at longest prefix match. So each packet has a destination IP address, which determines where the package should be forwarded next, and a router basically looks up a table entry that matches that address. So, for example, a forwarding table might have a number of prefixes in it, and many of these prefixes might be overlapping. But when we see an IP address, it may match on one or more prefixes in this table, you simply match that IP address to the entry in the forwarding table with the longest matching prefix. So the benefits of cider and longest matching prefix are efficiency, since prefix blocks can be allocated on a much finer granularity than with classful inter-domain routing, and the opportunity for aggregation if two downstream networks with more specific or longer prefixes, should be treated in the same way by an upstream network, who might simply aggregate two contiguous shorter prefixes into one forwarding table entry with a shorter prefix. For example, a benefit for aggregation might exist if two downstream networks A and B each had slash 16 address space allocated to them. But upstream, all the traffic always came through the same upstream network, C. If the rest of the internet only reached A and B via C, then the rest of the internet need only know about C\u0026rsquo;s address space which might be 12\u0026frasl;8. This might allow the upstream network to simply aggregate, or not announce these more specific prefixes, since they\u0026rsquo;re already covered by the less specific upstream prefix.\nNow cider had a significant effect on slowing the growth of the internet routing tables from 1994 to 1998. So, from 1994 to 1998, we see roughly linear growth in the number of prefixes in the internet routing table. Around 2000, fast growth in routing tables resumed.\nYou can see that growth here once again started to pick up a significant contributor to this growth, was a practice called multi-homing. Multi-homing can actually make it difficult for upstream providers to aggregate IP prefixes together, often requiring an upstream provider to store multiple IP prefixes for a single autonomous system. Sometimes those IP prefixes are contiguous and sometimes they aren\u0026rsquo;t. Let\u0026rsquo;s take a quick look at how multi-homing can stymie aggregation.\nMultihoming Frustrates Aggregation This example, a stub AS, in this case 30308, might receive IP address space, say, 12.20.249\u0026frasl;24, from one of its providers, such as AT\u0026amp;T, which happens to own 12.0.0.0/8. Now in this case AS 30308 wants to be multihomed. In other words, it wants to be reachable via two upstream Internet service providers. In this diagram, the two Internet service providers are AT\u0026amp;T and Verizon. To be reachable by both of these ISPs, AS 30308 has to advertise its prefix, which it received from AT\u0026amp;T via both AT\u0026amp;T and Verizon. The problem occurs when AT\u0026amp;T and Verizon want to advertise that prefix to the rest of the internet. Well, unfortunately, although AT\u0026amp;T might like to aggregate this prefix as I previously described, it can\u0026rsquo;t. If it did, Verizon would still be advertising the longer /24 via it\u0026rsquo;s upstream link. And because of longest prefix match, all of the traffic would then arrive via the Verizon link regardless of what AS 30308 wanted to have happened to that incoming traffic.\nAs a result, both AT\u0026amp;T and Verizon must advertise the same /24 to the rest of the internet. This results in an explosion of /24s in the global internet routing table. You can imagine, that if a lot\nof stub AS\u0026rsquo;s wanted to be multihomed, then suddenly, we\u0026rsquo;ve got a lot more /24s in the global routing table than might otherwise exist without multihoming.\nLongest Prefix Match to Control Inbound Traffic Now in a previous lesson, we looked at how AS path prepending, can be used to control inbound traffic. As it turns out, longest prefix match can also be used to control inbound traffic. Suppose that AS A owns 10.1.0.0/16, and it might advertise that prefix out both of its upstream links and that route might similarly be advertised further upstream. Now of course as we know from a previous lesson, given the advertisement of one prefix upstream, AS D is going to pick one best BGP route along which to send traffic back to A. But let\u0026rsquo;s suppose that AS A wanted to balance that traffic across its incoming links. Well in that case, ASA could actually advertise routes for 2 more specific prefixes, effectively splitting the slash 16 in half, so in addition to advertising 10.1\u0026frasl;16, across both links, AS A might advertise 10.1\u0026frasl;17 on the top link and 10.1.128.0/17, the other half of the /16 on the bottom link. Now, if either link fails, the covering /16 will ensure that the prefix remains reachable by one of the two upstream links. But because longest prefix match wins, the traffic for 10.1.128 would now traverse the bottom link, and the traffic for 10.1/ would now traverse the top link, effectively sending traffic for half of the prefixes along the top path and traffic for the other half of the prefixes along the bottom path. Although we just explored a perfectly good reason to deaggregate a contiguous prefix, it turns out that sometimes autonomous systems may deaggregate larger prefixes unnecessarily.\nA report called the CIDR Report, which is released weekly, shows autonomous systems who are advertising IP prefixes that, at least according to observation, are continuous and could be aggregated. For example, the top offender for the week of December 12th, 2013, was AS6389. This single autonomous system is actually advertising more than 3,000 unique IP prefixes. The CIDR report analysis suggests that with appropriate aggregation, this autonomous system could instead advertise only 56 unique IP prefixes. Now this might be overly optimistic. As we just explored, there are perfectly good reasons to deaggregate a contiguous IP prefix into multiple smaller contiguous IP prefixes. But nonetheless, the report shows that there are probably a lot more IP prefixes in the Global Internet Routing table than there could be if AS\u0026rsquo;s took full advantage of aggregation.\nCIDR Quiz Let\u0026rsquo;s have a quick quiz about cider. So, how many IP addresses does a /22 prefix represent? Two to the 22, two to the 32, two to the tenth, or two to the eighth?\nCIDR Solution The /22 represents the length of the network ID, and the remaining 10 bits are for hosts in that /22 prefix. So those 10 bits reserved for the host for that /22 mean that this /22 prefix represents 2 to the tenth IP addresses.\nLookup Tables and How LPM Works (put with other slide) Okay, in this lesson, we will explore how lookup tables in routers are designed and how longest prefix match works; we\u0026rsquo;ll explore exact match versus longest prefix match and when each is used; we\u0026rsquo;ll explore IP address lookup in more depth; and finally, we\u0026rsquo;ll explore how longest prefix match is implemented in the form of tries.\nLookup Algorithm Depends on Protocol So, the look up algorithm that a router uses depends on the protocol that it\u0026rsquo;s using to forward packets, and the look up mechanism might be implemented with a variety of different algorithms or techniques. For example, MPLS, Ethernet, and ATM use an exact match look up. Exact matches can be implemented as a direct look up, an associative look up, hashing, or via a binary tree. IPv4 and IPv6 on the other hand are implemented with what\u0026rsquo;s called longest prefix match. We\u0026rsquo;ve already looked at longest prefix match a little bit in some lessons and, in this lesson we\u0026rsquo;ll look at it in a bit more detail as well as how it\u0026rsquo;s implemented. It might be implemented as a radix trie, a compressed trie, which is something that we will look at in this lesson, and it can also be implemented as a binary search on the prefix intervals. Ethernet based forwarding is based on exact match of a layer two address which is usually 48 bits long. It\u0026rsquo;s address is global, not just local to the link. And the range or size of the address is not negotiable. Now 2 to the 48th is far bigger than 2 to the 12th, therefore, it\u0026rsquo;s not possible to hold all the addresses in the table and use direct look up. The advantages of exact matches and Ethernet switches is that exact match is simple and the expected lookup time is small, or O of 1. But the disadvantages include inefficient use of memory. This potentially results in nondeterministic lookup time if the lookup might require multiple memory accesses. Lets now take a closer look at longest prefix match.\nIP Lookups Find Long Prefixes IP lookups find longest prefixes. Let\u0026rsquo;s suppose that we want to represent a particular IP address as one point in the space from zero to 2 to the 32 minus 1, or the range of all 32 bit IP addresses. Each prefix represents a smaller range inside this larger range of 32-bit numbers. Obviously, this is not to scale. Now these ranges, of course, might be overlapping, as is shown here, and the idea is that longest prefix match will match the smallest prefix for which the IP address range overlaps that of the specified IP address. So longest prefix match is harder to perform than exact match. For one, the destination IP address does not indicate the length of the longest matching prefix, so some algorithm needs to determine the length of the longest matching prefix, which in this case is 21. So we somehow need a way to search the space of all prefix lengths, as well as prefixes of a given length.\nLPM in IPv4 Exact Match Suppose, for example, that we wanted to implement longest prefix match for IPv4 using exact match. Now in this case we might take our network or our IP address, and send it to a bunch of different exact match tables. And then among the tables that had a match, we would select the longest, and then forward the packet out the appropriate output port. Of course, this is horribly inefficient, because we\u0026rsquo;d have to have tables for each of these links, and every time a packet arrived, we\u0026rsquo;d have to send it to each one of these 32 tables. This is extremely wasteful of memory.\nAddress Lookup Using Tries An alternative is to perform address lookups using a data structure called a trie. In a trie, prefixes are spelled out by following a path from the root. And to find the best prefix, we simply spell out the address in the trie. For example, let\u0026rsquo;s suppose we had the following table. Such a lookup table has entries of varying lengths. Let\u0026rsquo;s see how this might be encoded in a trie. In a trie, spelling out the bit one always takes us to the right, and spelling out the bit zero always takes us to the left. So to insert one one one star, we\u0026rsquo;d basically start here. One. One. One. And then we insert P1, and then we repeat this process. One zero star results in P2. One zero one zero results in P3. And one zero one zero one results in P4. If we want to insert one one one zero, insertion is easy. We can simply insert P5 as such. Look ups are easy, so for example let\u0026rsquo;s suppose we want to look up 10111. Well all we have to do, is spell this out in the trie. So we can follow 1-0-1 and now, we see that there\u0026rsquo;s no entry for 1011. So, we use the entry of the last node in the tree that we traverse that has an entry, in this case P2. Now this structure here is what\u0026rsquo;s called a single bit try. Single bit tries are very efficient. Note that every node in this try exists due to one of the five folding table entries that we\u0026rsquo;ve inserted in the try. So, a single bit trie is a very efficient use of memory. Updates are also very simple. We saw how easy it was, to insert the entry for P5. Unfortunately, the main problem is the number of memory accesses that are required to perform a lookup. For 32 bit address, we can see, that looking up the address in a single bit trie, might require 32 look ups, in the worst case, one for each bit. So it\u0026rsquo;s each bit in the address requires, one traversal in the trie, or one memory look up. So this could be very bad. At worst, 32 accesses in the worst case. To put this in perspective, an OC48 requires a 160 nanosecond lookup, or simply 4 memory accesses. So 32 accesses, is far too many, especially for high speed links.\nDirect Trie The other extreme, of course, is to use a direct trie where instead of 1 bit per look up we might have 1 memory access responsible for looking up a much larger number of bits. So, for example, we might have a two level try where the first memory access is dictated by the first 24 bits of the address, and the second memory access is dictated by last 8 bits of the address. Now here we can look up an entry in the forwarding table with just two memory accesses. The problem is that this structure results in a very inefficient use of memory, unlike the single bit trie. To see why, suppose that we want to represent a /16 prefix. Well unfortunately we have no way of encoding a lookup that\u0026rsquo;s just 16 bits. We have to rather encode 2 to the 8th identical entries, corresponding to the 2 to the 8th /24 prefixes that are contained in that /16, so this is extremely inefficient use of memory.\nDirect Trie Quiz As a quick quiz, suppose we have a direct trie, and the first level is 16 bits, the next level is eight bits, and the third level is the final eight bits. In the worst case, how many accesses would be required per lookup?\nDirect Trie Solution Because the Trie has a depth of three, in the worst case, a look up might require three memory accesses.\nDirect Trie Quiz 2 How many entries, would I need, to represent a /20 prefix?\nDirect Trie Solution 2 A /20 prefix is 2 to the 4th, or 16, /24\u0026rsquo;s. And I need to basically represent 16 entries, at the 24 bit level of the trie, or the second level, and therefore, I\u0026rsquo;d need 16 entries to represent, a /20 prefix.\nMemory Efficiency and Fast Lookup To achieve the memory efficiency of a single bit trie with the fast lookup properties of a direct trie, a compromise is to use what\u0026rsquo;s called a multi-bit trie, or a multi-ary trie. Let\u0026rsquo;s start with a binary trie, where one bit is resolved at each node. Here, the depth is big W, the degree of each node is two, and the stride for each lookup is one bit. Now we can generalize this to a multi-ary trie, where the depth is now W over K if the degree is 2 to the K, and each level resolves K bits. The binary trie is a simple case of the multi-ary trie, where K equals 1.\n4 ary Trie Let\u0026rsquo;s take a look at the 4-ary trie where k equals 2. Suppose we have the same forwarding table as before. But now, each node in the trie is responsible for resolving two bits. So if we take one one, and now we take one star, that\u0026rsquo;s one zero and one one. And now we basically have to put p in two places in the tree. One zero star results in just one entry. 1010 star results in two traversals, and 10101 star again represents two entries, for 101010 and 101011. Now suppose we want to look up 10111. Again, we can spell this out, 101, and we can see that we get no further than P2 and again, we match at P2.\nOne thing we can do to save space further is create what\u0026rsquo;s called a leaf-pushed trie. In such a setting, we can save our self some space. Instead of having these pointers, we can push these entries into the left and right side of this node, respectively.\nSo 10 becomes P1 on the left side and 11 becomes P1 on the right side. There are variety of other optimization algorithms, including one called Lulea and another called Patricia. Each of them use the same basic idea that we have explored here, except some of them like Lulea are a three level trie, and often they use a bitmap to compress out repeated entry such as those that exist here.\nAlternatives to LPM with Tries Now there are alternatives to implementing longest prefixes match with a trie. One could start with a content addressable memory or a CAM. Now a CAM is a hardware base route look up where the input is a tag and the output is a value. So, for example, the tag might be an address and the value might be the output port. Now the CAM really only supports exact match but it is an O of 1 lookup. There is something called a ternery CAM, where instead of exact matching In the tag, you can have 0, 1, or don\u0026rsquo;t care, or a star. The ternary CAM and in particular its support for a wild card permits an implementation of longest prefix match. One can thus have multiple matching entries, but prioritize the match according to the longest prefix in the ternary CAM.\nNAT and IPv6 Let\u0026rsquo;s now talk about various problems that resulted from IPv4 and the growth of the internet routing tables, and two different solutions to internet routing table growth: network address translation, or NAT, and IPv6. So the main problem that we are seeing is that IPv4 addresses have only 32 bits, which means that there can only be a total of 2 to the 32 unique IP addresses. Not only that, as we\u0026rsquo;ve seen, IP addresses are allocated in blocks, and fragmentation of this space can mean that IPv4 addresses can be quickly exhausted. In fact, we\u0026rsquo;ve already seen the last slash eight from IPv4 address space allocated to the registries. So we\u0026rsquo;re well on our way towards running out of IPv4 addresses. In some sense, you can say that we\u0026rsquo;ve essentially already run out. In this lesson, we\u0026rsquo;re going to look at two solutions: network address translation, or NAT, and IPv6, whose main feature is 128 bit addresses. Let\u0026rsquo;s first take a look at NAT.\nNetwork Address Translation NAT allows multiple networks to reuse the same private IP address space. Let\u0026rsquo;s suppose that we have two networks. These networks might be, for example, homes or they might be larger networks in regions of the Internet, where IPv4 address space is scarce, for example, in developing regions. What NAT allows these networks to do is reuse the same portion of internet address space. For example, a particular, special, private IP address space, is 192.168\u0026frasl;16. Other private IP address space is specified in RFC 3130. Now, obviously these two networks couldn\u0026rsquo;t coexist on the public Internet, because routers wouldn\u0026rsquo;t know if they got a packet destined for an IP address in this space, which network the packet should be sent to. What a NAT, or a Network Address Translator does, is take the private IP addresses that are behind the NAT and translate those IP addresses to a single, globally visible IP address.\nNow, to the rest of the Internet, network one appears to be reachable by a single IP address, 203.178.1.1, and network two is reachable via a single distinct global IP address 133.4.1.5. Now, a host back here, say 192.168.1.10 might send a packet towards a global internet destination. Now, the key behind NAT is that this packet has a source port and the NAT is basically going to take that source IP address and port and it\u0026rsquo;s going to translate it into a publicly reachable source IP address and port, and the destination will remain the same. That packet will make its way to a global destination and the reply will make its way to the globally reachable IP address on the corresponding port. Now, when that packet with that particular destination IP address and port reaches the NAT, the NAT has a table that knows the mapping between that public IP address and port and the private one that it rewrote to generate the corresponding public IP address and port. So we can simply now rewrite the destination IP address of this packet to the corresponding private address and port. NATs are popular on broadband access networks, small or home offices and VPNs. There\u0026rsquo;s a clear savings in IPv4 address space, since there can be many devices in one of these private networks and yet all of the devices that are behind the NAT only use up one public IP Address. The drawback, of course, is that the end-to-end model is broken. And we talked about the end-to-end model in a previous lesson and let me just remind you how NAT breaks it. If the NAT device failed in this instance, for example, the mapping between the private source IP address and port and the public source IP address and port would be lost, thereby breaking all active connections for which the NAT is on the path. It\u0026rsquo;s also asymmetric. Under ordinary circumstances it\u0026rsquo;s rather difficult for a host on the global Internet to reach a device in a private address space in network one or network two, because by default those devices in these private networks do not have public globally reachable IP addresses. So, NAT both breaks end- to-end communication, and it also breaks by directional communication.\nIPv4 to IPv6 Another possible solution to the IP address space exhaustion problem is to simply add more bits. This is the gist of the contribution of the IPv6 protocol. Here\u0026rsquo;s a picture of the IPv4 protocol header, and all of the fields shown in red have basically been removed in IPv6, resulting in both a much simpler header and addresses that are much larger.\nBy contrast, here\u0026rsquo;s the IPv6 header. The IPv6 header provides 128 bits for both the source and destination IP addresses. Now the format of these addresses are as follows. Of the 128 bits, the top 48 bits are for the public routing topology, and we have a 16-bit site identifier. And finally, a 64-bit interface ID, which effectively has the 48-bit Ethernet address of the interface plus 16 more bits. Now, the top 48 bits can be broken down further. They include top level provider, something like a tier one ISP, 8 reserve bits, and 24 additional bits. Now, note that there are 13 bits in the top 48 that directly map to the tier one ISP, meaning that addresses are purely provider-based, thus changing ISPs would require renumbering. IPv6 has many claimed benefits. There are more addresses, the header is simpler, multihoming is supposedly easier, various aspects of security are built in, such as the IPv6 crypto extensions. Now despite all of these benefits, we have yet to see a huge deployment of IPv6 yet.\nIPv6 Routing Table Entries Now despite all of these benefits, we\u0026rsquo;ve yet to see a significant deployment of IPv6. Here you can see the number of routing table entries for IPv6 routes, as well as the growth over time from 2004. To the end of 2013. What\u0026rsquo;s remarkable is that we only see 16,000 IPv6 routes in the global routing table. This is not that many considering that there are about 500,000 IPv4 routes in the global routing table. The problem is that IPv6 is very hard to deploy incrementally. Remember our discussion of the narrow waist. Everything runs over IPv4 and IPv4 was designed to run over a variety of physical layers. This common protocol has allowed tremendous growth, but because everything depends on the narrow waist of IPv4 and because IPv4 is built on top of so many other types of infrastructure, changing it becomes extremely tricky. Incremental deployment where part of the internet is running IPv4 and other parts have been upgraded to IPv6 results in significant incompatibility. There are various incremental deployment options for IPv6.\nIPv6 Incremental Deployment One is what\u0026rsquo;s called a dual stack deployment. In a dual stack deployment a host can speak both IPv4 and IPv6. It communicates with an IPv4 host using IPv4 and communicates with an IPv6 host using IPv6. What this means is that the dual stack host has to have an IPv4 compatible address. Either the host has both an IPv4 and an IPv6 address, thus allowing it to speak to an IPv4 host, or it must rely on a translator which knows how to take a v4 compatible IPv6 address, and translate it to the v4 address. One possible way of ensuring compatibility of a v6 address with IPv4, is simply to embed the IPv4 address in 32 bits of the 128 that are allocated for the IPv6 address.\nNow, a dual stack host configuration or a v4 compatible IPv6 address solves the problem of host IP address assignment, but it doesn\u0026rsquo;t solve the problem that IPv6 deployments might exist as islands. For example, multiple independent portions of the Internet might deploy IPv6, but what if the middle of the network only speaks in routes IPv4? The solution here is to use what\u0026rsquo;s called 6 to 4 tunneling. In 6 to 4 tunneling, a v6 packet is encapsulated in a v4 packet. Now, that v4 packet is routed to a particular v4 to v6 gateway corresponding to the v6 address that lies behind that gateway. And at this point the outer layer of encapsulation can be stripped, and the v6 packet can be sent to its destination. This of course, requires the gateways at the boundaries between the v4 and v6 networks to perform encapsulation of the packet as it enters the v4 only part of the network, and de-capsulation as the packet enters the v6 island, where the destination host resides.\n"
},
{
	"uri": "/7641/",
	"title": "7641",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/router-design/",
	"title": "Router Design",
	"tags": [],
	"description": "",
	"content": " Router Design In this lesson, we will cover the design of big, fast, modern routers. Here\u0026rsquo;s a picture of two modern router chassis. On the left, we have a picture of the Cisco CRS-1, and on the right, we have a picture of the Juniper M320. The M320 has for some time been used at the border of the Georgia Tech network between Georgia Tech and the rest of the Internet.\nHere\u0026rsquo;s a picture of a couple of line cards that go into these chassis. These kind of look like network interface cards, except the ports are special. Instead of terminating Ethernet, these ports terminate high capacity fiber links. As you can see, these cards are actually a whole lot bigger than your typical network interface card as well. And, as a result, these chassis are often anywhere from three to six feet tall, and can fill up an entire rack.\nThere\u0026rsquo;s a significant need for big, fast routers. Links are getting faster. Traffic demands are also increasing, particularly with the rise of demanding applications, such as streaming video. Networks are getting bigger too, in terms of the number of hosts, routers, and users. So there\u0026rsquo;s a perennial need to design big, fast routers, particularly in Internet backbone networks. The rest of this lesson will focus on how a router works, in particular, how it goes from the process of taking a packet as input and sending it on to where it needs to go. The Internet\u0026rsquo;s routing protocols, of course, are responsible for populating the forwarding tables on a router. But once those tables are populated, the router still has the hard job of taking a packet as input and ultimately getting it to the appropriate output port, so that the traffic can proceed en route to the destination.\nBasic Router Architecture Let\u0026rsquo;s take a look at a generic router architecture. As a summary of basic router function, a router receives a packet. It then looks at the packet header, to determine the packet\u0026rsquo;s destination. It looks in the forwarding table to determine the appropriate output interface for the packet. It modifies the packet header, such as decrementing the time to live field and updating the IP header check sum appropriately. And finally, it sends the packet to the appropriate output interface. The basic I/O component of a router architecture is the line card, which is the interface by which a router sends and receives data. When a packet arrives, the line card looks at the header to determine the destination, and then it looks in the forwarding table to determine the output interface. It then updates the packet header and finally sends the packet to the appropriate output interface. Now this drawing shows just a single line card. But in fact, when the packet is sent to the output interface, it must traverse the router\u0026rsquo;s interconnection fabric, to be sent to the appropriate output port.\nSo in fact, we can zoom out from that depiction of a single line card, and what we have is a bunch of line cards that are all connected via an interconnection fabric. Each of these line cards has a lookup table, the capability to modify headers, and a queue, or buffer, for packets as they enter and leave the line card. In other lessons we talk about several important questions such as how big queues should be and how lookup works. In the rest of this lesson, I\u0026rsquo;ll discuss important decisions in router design such as the placement of lookup tables on each line card and the design of the interconnection fabric.\nEach Line Card Has Own Forwarding Table Copy One important decision in the design of the modern routers was to place a copy of the forwarding table on each line card in the router. Well, this introduces some complications in making copies of the forwarding table. Doing so prevents a central table on the router from becoming a bottleneck at high speeds. Consider an alternative where the router has only one copy of the forwarding table. In that case all of the line cards would need to be performing look ups on a central table which involves communication across the back plane as well as many more look ups against a central table. So while distributing the forwarding table across line cards prevents a central table from becoming a bottleneck, early router architectures did not place the look up table on each line card. And as a result, when packets arrived at an individual line card, they would induce a look up in a shared buffer memory which could be accessed over a shared bus. But this shared bus, of course, introduces a bottleneck, as well as contention between the different line cards that may be all performing lookups to the same shared memory. The solution, of course, was thus to remove the shared memory and instead place copies of the forwarding table on each line card. In summary, an important innovation in the design of these router was to eliminate the shared bus and place the look up table on individual line bus.\nDecision Crossbar Switching The second important decision is the design of the interconnect, or how the line cards should be connected to one another. Now one possibility is to use a shared bus. But the disadvantage of a bus for the interconnect is that it can only be used by one input-output combination in any single time slot. What we\u0026rsquo;d like to do is enable input output pairs that don\u0026rsquo;t compete to send traffic from input to output during the same time slot. For example, one should be able to send one to four, two to six and three to five, all in the same time slot. The solution to this problem is to create what\u0026rsquo;s called a crossbar switch, or sometimes is also called a switched backplane.\nCrossbar Switching In crossbar switching every input port has a connection to every output port, and during each time slot, each input is connected to zero or one outputs. The crossbar is often depicted as follows. So if one wants to send to four, we could connect the input to the output in that time slot, and now this row and this column is occupied. But we could connect two to six and three to five in the same time slot without introducing contention. So the advantage of this design is that it can exploit parallelism by allowing multiple packets to be forwarded across the interconnect in parallel. But of course we also need proper scheduling algorithms to ensure fair use of the crossbar switch. Let\u0026rsquo;s take a quick look at what this algorithm needs to achieve.\nSwitching Algorithm Maximal Matching We\u0026rsquo;d like the cross bar switching algorithm to achieve what\u0026rsquo;s called a maximal matching. Conceptually we have a router with n inputs and n outputs, but of course the inputs are also outputs. It\u0026rsquo;s just easier to think about the inputs and the outputs being separate when we talk about the switching problem. Now in each time slot we would like to achieve a one-to-one mapping between inputs and outputs, which is a matching. And our goal is that the matching should be maximal. So in a particular time slot, we might have a certain set of traffic demands, or traffic at certain input ports, that is destined for certain output ports. And our goal is, given these demands to produce a matching that is maximal and fair. Now, given demands for a particular time slot and the resulting matching, notice that certain demands were not satisfied. These packets that arrived at inputs must wait until the next time slot to be forwarded to the appropriate output port, because they couldn\u0026rsquo;t be matched in the same time slot as those shown here. Remember that there must be exactly a one-to-one matching between any inputs and outputs in a particular time slot. Most router crossbars have a notion of speedup whereby multiple matchings can be performed in the same time slot. So, for example, if the line cards are running at, say, ten gigabits per second, then running the interconnect twice as fast would allow matchings to occur twice as fast, as packets would arrive on the inputs or be forwarded from the outputs. It is thus common practice to run the interconnect at a higher speed than the input and output ports. Just speeding up the interconnect does not solve all problems. Note, for example, that in this set of demands we have packets arriving at this input port destined for this output port, but if there\u0026rsquo;s only a single queue at this input, the packets that are destined for the output port circled in orange, might actually be blocked behind a set of packets that are destined for other output ports. So even if we could induce a speed up at the interconnect, certain packets may be blocked in the queue by packets ahead of them destined for other output ports.\nHead of Line Blocking For example, if we have packets arriving in this queue destined for the orange queue, at the front of the queue, then even with the speed up, there may be packets that are sufficiently far behind in the queue that they\u0026rsquo;re waiting behind the orange packets. What we\u0026rsquo;d like to be able to do is perform matchings to allow these packets to be sent to the output ports, and not have to wait for the entire queue to be drained of packets destined for the orange output port.\nA solution is to create virtual output queues, where instead of having a single queue at the input, we have one queue per output port. This prevents packets at the front of the queue that are destined for a particular output port from blocking packets that could otherwise be matched to other output queues in earlier timeslots.\nScheduling and Fairness Let\u0026rsquo;s now talk about scheduling and fairness. And when we talk about, in a crossbar switch, the process of matching input ports to output ports, the decision about which ports should be matched in any particular time slot is a process called scheduling. There are two important goals in scheduling. One is efficiency, which is to say that if there is traffic at inputs destined for output ports, the crossbar switch should schedule inputs and outputs so that traffic isn\u0026rsquo;t sitting idle at the input ports if some traffic could be sent to the available output ports. Another consideration in scheduling is fairness, which is to say that given demands at the inputs, we want to make sure that each queue at the input is scheduled fairly for some definition of fairness. Now, defining fairness is tricky. And there are multiple possible definitions of fairness. Here, we\u0026rsquo;ll look at an important fairness definition called max min fairness.\nMax Min Fairness Now, to define max-min fairness, let\u0026rsquo;s first assume that we have some allocation of rates across flows x_i. Now, we say that this allocation is max-min fair if increasing any rate x_i implies that some other x_j that is smaller than x_i must be decreased to accommodate for the increase in x_i. So in other words, the allocation is max-min fair if we can\u0026rsquo;t make any one of these flow rates better off without making some flow rate worse off, that\u0026rsquo;s already worse than the flow rate x_i. So the upshot results in small demands getting exactly what they asked for, and the larger demands splitting the remaining capacity among themselves equally. More formally, we perform this procedure as follows. We allocate resources to users in order of increasing demand. No user receives more than what they requested. And users that still have unsatisfied demands, split the remaining resources.\nMax Min Fairness Example Let\u0026rsquo;s consider an example for max-min fair allocation. Let\u0026rsquo;s suppose that we have a link with capacity ten and four demands: 2, 2.6, 4, and 5. Now, obviously the demands exceed capacity. So we need to figure out a way of allocating rates to each of these demands that is max-min fair. First, note that 10 divided by 4 is 2.5. But this is not a good solution, because the first user only needs 2. So the first user would have an excess of .5 under this allocation. So what we want to do is take this excess of .5 and divide it among the remaining 3 users, whose demands have not yet been fulfilled. This would yield an allocation of 2, 2.67, 2.67, and 2.67. But now user two has an excess of 0.07, so we take that excess, divide it among the remaining 2, and that gives us our final max-min fair allocation. Note that this is called max-min fairness, because it maximizes the minimum share to each user whose demand is not fully serviced. In this case, the 3rd and 4th users.\nMax Min Fairness Quiz As a quick quiz, let\u0026rsquo;s try doing a max min fair allocation. Suppose that we have demands of one, two, five, and ten, and link, whose rate is 20. Please give the max-min fair allocation across these four users.\nMax Min Fairness Solution To compute the max min fair allocation, we take 20 and we divide it by 4, which yields 5. But the first user only needs one, which yields an excess of four. The second user only needs two, which yields an excess of three. So in this case the max min fair allocation is easy. We simply take this excess of seven and give it to the only user whose demand is not yet satisfied, resulting in the max min fair allocation of one, two, five, and twelve.\nHow to Achieve Max Min Fairness Now, how do we achieve max-min fairness? One approach is via round robin scheduling, where given a set of cues, the router simply services them in order. The problem here is that packets may have different sizes. So if the first queue had a huge packet, and the second queue had a little packet, and the third queue had a medium sized packet, then servicing these queues in order obviously isn\u0026rsquo;t fair. Because the first queue would effectively get more of its fair share, because its packet just happened to be bigger. An alternative is to use bit by bit scheduling, where during each time slot, each queue only has one bit serviced. This, of course, is perfectly fair, but the problem is feasibility. How do we service one bit from a queue? A third alternative is called Fair Queuing, which achieves max-min fairness by servicing packets according to the soonest finishing time. A Fair Queuing algorithm computes the virtual finishing time of all candidate packets, which are the packets at the head of all non-empty flow queues. Based on these virtual finishing times, Fair Queuing compares the finishing times of each queue and services the queue with the minimum finishing time. So the queue whose packet has the minimum virtual finishing time is serviced.\n"
},
{
	"uri": "/6250/dns/",
	"title": "DNS",
	"tags": [],
	"description": "",
	"content": " DNS: Domain Name System Domain Name System Part 1 We\u0026rsquo;ll now have a look at the domain name system or DNS. The purpose of the domain name system is to map human readable names such http://www.gatech.edu to IP addresses such as 130.207.160.173. A name such as this is human readable and much easier to remember and type than an IP address. But in fact, the IP address is what\u0026rsquo;s needed to send traffic to the intended destination. So, we need a lookup mechanism that takes a human readable name and maps it to an IP address. The system that does this is a Domain Name System, or the DNS. The system roughly works as follows. A client may want to look up a domain name such as http://www.gatech.edu. A networked application source code might do so by invoking a function such as get_host_by_name, which takes as an argument a domain name and returns an IP address. The client typically has what\u0026rsquo;s called a stub resolver, and that stub resolver takes that name and issues a query. The stub resolver might have cached the answer or the IP address corresponding to this name, but if not, the query is sent to what\u0026rsquo;s called a local DNS resolver.\nYour local DNS resolver, is typically configured automatically when your host is assigned an IP address using a protocol called the domain host control protocol or DHCP. In your host configuration such as this one, you can see that this local host has two local DNS resolvers. Typically, a client will try the first DNS resolver and if it doesn\u0026rsquo;t receive a response within a preconfigured timeout, it will try sending the same query to the second local DNS resolver as a backup. This query is typically issued recursively, meaning the client does not want intermediate referrals sent back to it. It only wants to hear when it\u0026rsquo;s received the final answer. The local resolver on the other hand will perform iterative queries. It might have the answer to this particular query in the cache, in which case it would simply reply with the answer. But let\u0026rsquo;s suppose for the moment, that nothing is cached. Each fully qualified domain name is presumed to end with a dot, indicating the root of the DNS hierarchy. Now the IP addresses for the root servers, or those that are authoritative for the root, may already be configured in the local DNS resolver. In this case, the resolver may be able to query for the authoritative server for .edu, say a.rootservers. net. This would be an A record query. The answer might return with what\u0026rsquo;s called an NS record, which is a referral. In this case the answer might be a referral to the edu servers. Now the local resolver issues the same query to the edu servers and receives a referral to the authoritative servers for gatech.edu. Finally the local resolver might query the authoritative name server for gatech.edu and actually receive an A record indicating the actual IP address that corresponds to that name.\nDomain Name System Part 2 Now this process of referrals, as you can see, can be rather slow. A particular DNS query might thus require round trips to multiple servers that are authoritative for different parts of the hierarchy. The blue server is authoritative for the root. The purple server is authoritative for .edu and the red server is authoritative for gatech.edu. Now supposing we wanted to save the extra time in trouble of these round trip times. This local resolver would typically have a cache that stores the NS records for each level of the hierarchy as well as the A records. And each of these answers would be stored or cached for a particular amount a time. Each one these replies has what\u0026rsquo;s called a time to live or a TTL that indicates how long each of these answers can be saved before they need to be looked up again. Caching allows for quick responses from the local DNS resolver, especially for repeated mappings. For example, since everyone is probably looking up domain names such as google.com it\u0026rsquo;s much faster to keep the answer in cache. So, given multiple clients trying to resolve the same domain name, the answers can all be resolved in a local cache. Some queries can reuse parts of this look up. For example, it\u0026rsquo;s unlikely that the authoritative name server for the root is going to change very often. So that answer might be kept, or cached, for a much longer period of time. A typical time might be hours or days, or even weeks. The mapping for a local name, such as http://www.gatech.edu, on the other hand, might change more frequently and thus these local TTL\u0026rsquo;s might need to be smaller. Now the most common type of DNS record is what\u0026rsquo;s called an A record, which maps an IP address to a domain name. But there are other important record types as well.\nRecord Types A records map names to IP addresses as we have seen. We have also seen what\u0026rsquo;s called an NS or a Nameserver record which maps a domain name to the authoritative nameserver for that domain. So we saw a bunch of NS records in the form of referrals, whereby, if we ask the route for a mapping of gatech.edu to an IP address, it doesn\u0026rsquo;t specifically know the answer, but it can issue a nameserver reply or an NS record referring the resolver to a different nameserver that could be responsible for that part of the domain name space. This allows the domain name system to be implemented as a hierarchy. Another important DNS record type is an MX record, which shows the mail server for a particular domain. Occasionally, one name actually is just an alias for another name. For example, http://www.gatech.edu actually has a slightly different real name. The CNAME is basically a pointer from an alias to another domain name that needs to be looked up. The PTR is another record that we\u0026rsquo;ll look at, and this maps IP addresses to domain names. For example if you wanted to know the name for a particular IP address, you need to issue a PTR query. This is sometimes called a reverse lookup. Finally, a AAAA record maps a domain name to an IPV6 address. Let\u0026rsquo;s take a look at a couple of different examples of domain name lookups using a command line utility called dig.\nDNS Quiz As a quick quiz, which DNS record is used for referral? Is it the MX record? A record? The Quad A record? The NS record? or the PTR?\nDNS Solution The NS record indicates the authoritative name server for a particular portion of the domain name space, and an NS record reply is often referred to as a referral. MX records indicate mail servers. A records are IP addresses for the domain name. AAAA are for IPv6 addresses, and a PTR is a name corresponding to an IP address being queried.\nExample DNS Lookup Example (using dig) Part 1 Here\u0026rsquo;s an example of a lookup for an A record for gatech.edu. You can try this at your own command line by typing, for example, dig http://www.gatech.edu. Now there are some interesting things to note in this trace. Here is our query and you can see that this is an A record query.\nHere\u0026rsquo;s our answer. You can see that the answer actually has a CNAME in it, which basically says, well you asked for gatech.edu but in fact what you really want to ask for is tlweb.gtm.gatech.edu. So then we issue an A record query for that name, and we ultimately get the IP address. These numbers here indicate the time to live or the amount of time in seconds that the entry can be stored in the cache.\nHere\u0026rsquo;s another example of a DNS lookup from nytimes.com. The interesting thing to note here is that in response to the A record query, we see two IP addresses. This is typically performed when a service wants to perform load balancing So, the client could use either one of these. It might prefer the first one, but if we issued the same query again, we might actually get these IP addresses in a different order. Now, again, here you can see the TTL value which indicates how long these A records can be stored in cache. In a subsequent example, we\u0026rsquo;ll look at other query types that have much longer TTL values.\nHere\u0026rsquo;s an example of a query for the NS record for gatech.edu. You can see this output by typing dig ns gatech.edu. You can see here in the question section, now instead of an A record query we have an NS record query. And our answer is a bunch of NS records that are dns1, 2, and 3.gatech.edu, any of which could answer authoritatively for sub-domains of gatech.edu. You can see that in addition to the answer, which return the name servers, we also need the IP addresses of those name servers, which is returned in the additional section of the answer. You can see here that we received not only A records for each domain name but also quad A or IPv6 addresses corresponding to each authoritative name server.\nHere\u0026rsquo;s an example of a query for an MX record or the mail server corresponding to gatech.edu. Now, here again you can see the question is the MX record and you can see the answer which returns two mail servers as well as the additional section, which returns an A record indicating the IP address corresponding to the mail server that was returned in the MX record. In addition to the TTL, we also have some metrics that indicate priorities that would allow a system administrator to configure a primary and a backup mail server.\nIn this case, the mail servers, just happen to have the same priority level.\nExamples (using dig) Part 2 Let\u0026rsquo;s put everything together now by looking at a trace of an entire lookup. Now in the examples before, we didn\u0026rsquo;t get to see the full lookup hierarchy because we issued a recursive query. But let\u0026rsquo;s suppose we wanted to see every step of the DNS lookup process. You can do this by using the trace option in dig. Here you can see exactly what we saw before, which is the local resolver. In this case, issuing a query to a local resolver and receiving a referral to an authoritative server for dot which could be any of the following. That query, elicits an answer for the .edu servers which subsequently issues a referral to the servers that are authoritative for gatech, which ultimately reply with the appropriate a records as well as the authoritative nameservers for gatech.edu.\nA final interesting example explores how to map an IP address. back to a name. In this case, we\u0026rsquo;re ultimately looking for PTR record, which is the name corresponding to this IP address. But first, what happens is we receive a special referral. When we ask the root servers about this particular IP address, instead of being referred to a particular .com or .edu domain, we\u0026rsquo;re referred to a special top level domain called inaddr.arpa, which maintains referrals to authoritative servers that are maintained by the respective internet routing registries, such as ARIN, RIPE, APNIC and so forth. So here we see a referral to inaddr.arpa. Subsequently, we see a referral to 130.in- addr.arpa corresponding to the first octet of the IP address. Next when we ask ARIN about 130.in-addr.arpa we receive another referral, which is to is to 207.130.in-addr.arpa. And because 130.207 is allocated to gatech.edu, ARIN knows that the appropriate referral for this part of the address space is to DNS 1, 2, or 3.gatech.edu. Next we issue a query for the next part of the octet. 7.207.130.in-addr.arpa corresponding to the first 3 octets. And now we actually get the PTR because DNS3.gatech.edu knows the reverse mapping between 130.207.7.36 and the name for that IP address. So you can see that the PTR records, or those that map IP addresses to names,\nare resolved through a special hierarchy through in-addr.arpa at the root followed by a walk through the regional registries and ultimately, to the domains, such as gatech, that are responsible for particular regions of the IP address space.\nLookup IP Address Quiz As a quick quiz, suppose we wanted to look up the IP address 130.207.97.11. What is the corresponding in-addr.arpa domain name? Is it 130.207.97.11? Is it 130.207.97.11.in-addr.arpa? Is it in-addr.arpa.130.207.97.11? Or is it 11.97.207.130.in-addr.arpa?\nLookup IP Address Solution The corresponding domain name for the PTR lookup for this IP address is the record corresponding to 11.97.207.130.in-addr.arpa. Notice that the reversal of the octets in this name corresponds to a strict traversal of the hierarchy from the highest levels of the hierarchy at inaddr.arpa to the lower levels, as the IP address moves from higher to lower parts of the hierarchy.\n"
},
{
	"uri": "/6250/congestion-control-and-streaming/",
	"title": "Congestion Control and Streaming",
	"tags": [],
	"description": "",
	"content": " Lecture 6: Congestion Control, \u0026amp; Streaming In this section of the course we\u0026rsquo;ll learn about resource control and content distribution. Resource control deals with handling bandwidth constraints on links. And in this first section, we\u0026rsquo;ll focus on controlling congestion along links. To learn more about TCPA and congestion control, your Mininet project will explore what happens when congestion control goes wrong.\nCongestion Control Okay, we\u0026rsquo;re starting course two on congestion control and streaming. And we\u0026rsquo;ll first talk about congestion control. In particular, what is congestion control and why do we need it? Simply put, the goal of congestion control is to fill the Internet\u0026rsquo;s pipes without overflowing them. So to think about this in terms of an analogy, suppose you have a sink, and you\u0026rsquo;re filling that sink with water. Well, how should you control the faucet? Too fast and the sink overflows. Too slow and you are not efficiently filling up your sink. So what you would like to do is to fill the bucket as quickly as possible without overflowing. The solution here is to watch the sink. And as the sink begins to overflow, we want to slow down how fast we\u0026rsquo;re filling it. That\u0026rsquo;s effectively how congest control works.\nCongestion Let\u0026rsquo;s suppose that in a network we have three hosts shown as squares at the edge of the network that are connected by links with capacities as shown. The two senders on the left can send at rates of 10 megabits per second and 100 megabits per second, respectively. But the link to the host on the right is only 1.5 megabits per second. So these different hosts on the left are actually competing for the same resources inside the network. So the sources are unaware of each other and also of the current state of whatever resource they are trying to share, in this case, how much other traffic is on the network. This shows up as lost packets or long delays and can result in throughput that\u0026rsquo;s less than the bottleneck link, something that\u0026rsquo;s also known as congestion collapse.\nCongestion Collapse In congestion collapse, an increase in traffic load suddenly results in a decrease of useful work done. As we can see here, up to a point, as we increase network load, there is an increase in useful work done. At some point, the network reaches saturation, at which point increasing the load no longer results in useful work getting done. But at some point, actually increasing the traffic load can cause the amount of work done or the amount of traffic forwarded to actually decrease. There are many possible causes. One possible cause is the spurious re-transmissions of packets that are still in flight. So when senders don\u0026rsquo;t receive acknowledgements for packets in a timely fashion, they can spuriously re-transmit, thus resulting in many copies of the same packets being outstanding in the network at any one time. Another cause of congestion collapse is simply undelivered packets, where packets consume resources and are dropped elsewhere in the network. The solution to spurious re-transmissions is to have better timers and to use TCP congestion control, which we\u0026rsquo;ll talk about next. The solution to undelivered packets is to apply congestion control to all traffic. Congestion control is the topic of the rest of this lesson.\nCongestion Collapse Quiz Let\u0026rsquo;s take a quick quiz. What are some possible causes of congestion collapse? Faulty router software? Spurious retransmissions of packets in flight? Packets that are travelling distances that are too far between routers? Or, undelivered packets? Please check all options that apply.\nCongestion Collapse Solution Congestion collapse is caused by spurious retransmissions of packets in flight and undelivered packets that consume resources in the network but achieve no useful work.\nGoals of Congestion Control Congestion control has two main goals. The first is to use network resources efficiently. Going back to our sink analogy, we\u0026rsquo;d like to fill the sink as quickly as possible. Fairness, on the other hand, ensures that all the senders essentially get their fair share of resources. A final goal, of course, is to avoid congestion collapse. Congestion collapse isn\u0026rsquo;t just a theory, it\u0026rsquo;s actually been frequently observed in many different networks.\nTwo Approaches to Congestion Control There are two basic approaches to congestion control: end-to-end congestion control and network assisted congested control. In end-to-end congestion control the network provides no explicit feedback to the senders about when they should slow down their rates. Instead, congestion is inferred typically by packet loss, but potentially, also by increased delay. This is the approach taken by TCP congestion control. In network assisted congestion control, routers provide explicit feedback about the rates that end systems should be sending in. So they might set a single bit indicating congestion, as is the case in TCP\u0026rsquo;s ECN, or explicit congestion notification extensions, or they might even tell the sender an explicit rate that they should be sending at. We\u0026rsquo;re going to spend the rest of the lesson talking about TCP congestion control.\nTCP Congestion Control In TCP congestion control, the senders continue to increase their rate until they see packet drops in the network. Packet drops occur because the senders are sending at a rate that is faster than the rate at which a particular router in the network might be able to drain its buffer. So you might imagine, for example, that if all three of these senders are sending at a rate that is equal to the rate at which the router is able to send traffic downstream, then eventually this buffer will fill up. TCP interprets packet loss as congestion. And when senders see packet loss, they slow down as a result of seeing the packet loss. This is an assumption. Packet drops are not a sign of congestion in all networks. For example, in wireless networks, there may be packet loss due to corrupted packets as a result of interference. But in many cases, packet drops do result because some router in the network has a buffer that has filled up and can no longer hold anymore packets and hence it drops the packets as they arrive. So senders increase rates until packets are dropped, periodically probing the network to check whether more bandwidth has become available; then they see packet loss, interpret that as congestion, and slow down. So, congestion control has two parts. One is an increase algorithm, and the other is a decrease algorithm. In the increase algorithm, the sender must test the network to determine whether the network can sustain a higher sending rate. In the decrease algorithm, the senders react to congestion to achieve optimal loss rates, delays in sending rates. Let\u0026rsquo;s now talk about how senders can achieve these increase and decrease algorithms.\nTwo Approaches to Adjusting Rate One approach is a window based algorithm. In this approach, a sender can only have a certain number of packets outstanding, or quote, in flight. And the sender uses acknowledgements from the receiver to clock the retransmission of new data. So let\u0026rsquo;s suppose that the sender\u0026rsquo;s window was four packets. At this point, there are four packets outstanding in the network. And the sender cannot send additional packets until it has received an acknowledgement from the receiver. When it receives an acknowledgment, or an ACK from the receiver, the sender can then send another packet. So at this point there are still four outstanding or four unacknowledged packets in flight. In this case if a sender wants to increase the rate at which it\u0026rsquo;s sending, it simply needs to increase the window size. So, for example, if the sender wants to send at a faster rate, it can increase the window size from four, to five. A sender might increase its rate anytime it sees an acknowledgement from the receiver. In TCP, every time a sender receives an acknowledgement, it increases the window size.\nUpon a successful receipt, we want the sender to increase its window by one packet per round trip. So, for example, in this case if the sender\u0026rsquo;s window was initially four packets, then at the end of a single round trip\u0026rsquo;s worth of sending, we want the next set of transmissions to allow five packets to be outstanding. This is called Additive Increase. If a packet is not acknowledged, the window size is reduced by half. This is called Multiplicative Decrease. So TCP\u0026rsquo;s congestion control is called additive increase multiplicative decrease, or AIMD.\nThe other approach to adjusting rates is an explicit rate-based congest control algorithm. In this case the sender monitors the loss rate and uses a timer to modulate the transmission rate. Window based congestion control, or AIMD, is the common way of performing congestion control in today\u0026rsquo;s computer networks. In the next lesson we will talk about the two goals of TCP congestion control further (efficiency and fairness) and explore how TCP achieves those goals.\nWindow Based Congestion Control Quiz Let\u0026rsquo;s have a quick quiz on window based congestion control. Suppose the round trip time between the sender and receiver is 100 ms, each packet is 1kb, and the window size is 10 packets. What is the rate at which the sender is sending? Please put your answer here in terms of kilobits per second, keeping in mind that 1 byte is 8 bits.\nWindow Based Congestion Control Solution The sending rate of the sender is approximately 800 kbps. With a window size of ten packets and a round trip time of 100 milliseconds, the sender can send 100 packets per second. With each packet being one kilobyte, or 8000 bits, that gives us 800,000 bits per second or about 800 kilobits per second.\nFairness and Efficiency in Congestion Control The two goals of congestion control are fairness (meaning every sender gets their fair share of the network resources) and efficiency (meaning that the network resources are used well). In other words we shouldn\u0026rsquo;t have a case where there are spare capacity or resources in the network, and senders have data to send, but are not able to send it. So, we\u0026rsquo;d like the network to be used efficiently, but we\u0026rsquo;d also like it to be shared among the senders.\nWe can represent fairness and efficiency in terms of a phase plot, where each axis represents a particular user, or particular senders\u0026rsquo; allocation. In this case we just have two users, one and two, and we represent their allocations with X1 and X2. If the capacity of the network is C, then we can represent the optimal operating line as X1 + X2 being some constant, C. Anything to the left of this diagonal line represents under utilization of the network, and anything to the right of the line represents overload. We can also represent another line, X1 = X2 as some notion of fair allocation. So the optimal point is where the network is neither under or over utilized, and when the allocation is fair. So being on this diagonal line represents efficiency, and being on the green diagonal line represents fairness. We can use the phase plot to understand why senders who use additive increase multiplicative decrease, converge to fairness. The senders also converge to the efficient operating point. Let\u0026rsquo;s suppose that we start at the operating point shown in blue. At this point both senders will additively increase their sending rates. Additive increase results in moving along a line that is parallel to x1 and x2, since both senders increase their rate by the same amount. Additive increase will continue until the network becomes overloaded. At this point the senders will see a loss and perform multiplicative decrease. In multiplicative decrease each sender decreases its rate by some constant factor of its current sending rate. For example, suppose each one of these senders decreases its sending rate by half. The resulting operating point is shown by this second blue dot. Note that that new operating point, as a result of multiplicative decrease, is on a line between the point on the efficiency line that the centers hit, and the origin. At this point the sender\u0026rsquo;s will again increase their sending rate along a line that\u0026rsquo;s parallel to X1 equals X2 until they hit over load again, at which point they will again retreat towards the origin. You can see that eventually the senders will reach this optimal operating point through the path that\u0026rsquo;s delineated by the blue line. To think about this a bit more you can see that every time additive increase is applied, that increases efficiency. Every time multiplicative decrease is applied that improves fairness because every time we apply multiplicative decrease, we get closer to this X1 equals X2 line.\nAIMD The result is the additive increase multiplicative decrease congestion control algorithm. The algorithm is distributed, meaning that all the senders can act independently, and we\u0026rsquo;ve just shown using the phase plots that it\u0026rsquo;s both fair and efficient. To visualize this sending rate over time, the sender\u0026rsquo;s sending rate looks roughly as shown. We call this the TCP sawtooth behavior or simply the TCP sawtooth. TCP periodically probes for available bandwidth by increasing its rate using additive increase. When the sender reaches a saturation point by filling up a buffer in a router somewhere along the path, it will see a packet loss, at which point it will decrease its sending rate by half. You can thus see that a TCP sender sends at a sending rate shown by the dotted green line that is halfway between the maximum window size at which the sender sends, and half that rate which it backs off to when it sees a loss. You can see that between the lowest sending rate and the highest is w_m over 2 plus 1 round trips. Now, given that rate we can compute the number of packets between periods of packet loss and compute the loss rate from this. The number of packets sent for every packet lost is the area of this triangle. So the lost rate is on the order of the square of the maximum window divided by some constant. Now, the throughput is the average rate, 3 4ths w_max divided by the RTT. Now if we want to relate the throughput to the loss rate, where we call the loss rate p and the throughput lambda, we simply need to solve for w_m.\nAnd I\u0026rsquo;m just going to get rid of the constant. So a loss occurs once for this number of packets, so the loss rate is simply 1 over that quantity. And then when we solve for w_m and plug in for throughput, we see that the throughput is inversely proportional to both the round trip time and the square root of the loss rate.\nAdditive Increase Quiz So as a quick quiz, and returning to our phase plot, does additive increase increase or decrease fairness? And does it increase, or decrease, or reduce, efficiency? Please check all that apply\nAdditive Increase Solution Additive increase increases efficiency because it gets us closer to that efficiency line parallel to the X1 equals X2 line. It technically also decreases fairness because the centers are both increasing their rates, so we\u0026rsquo;re relatively further away from this X1 equals X2 line. In contrast, remember that multiplicative decrease reduces efficiency by moving us further away from this green dotted line, but it increases fairness by moving us closer to the blue dotted line.\nData Centers and TCP Incast We\u0026rsquo;ll now talk about TCP congestion control in the context of modern datacenters. And we\u0026rsquo;ll talk about a particular TCP throughput collapse problem called the TCP incast problem. A typical data center consists of a set of server racks, each holding a large number of servers, the switches that connect those racks of servers, and the connecting links that connect those switches to other parts of the topology. So the network architecture is typically made up of some sort of tree and switching elements that progressively are more specialized and expensive as we move up the network hierarchy. Some of the characteristics of a data center network include a high fan in. There is a very high amount of fan in between the leaves of the tree and the top of the root workloads are high bandwidth and low latency, and many clients issue requests in parallel, each with a relatively small amount of data per request. The other constraint that we face is that the buffers in these switches can be quite small. So when we combine the requirements of high bandwidth and low latency for the applications, the presence of many parallel requests coming from these servers. and the fact that the switches have relatively small buffers, we can see that potentially there will be a problem. The throughput collapse that results from this phenomenon is called the TCP Incast problem. Incast is a drastic reduction in application throughput that results when servers using TCP all simultaneously request data, leading to a gross underutilization of network capacity in many-to-one communication networks like a datacenter. The filling up of the buffers here at the switches result in bursty retransmissions that overfill the switch buffers. And these bursting retransmissions are cause by TCP timeouts. The TCP timeouts can last hundreds of milliseconds. But the roundtrip time in a data center network is typically less than a millisecond. Often just hundreds of microseconds. Because the roundtrip times are so much less than TCP timeouts, the centers will have to wait for the TCP timeout before they retransmit an application. Throughput can be reduced by as much as 90% as a result of link idle time.\nBarrier Synchronization and Idle Time A common request pattern in data centers today is something called barrier synchronization whereby a client or an application might have many parallel threads, and no forward progress can be made until all the responses for those threads are satisfied. For example, a client might send a synchronized read with four parallel requests. But, suppose that the fourth is dropped. At this point we have a request sent at time zero, then we see a response less than a millisecond later, and at this point, threads one to three complete but TCP may time out on the fourth. In this case, the link is idle for a very long time while that fourth connection is timed out. The addition of more servers in the network induces an overflow of the switch buffer, causing severe packet loss, and inducing throughput collapse. One solution to this problem is to use fine grained TCP retransmission timers, on the order of microseconds, rather than on the order of milliseconds. Reducing the retransmission timeout for TCP thus improves system throughput. Another way to reduce the network load is to have the client acknowledge every other packet rather than every packet, thus reducing the overall network load. The basic idea here, and the premise, is that the timers need to operate on a granularity that\u0026rsquo;s close to the round-trip time of the network. In the case of a data center that\u0026rsquo;s hundreds of microseconds or less.\nTCP Incast Quiz As a quick review what are some solutions to the TCP incast problem? Having senders send smaller packets? Using finer granularity TCP timeout timers. Having the clients send fewer ackowledgements or having fewer TCP senders?\nTCP Incast Solution Using finer granularity timers and having the clients acknowledge only every other packet, as oppose to every packet, are possible solutions to the TCP Incast problem.\nMultimedia and Streaming In this lesson, we\u0026rsquo;ll talk about multimedia and streaming. We\u0026rsquo;ll talk about digital audio and video data, multimedia applications (in particular, streaming audio and video for playback), multimedia transfers over a best-effort network (in particular, how to tolerate packet loss delay and jitter), and quality of service. So we use multimedia and streaming video very frequently on today\u0026rsquo;s internet. YouTube streaming videos are an example of multimedia streaming as are applications for video or voice chat such as Skype or Google Hangout. In this lecture we\u0026rsquo;ll talk about the challenges for streaming these types of applications over best effort networks as well has how to solve those challenges. We\u0026rsquo;ll also talk about the basics of digital audio and video data. First of all, let\u0026rsquo;s talk about the challenges for media streaming.\nChallenges One challenge is that, there\u0026rsquo;s a large volume of data. Each sample is a sound or an image and there are many samples per second. Sometimes because of the way data is compressed, the volume of data that\u0026rsquo;s being sent may vary over time. In particular, the data may not be set at a constant rate. But in streaming, we want smooth playout, so the variable volume of data can pose challenges. Users typically have a very low tolerance for delay variation. Once playout of a video starts for example, you want that video to keep playing. It\u0026rsquo;s very annoying if once you\u0026rsquo;ve started playing, that the video stops. The users might have a low tolerance for delay period, so in cases like games or Voice over IP, delay is typically just unacceptable, although users can tolerate some loss. Before we get into how the network solves these challenges. Let\u0026rsquo;s talk a little bit about digitizing audio and video.\nDigitizing Audio and Video Suppose we have an analog audio signal that we\u0026rsquo;d like to digitize, or send as a stream of bits. What we can do is sample the audio signal at fixed intervals, and then represent the amplitude of each sample with a fixed number of bits. For example, if our dynamic range was from 0 to 15, we could quantize the amplitude of this signal such that each sample could be represented with four bits.\nDigitizing Audio and Video Quiz 1 Let\u0026rsquo;s take a couple of examples. So with speech you might take 8000 samples per second, and you might have 8 bits per sample. So what is the sampling rate in this case? Please give your answer in kilobits per second.\nDigitizing Audio and Video Solution 1 At 8,000 samples per second, and eight bits for every sample, the rate of digitized speech would be 64 kbps, which is a common bit rate for audio.\nDigitizing Audio and Video Quiz 2 Suppose we have a MP3 with 10,000 samples per second and 16 bits per sample. What\u0026rsquo;s the resulting rate in this case in kbps?\nDigitizing Audio and Video Solution 2 The resulting rate in this case is, a 160 kbps.\nVideo Compression Video compression works in slightly different ways. Each video is a sequence of images and each image can be compressed with spatial redundancy, exploiting aspects that humans tend not to notice. Also there is temporal redundancy. Between any two video images, or frames, there might be very little difference. So if this person was walking towards a tree, you might see a version of the image that\u0026rsquo;s almost the same, except with the person shifted slightly to the right. Video compression uses a combination of static image compression on what are called reference frames, or anchor frames (sometimes called I frames), and derived frames, sometimes called P frames. The P frame can be represented as the I frame, compressed.\nIf we take the I frame and divide it into blocks, we can then see that the P frame is almost the same except for a few blocks here that can be represented in terms of the original I frame blocks, plus a few motion vectors.\nA common video compression format that\u0026rsquo;s used on the internet is called MPEG.\nStreaming Video In a streaming video system where the server streams stored audio and video, the server stores the audio or video files, the client requests the files, and plays them as they download. It\u0026rsquo;s important to play the data at the right time. The server can divide the data into segments and then label each segment with a time stamp indicating the time at which that particular segment should be played, so the client knows when to play that data. The data must arrive at the client quickly enough, otherwise the client can\u0026rsquo;t keep playing. The solution is to have a client use what\u0026rsquo;s called a playout buffer, where the client stores data as it arrives from the server, and plays the data for the user in a continuous fashion. Thus, data might arrive more slowly or more quickly from the server, but as long as the client is playing data out of the buffer at a continuous rate, the user sees a smooth playout. A client may typically wait a few seconds before it starts playing the stream to allow data to be built up in this buffer to account for cases when the server might have times where it is not sending at a rate that\u0026rsquo;s sufficient to satisfy the client\u0026rsquo;s playout rate.\nPlayout Delay Looking at this graphically, we might see packets generated at a particular rate and the packets might be received at slightly different times, depending on network delay. These types of delays are the types that we want to avoid when we playout. So if we wait to receive several packets and fill the buffer before we start playing, say to here, then we can have a playout schedule that is smooth regardless of the erratic arrival times that may result from network delays. So this playout delay or buffering allows the client to achieve a smooth playout. Some delay at the beginning of the playout is acceptable. Startup delays of a few seconds are things that users can typically tolerate, but clients cannot tolerate high variation in packet arrivals if the buffer starves or if there aren\u0026rsquo;t enough packets in the buffer. Similarly, small amount of loss or missing data does not disrupt the playback, but retransmitting a lost packet might actually take too long and result in delays or starvation of the playout buffer.\nStreaming Quiz So as a quick review, which of these pathologies can streaming audio and video tolerate? Packet loss, delay, variation in delay, or jitter?\nStreaming Solution Some delay at the beginning of a packet stream is acceptable. And similarly, some small amount of missing data is okay. We can tolerate small amounts of missing data that result in slightly reduced quality of the audio or video stream. On the other hand, a receiver is not very good at tolerating variability in packet delay in the packet stream, particularly if the client buffer is starved.\nTCP is Not a Good Fit It turns out that TCP is not a good fit for congestion control for streaming video, or streaming audio. TCP retransmits lost packets, but retransmissions may not always be useful. TCP also slows down its sending rate after packet loss, which may cause starvation of the client. There\u0026rsquo;s also a fair amount of overhead in the protocol. A TCP header of 20 bytes for every packet is large for audio samples and sending acknowledgements for every other packet may be more feedback than is needed. Instead, one might consider using UDP. UDP does not retransmit lost packets and it does not automatically adapt the sending rate. It also has a smaller header. Because UDP does not automatically retransmit packets or adjust the sending rate, many things are left to higher layers, potentially the application, such as when to transmit the data, how to encapsulate it, whether to retransmit, and whether to adapt the sending rate, or to adapt the quality of the video or audio encoding. So higher layers must solve these problems. In particular, the sending rate still needs to be friendly or fair to other TCP senders, which may be sharing the link. There are a variety of video streaming and audio streaming transport protocols that are built on top of UDP that allows senders to figure out when and how to retransmit lost packets and how to adjust sending rates.\n(More) Streaming We\u0026rsquo;re going to talk a little bit more about streaming and in particular, specific applications and how they work. We\u0026rsquo;ll talk about a streaming video application, YouTube, and a Voice over IP (or streaming audio) application, Skype. With YouTube, all uploaded videos are converted to Flash or html5 and nearly every browser has a Flash plug-in. Thus, every browser can essentially play these videos. HTTP and TCP are implemented in every browser and the streams easily get through most firewalls. So, in this case, even though we\u0026rsquo;ve talked about why TCP is suboptimal for streaming, the designers of YouTube decided to keep things simple, potentially at the expense of video quality. When a client makes an HTTP request to youtube.com, it is redirected to a content distribution network server located in a content distribution network, such as Limelight or perhaps even YouTube\u0026rsquo;s own content distribution network. We will talk about content distribution networks later on in this course. When the client sends an HTTP get message to this CDN server, the server responds with a video stream. Similarly with Skype, or Voice Over IP, your analog signal is digitized through an A to D conversion and this resulting digitized bit stream is sent over the internet. In the case of Skype, this A to D conversion happens by way of the application. And in the case of Voice over IP, this conversion might be performed with some kind of phone adapter that you actually plug your phone into. An example of that might be Vonage. In VoIP with an analogue phone, the adapter converts between the analogue and digital signal. It sends and receives data packets and then communicates with the phone in a standard way. Skype, on the other hand, is based on what\u0026rsquo;s known as peer-to-peer technology where individual users using Skype actually route voice traffic through one another. We will talk more about peer-to-peer content distribution later in this course.\nSkype So Skype has a central log-in server but then uses pier to pier to exchange the actual voice streams, and compresses the audio to achieve a fairly low bit rate. At around 67 bytes per packet and 140 packets per second, we see Skype uses about 40 kilobites per second in each direction. Data packets are also encrypted in both directions. Good audio compression and avoiding hops through far away hosts can improve the quality of the audio, as can forward error correction. Ultimately, though, the network is a major factor. Long propagation delays, high congestion, or disruptions as a result of routing changes can all degrade the quality of a voice over IP call. To ensure that some streams achieve acceptable performance levels, we sometimes use what\u0026rsquo;s called Quality of Service. One way of doing Quality of Service is through explicit reservations. But, another way is to simply mark certain packet streams as higher priorities than others. Let\u0026rsquo;s first take a look at marking and policing of traffic sending rates.\nMarking (and Policing) So we know from before that applications compete for bandwidth. Consider a Voice over IP application and a file transfer application that want to share a common network link. In this case, we\u0026rsquo;d like the audio packets to receive priority over the file transfer packets since the user\u0026rsquo;s experience can be significantly degraded by lost or delayed audio packets. In this case, we want to mark the audio packets as they arrive at the router so that they receive a higher priority than the file transfer packets. You might imagine implementing this with priority queues where the VOIP packets were put in one queue and the file transfer packets are put in a separate queue that is served less often than a high priority queue. An alternative is to allocate fixed bandwidth per application, but the problem with this alternative is that it can result in inefficiency if one of the flows doesn\u0026rsquo;t fully utilize its fixed allocation. So the idea, in addition to marking and policing, is to apply scheduling. One way of applying scheduling is to use what\u0026rsquo;s called weighted fair queuing, whereby the queue with the green packets is served more often than the queue with the red packets. Another alternative is to use admission control, whereby an application declares its needs in advance, and the network may block the application\u0026rsquo;s traffic if the application can\u0026rsquo;t satisfy the needs. A busy signal on a telephone network is an example of admission control. But can you imagine how annoying it would be if you attempted to load a web page and were blocked. This blocking, or the user experience that results from it, is a negative consequence of admission control and is one of the reasons it\u0026rsquo;s not commonly applied for internet applications.\nQoS Quiz So as a quick quiz what are some commonly used Quality of Service techniques for streaming audio and video? Marking packets and putting them into queues with different priorities, scheduling the service of these queues at different rates depending on the priority of the application, blocking flows with admission control if the network can\u0026rsquo;t satisfy their rates, or performing fixed bandwidth allocations? Please check all that apply.\nQos Solution A common way of applying QoS for streaming applications is to mark the packets at a higher priority, put those packets into a higher priority queue, and then schedule that queue so that it\u0026rsquo;s serviced more regularly, or more frequently, than traffic or applications that are lower priority.\nParking Lot Problem Recall that congestion collapse occurs when packets consume valuable network resources only to get dropped later at a downstream link. And although the network is forwarding packets, none of them actually reach the destination and no useful communication occurs, resulting in what we called congestion collapse. We talked about congestion collapse in an earlier lesson in this course.\nRecall that the goal of TCP is to prevent congestion collapse. In this assignment, you will become familiar with the Mininet environment, creating custom network topologies, running programs in virtual hosts that generate TCP traffic, and learn about TCP congestion control, an the TCP sawtooth. You\u0026rsquo;ll also see how bandwidth is shared across multiple flows.\nWe are going to explore these concepts in a particular topology that we\u0026rsquo;ll just call a parking lot. So in the assignment, you\u0026rsquo;ll create the topology below with the following parameters. N will be the number of receivers connected via this switch like topology, the data rate of each link will be 100 megabits per second, and the delay of each link will be one millisecond. Now you\u0026rsquo;re going to use iperf to generate simultaneous TCP flows from each of the senders to the lone receiver. Then you\u0026rsquo;re going to plot the time series of throughput versus time for each of these senders for each of your experiments as you vary the value of N. Your plot should run for 60 seconds. We have given an initial file called parkinglot.py. Your goal is to fill in the gaps. You\u0026rsquo;ll need to both fill in the part that sets up the topology using a parameter of N, and then you\u0026rsquo;ll need to fill in the part that actually generates the TCP flows between the senders and the receiver using iperf, and monitor the throughput of each of these flows.\n"
},
{
	"uri": "/6476/",
	"title": "6476",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6750/",
	"title": "6750",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6460/",
	"title": "6460",
	"tags": [],
	"description": "",
	"content": "TODO: coming soon\u0026hellip; PR\u0026rsquo;s welcome\n"
},
{
	"uri": "/6250/",
	"title": "6250",
	"tags": [],
	"description": "",
	"content": " About:  todo  Resources:  todo Reading list:\u0026hellip;  "
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " OMS:Notes About This project has two goals:\n Accelerate the rate of learning for all Provide resources to help those either 1) new to OMSCS or 2) new to a technical domain   If you want to go fast, go alone. If you want to go far, go together. \u0026ndash; AFRICAN PROVERB\n Contributing Contributions are very welcome. This is first and foremost a community effort by OMSCS for OMSCS.\n If you would like to make an occasional edit, please do so with the tab on the relevent page. If you would like to contribute more, please see the contributing guide \u0026ndash; coming soon  If any material here is out of date or can be improved, click on the \u0026ldquo;Improve this page\u0026rdquo; tab.\n "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]